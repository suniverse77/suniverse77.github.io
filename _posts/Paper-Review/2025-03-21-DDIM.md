---
title: "[ë…¼ë¬¸ë¦¬ë·°] Denoising Diffusion Implicit Models"
date: 2025-03-21 00:00:00 +/-TTTT
categories: [ë…¼ë¬¸ë¦¬ë·°, Generative AI]
tags: [Diffusion, Image Generation]
math: true
toc: true
author: sunho
description: ğŸ“ ICLR 2021
---

[[Paper]](https://arxiv.org/abs/2010.02502)

<details>
<summary><font color='#FF8C00'>ğŸ“ Summary</font></summary>
<div markdown="1">
<br>
í•˜ë‚˜ì˜ ëª¨ë¸ì„ í•™ìŠµí•´ë‘ë©´, ì´í›„ ìƒì„± ë‹¨ê³„ì—ì„œ ì›í•˜ëŠ” ìƒ˜í”Œë§ ê²½ë¡œ(ê¸¸ì´/ê°„ê²©)ë¥¼ ì„ íƒí•˜ì—¬ ìœ ì—°í•˜ê³  ë¹ ë¥´ê²Œ ìƒì„±í•  ìˆ˜ ìˆë‹¤.

</div>
</details>

## Introduction

**ê¸°ì¡´ ì—°êµ¬ë“¤ì˜ í•œê³„ì **

- DDPMê³¼ ê°™ì€ iterative generative modelë“¤ì€ ê³ í’ˆì§ˆ ìƒ˜í”Œì„ ìƒì„±í•˜ê¸° ìœ„í•´ ë§ì€ ë°˜ë³µ ë‹¨ê³„ë¥¼ í•„ìš”ë¡œ í•œë‹¤.
- ë°˜ë©´ GANì€ ë‹¨ í•œ ë²ˆì˜ ë„¤íŠ¸ì›Œí¬ í†µê³¼ë§Œìœ¼ë¡œ ìƒ˜í”Œì„ ìƒì„±í•˜ë¯€ë¡œ í›¨ì”¬ ë¹ ë¥´ë‹¤.

**ì œì•ˆí•˜ëŠ” ë°©ë²•**

- DDPMê³¼ ë™ì¼í•œ í•™ìŠµ ëª©ì  í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, non-Markovian í™•ì‚° ê³¼ì •ì„ ë„ì…í•´ ì§§ì€ ìƒì„± ê²½ë¡œë¥¼ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.
- ê°™ì€ ì´ˆê¸° latentì—ì„œ ìƒì„±ëœ ê²°ê³¼ë“¤ì´ êµ¬ì¡°ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì¼ê´€ì„±ì„ ê°€ì§„ë‹¤.

## Methods

### 1. Variational Inference for non-Markovian Forward Process

ì €ìë“¤ì€ DDPMì˜ ëª©ì  í•¨ìˆ˜ $L_\gamma$ëŠ” joint ë¶„í¬ $q(\mathbf x_{1:T}\mid\mathbf x_0)$ê°€ ì•„ë‹ˆë¼ marignal ë¶„í¬ $q(\mathbf x_t\mid\mathbf x_0)$ì—ë§Œ ì˜ì¡´í•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•˜ì˜€ë‹¤.

ë™ì¼í•œ marignal ë¶„í¬ë¥¼ ê°–ëŠ” joint ë¶„í¬ê°€ ì—¬ëŸ¬ ê°€ì§€ê°€ ì¡´ì¬í•˜ë¯€ë¡œ, ê¸°ì¡´ì˜ Markovian ì¶”ë¡  ê³¼ì •ì„ ëŒ€ì²´í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ non-Markovian ì¶”ë¡  ë¶„í¬ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤.

![fig1](paper/DDIM-1.png){: style="display:block; margin:0 auto; width:100%;"}

<center><img src='{{"/assets/images/ë…¼ë¬¸ë¦¬ë·°/DDIM-1.png" | relative_url}}' width="100%"></center>

#### Non-Markovian Forward Process

ê¸°ì¡´ DDPMê³¼ ë™ì¼í•œ marginal ë¶„í¬ë¥¼ ê°–ëŠ” non-Markovian joint ë¶„í¬ë¥¼ ì•„ë˜ì™€ ê°™ì´ ìƒˆë¡­ê²Œ ì •ì˜í•˜ì˜€ë‹¤.

$$
q_\sigma(\mathbf{x}_{1:T} \mid \mathbf{x}_0) :=
q_\sigma(\mathbf{x}_T \mid \mathbf{x}_0) \prod_{t=2}^{T} q_\sigma(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)
$$

$\sigma=[\sigma_1,\dots,\sigma_T]=\in\mathbb{R}^T$ëŠ” 0ì´ìƒì˜ ì‹¤ìˆ˜ ë²¡í„°ë¥¼ ì˜ë¯¸í•œë‹¤. (ë¶„ì‚°ì´ ì•„ë‹˜)

$q_\sigma(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)$ëŠ” reverse process ìˆ˜ì‹ì²˜ëŸ¼ ë³´ì´ì§€ë§Œ, forward processì˜ êµ¬ì„± ìš”ì†Œì´ë¯€ë¡œ forward processë¼ê³  ë´ì•¼ í•œë‹¤.

ëª¨ë“  $t$ì— ëŒ€í•´ì„œ $q_\sigma(\mathbf{x}_T \mid \mathbf{x}_0) = \mathcal{N}(\sqrt{\alpha_t} \mathbf{x}_0,\ (1 - \alpha_t) \mathbf{I})$ì˜ ì‹ì„ ë§Œì¡±í•˜ê¸° ìœ„í•´ì„œ í‰ê·  í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ì´ ìœ ë„ë˜ì—ˆë‹¤.

$$
q_\sigma(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) =
\mathcal{N}\bigg(
\sqrt{\bar\alpha_{t-1}}\, \mathbf{x}_0
+ \sqrt{1 - \bar\alpha_{t-1} - \sigma_t^2} \cdot
\frac{\mathbf{x}_t - \sqrt{\bar\alpha_t} \mathbf{x}_0}{\sqrt{1 - \bar\alpha_t}},
\sigma_t^2 \mathbf{I}
\bigg)
$$

ë² ì´ì¦ˆ ì •ë¦¬ì— ì˜í•´ forward processëŠ” ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìœ¼ë©°, ì´ ë˜í•œ ê°€ìš°ì‹œì•ˆ ë¶„í¬ì´ë‹¤.

$$
q_\sigma(\mathbf{x}_t \mid \mathbf{x}_{t-1}, \mathbf{x}_0) =
\frac{q_\sigma(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \, q_\sigma(\mathbf{x}_t \mid \mathbf{x}_0)}{q_\sigma(\mathbf{x}_{t-1} \mid \mathbf{x}_0)}
$$

$\mathbf{x}\_t$ê°€ $\mathbf{x}\_{t-1}$ê³¼ $\mathbf{x}\_0$ì— ì˜ì¡´í•˜ë¯€ë¡œ, forward processëŠ” ë”ì´ìƒ Markovianì´ ì•„ë‹ˆë‹¤.

#### Generative Process and Unified Variational Inference Objective

Reverse processì—ì„œ ê° $p_\theta^{(t)}(\mathbf{x}\_{t-1}\mid\mathbf{x}\_t)$ëŠ” $q\_\sigma(\mathbf{x}\_t \mid \mathbf{x}\_{t-1}, \mathbf{x}\_0)$ì— ëŒ€í•œ ì§€ì‹ì„ í™œìš©í•œë‹¤.

ì§ê´€ì ìœ¼ë¡œ, noisyí•œ ê´€ì¸¡ê°’ $\mathbf{x}\_t$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ë¨¼ì € ê·¸ì— ëŒ€ì‘ë˜ëŠ” $\mathbf{x}\_0$ë¥¼ ì˜ˆì¸¡í•˜ê³ , $q_\sigma(\mathbf{x}\_t \mid \mathbf{x}\_{t-1}, \mathbf{x}\_0)$ë¥¼ ì´ìš©í•´ $\mathbf{x}\_{t-1}$ì„ ìƒì„±í•œë‹¤.

$$
\mathbf x_t=\sqrt{\bar{\alpha}_t}\mathbf x_0+\sqrt{(1-\bar{\alpha}_t)}\boldsymbol\epsilon
$$

ìœ„ì˜ ì‹ì€ DDPMì—ì„œ ì •ì˜ëœ forward samplingì„ ë‚˜íƒ€ë‚´ëŠ”ë°, ëª¨ë¸ì€ $\epsilon_\theta(\mathbf{x}_t,t)$ëŠ” $\mathbf{x}_t$ë¡œë¶€í„° ë…¸ì´ì¦ˆ $\epsilon_t$ë¥¼ ì˜ˆì¸¡í•˜ê¸° ë•Œë¬¸ì— ìœ„ì˜ ì‹ì„ ì•„ë˜ì™€ ê°™ì´ ë‹¤ì‹œ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.

$$
f_\theta^{(t)}(\mathbf{x}_t):=
\frac{\mathbf{x}_t-\sqrt{1-\bar\alpha_t}\cdot\boldsymbol\epsilon_\theta^{(t)}(\mathbf{x}_t)}{\sqrt{\bar\alpha_t}}
$$

$f_\theta^{(t)}(\mathbf{x}_t)$ëŠ” $\mathbf{x}_t$ë¡œë¶€í„° $\mathbf{x}\_0$ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.

ìµœì¢…ì ìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥í•œ ìƒì„± ê³¼ì • $p_\theta(\mathbf{x}_{0:T})$ëŠ” ì•„ë˜ì™€ ê°™ì´ ì •ì˜ëœë‹¤.

$$
p_\theta^{(t)}(x_{t-1} \mid x_t) = 
\begin{cases}
\mathcal{N}(f_\theta^{(t)}(x_1),\, \sigma_1^2 \mathbf{I}) & \text{if } t = 1 \\
q_\sigma(x_{t-1} \mid x_t,\, f_\theta^{(t)}(x_t)) & \text{if } t > 1
\end{cases}
$$

---

$$
J_\sigma(\boldsymbol{\epsilon}_\theta) :=
\mathbb{E}_{\mathbf{x}_{0:T} \sim q_\sigma(\mathbf{x}_{0:T})}
\left[
\log q_\sigma(\mathbf{x}_{1:T} \mid \mathbf{x}_0)
- \log p_\theta(\mathbf{x}_{0:T})
\right]
\\
=
\mathbb{E}_{\mathbf{x}_{0:T} \sim q_\sigma(\mathbf{x}_{0:T})}
\left[
\log q_\sigma(\mathbf{x}_T \mid \mathbf{x}_0)
+ \sum_{t=2}^{T} \log q_\sigma(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)
- \sum_{t=1}^{T} \log p_\theta^{(t)}(\mathbf{x}_{t-1} \mid \mathbf{x}_t)
- \log p_\theta(\mathbf{x}_T)
\right]
$$

ìœ„ ëª©ì  í•¨ìˆ˜ $J_\sigma$ì˜ ì •ì˜ë¡œë¶€í„°, $\sigma$ ì„ íƒì— ë”°ë¼ ë‹¤ë¥¸ ëª©ì  í•¨ìˆ˜ê°€ ìƒê¸°ê¸° ë•Œë¬¸ì—, ê° $\sigma$ ë§ˆë‹¤ ë‹¤ë¥¸ ëª¨ë¸ì„ í•™ìŠµí•´ì•¼ í•  ê²ƒì²˜ëŸ¼ ë³´ì¼ ìˆ˜ ìˆë‹¤.

í•˜ì§€ë§Œ Theorem 1ì— ì˜í•´, íŠ¹ì • ê°€ì¤‘ì¹˜ $\gamma$ì— ëŒ€í•´ $J_\sigma$ì™€ $L_\gamma$ëŠ” ë™ì¼í•˜ë‹¤.

> **Theroem 1**
> 
> 0ë³´ë‹¤ í° ëª¨ë“  $\sigma$ì— ëŒ€í•´ $J_\sigma=L_\gamma+C$ë¥¼ ë§Œì¡±ì‹œí‚¤ëŠ” $\gamma\in\mathbb{R}^T_{>0}$ì™€ $C\in\mathbb{R}$ê°€ ì¡´ì¬í•œë‹¤.
>
> ---
>
> $J_\sigma$: non-Markovian variational ëª©ì í•¨ìˆ˜
> 
> $L_\gamma$: DDPMì˜ ì¼ë°˜í™”ëœ ëª©ì í•¨ìˆ˜ (ê° ì‹œê°„ $t$ë§ˆë‹¤ ê°€ì¤‘ì¹˜ $\gamma_t$ ì ìš©)
>
> ì´ ì •ë¦¬ëŠ” DDPMì—ì„œ ì‚¬ìš©ë˜ëŠ” $L_1$ ëª©ì í•¨ìˆ˜ê°€ DDIM ë“± ë‹¤ë¥¸ ìƒì„± ê³¼ì •ì—ë„ ê·¸ëŒ€ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆìŒì„ ì´ë¡ ì ìœ¼ë¡œ ë³´ì¥í•œë‹¤.

### 2. Sampling from Generalized Generative Process

![fig2](paper/DDIM-2.png){: style="display:block; margin:0 auto; width:60%;"}

#### Denoising Diffusion Implicit Models

ì•„ë˜ì˜ ì‹ì„ í†µí•´ $\mathbf{x}_t$ë¡œë¶€í„° $\mathbf{x}_{t-1}$ì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤.

$$
\mathbf{x}_{t-1} =
\sqrt{\bar\alpha_{t-1}}
\underbrace{
\left(\frac{\mathbf{x}_t - \sqrt{1 - \bar\alpha_t} \, \boldsymbol{\epsilon}_\theta^{(t)}(\mathbf{x}_t)
}{\sqrt{\bar\alpha_t}}\right)
}_{\text{predicted } \mathbf{x}_0}
+ \underbrace{
\sqrt{1 - \bar\alpha_{t-1} - \sigma_t^2} \cdot \boldsymbol{\epsilon}_\theta^{(t)}(\mathbf{x}_t)
}_{\text{direction pointing to } \mathbf{x}_t}
+ \underbrace{\sigma_t \boldsymbol{\epsilon}_t}_{\text{random noise}}
$$

ëœë¤í•œ ë¶€ë¶„ì´ $\sigma_t \boldsymbol{\epsilon}_t$ë°–ì— ì—†ê¸° ë•Œë¬¸ì— $\sigma$ ê°’ì— ë”°ë¼ì„œ ìƒì„± ê³¼ì •ì´ ê²°ì •ëœë‹¤.

$\sigma_t=\sqrt{\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}}\sqrt{\frac{1-\bar\alpha_t}{\bar\alpha_{t-1}}}$ì¼ ë•Œ, forward processëŠ” Markovianì´ ë˜ë©°, generative processëŠ” DDPMì´ ëœë‹¤.

$\sigma_t=0$ì¸ ê²½ìš° forward process $q_\sigma(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)$ëŠ” $\mathbf{x}\_t$ì™€ $\mathbf{x}\_{t-1}$ì´ ì£¼ì–´ì¡Œì„ ë•Œ deterministicí•˜ê²Œ ë˜ë©°, generative processì˜ ëœë¤í•œ term ë˜í•œ 0ì´ ëœë‹¤.

ê²°ê³¼ì ìœ¼ë¡œ ëª¨ë¸ì€ ê³ ì •ëœ ì ˆì°¨ì— ë”°ë¼ ì ì¬ ë³€ìˆ˜ë¡œë¶€í„° ìƒ˜í”Œì„ ìƒì„±í•˜ëŠ” ë°©ì‹ì¸ implicit probabilistic ëª¨ë¸ì´ ëœë‹¤.

ë¹„ë¡ ìˆœë°©í–¥ ê³¼ì •ì€ ë” ì´ìƒ í™•ì‚°ì´ ì•„ë‹ˆì§€ë§Œ, DDPMì˜ ëª©ì  í•¨ìˆ˜ë¡œ í•™ìŠµë˜ëŠ” implicit probabilistic ëª¨ë¸ì´ê¸° ë•Œë¬¸ì— Denoising Diffusion Implicit Modelì´ë¼ê³  ëª…ëª…í•˜ì˜€ë‹¤.

#### Accelerated Generation Process

DDPMì—ì„œëŠ” forward processê°€ $T$ stepì´ë¼ë©´, generative processë„ $T$ step ë™ì•ˆ ìƒ˜í”Œë§ì„ í–ˆì–´ì•¼ í•œë‹¤.

í•˜ì§€ë§Œ $q_\sigma(\mathbf{x}_t\mid\mathbf{x}_0)$ê°€ ê³ ì •ëœë‹¤ë©´ ëª©ì í•¨ìˆ˜ $L_1$ì€ íŠ¹ì •í•œ forward procedure(ì„¸ë¶€ êµ¬ì¡°ë‚˜ step ê¸¸ì´)ì— ì˜ì¡´í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, $T$ë³´ë‹¤ ì‘ì€ forward processë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆë‹¤.

ë”°ë¼ì„œ ì•„ë˜ì™€ ê°™ì´ ì¼ë¶€ time stepì— ëŒ€í•´ì„œë§Œ forward processë¥¼ ì •ì˜í•  ìˆ˜ ìˆë‹¤. ë” ì§§ì€ ìƒ˜í”Œë§ ê²½ë¡œ(sampling trajectory)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

$$
\lbrace
\mathbf{x}_{\tau_1},\dots,\mathbf{x}_{\tau_S}
\rbrace
$$

ì—¬ê¸°ì„œ $\tau$ëŠ” ê¸¸ì´ê°€ $S$ì¸ $[1,\dots,T]$ì˜ ë¶€ë¶„ ìˆ˜ì—´ì´ë‹¤.

ìƒì„± ê³¼ì •ì€ $\tau$ ì—­ìˆœìœ¼ë¡œ ìƒ˜í”Œë§í•˜ë©°, ì´ë¥¼ sampling trajectoryë¼ê³  í•œë‹¤.

$\tau$ì˜ ê¸¸ì´ë¥¼ ì¤„ì´ë©´ ìƒ˜í”Œë§ íšŸìˆ˜ë„ ì¤„ì–´ë“¤ì–´ ê³„ì‚° íš¨ìœ¨ì´ í¬ê²Œ í–¥ìƒë˜ë©°, ê¸°ì¡´ $L_1$ìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì— í•™ìŠµ ê³¼ì •ì€ ë³€ê²½ ì—†ì´ ìƒì„± ë‹¨ê³„ë§Œ ìˆ˜ì •í•˜ë©´ ëœë‹¤.

#### Relevance to Neural ODEs

DDIM ë°˜ë³µ ê³¼ì •ì„ ì•„ë˜ì™€ ê°™ì´ ë‹¤ì‹œ ì •ë¦¬í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ODEë¥¼ í’€ê¸° ìœ„í•œ Euler ì ë¶„ ë°©ì‹ê³¼ êµ¬ì¡°ì ìœ¼ë¡œ ìœ ì‚¬í•˜ë‹¤.

$$
\frac{\mathbf{x}_{t - \Delta t}}{\sqrt{\bar\alpha_{t - \Delta t}}}
=
\frac{\mathbf{x}_t}{\sqrt{\bar\alpha_t}}
+
\left(
\sqrt{\frac{1 - \bar\alpha_{t - \Delta t}}{\bar\alpha_{t - \Delta t}}}
-
\sqrt{\frac{1 - \bar\alpha_t}{\bar\alpha_t}}
\right)
\boldsymbol{\epsilon}_\theta^{(t)}(\mathbf{x}_t)
$$

ì´ì— ëŒ€ì‘í•˜ëŠ” ODEë¥¼ ìœ ë„í•˜ê¸° ìœ„í•´ $\frac{\sqrt{1-\bar\alpha}}{\sqrt{\bar\alpha}}=\sigma$, $\frac{\mathbf{x}}{\sigma}=\bar{\mathbf{x}}$ë¡œ ì¹˜í™˜í•˜ì—¬ ì•„ë˜ì˜ ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ì˜€ë‹¤.

$\sigma(0)=0$ì¼ ë•Œ, ìœ„ì˜ ì‹ì€ ì•„ë˜ì˜ ODEë¥¼ í’€ê¸° ìœ„í•œ Euler methodë¡œ ë‹¤ë¤„ì§ˆ ìˆ˜ ìˆë‹¤.

$$
\mathrm{d}\bar{\mathbf{x}}(t)
=
\boldsymbol{\epsilon}_\theta^{(t)}\left(
\frac{\bar{\mathbf{x}}^{(t)}}{\sqrt{\sigma^2 + 1}}
\right)
\, \mathrm{d}\sigma(t)
$$

ì¶©ë¶„í•œ discretization stepë“¤ì„ ê±°ì¹˜ë©´, ìœ„ ì‹ì˜ ODEë¥¼ reverseí•´ì„œ generation processì˜ reverseë„ ê°€ëŠ¥í•´ì§„ë‹¤.

ì¦‰, $\mathbf{x}_0$ë¥¼ $\mathbf{x}_T$ë¡œ ì¸ì½”ë”©ì´ ê°€ëŠ¥í•´ì§„ë‹¤.

## Experiments

### Sample Quality and Efficiency

![fig3](paper/DDIM-3.png){: style="display:block; margin:0 auto; width:100%;"}

ìƒ˜í”Œì„ ìƒì„±í•  ë•Œ ì‚¬ìš©ëœ time step ìˆ˜ $S=\text{dim}(\tau)$ì™€ ê³¼ì •ì˜ í™•ë¥ ì„± ì •ë„(stochascticity) $eta$ì˜ ë³€í™” ë”°ë¥¸ FIDì™€ ìƒ˜í”Œ í’ˆì§ˆì„ ë¹„êµí•œ ê²ƒì´ë‹¤.

Tableì—ì„œ íŒŒë€ìƒ‰ í˜•ê´‘íœì€ DDIM, ì£¼í™©ìƒ‰ í˜•ê´‘íœì€ DDPMì´ë‹¤.

### Sample Consistency in DDIMs

![fig4](paper/DDIM-4.png){: style="display:block; margin:0 auto; width:100%;"}

ê°™ì€ ì´ˆê¸° ë…¸ì´ì¦ˆ $\mathbf{x}_T$ì—ì„œ ì‹œì‘í•˜ë©´, generative trajectory($\tau$)ê°€ ë‹¬ë¼ë„ ë¹„ìŠ·í•œ ì´ë¯¸ì§€ê°€ ìƒì„±ëœë‹¤.
