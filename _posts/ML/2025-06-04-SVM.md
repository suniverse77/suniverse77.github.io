---
title: "[지도학습] SVM (Support Vector Machine)"
date: 2025-06-04 00:00:00 +/-TTTT
categories: [AI, 머신러닝]
tags: [머신러닝, 지도 학습]
math: true
toc: true
author: sunho
---

## Support Vector Machine

결정 경계 (Decision Boundary)와 가장 가까운 데이터 샘플을 Support Vector라고 부르고, 각 클래스의 Support Vector 사이의 거리를 마진 (margin)이라고 한다.

SVM은 이 마진을 최대화할 수 있는 결정 경계를 찾는 방법이다.

![fig1](ml/4-1.png){: style="display:block; margin:0 auto; width:50%;"}
_[[출처]](https://python.plainenglish.io/a-comprehensive-guide-to-support-vector-machine-svm-algorithm-76dbcf18b5ae)_

이전 포스터에서 설명했듯이, 결정 경계는 아래와 같이 표현할 수 있다.

$$
\mathbf{w}^\top\mathbf{x}+b=0
$$

예를 들어, 위의 그림에서 파란색 데이터들의 라벨을 $y=+1$, 빨간색 데이터들의 라벨을 $y=-1$이라고 해보자. 범위는 결정 경계보다 $+1$만큼, 빨간색 데이터들은 결정 경계보다 $-1$ 이하의 범위라고 해보자.

$$
\begin{cases}
\mathbf{w}^\top\mathbf{x}+b\geq1&\text{if}~~ y=+1\\
\mathbf{w}^\top\mathbf{x}+b\leq-1&\text{if}~~ y=-1
\end{cases}
$$

그리고 파란색 데이터의 라벨을 $1$, 빨간색 데이터의 라벨을 $-1$로 설정하면, 조건식은 아래와 같이 정리될 수 있다.

$$
y_i(\mathbf{w}^\top\mathbf{x}_i+b)\geq1
$$

이때 마진은 아래와 같이 정의된다.

$$
\text{Margin}=\frac{2}{\lVert\mathbf{w}\rVert}
$$

<details>
<summary><font color='#0000FF'>공식 유도</font></summary>
<div markdown="1">

$$
\begin{aligned}
\mathbf{w}^\top\mathbf{x}+b=1\quad\to\quad w_2x_2+w_1x_1+b=1~~~\\
\mathbf{w}^\top\mathbf{x}+b=-1\quad\to\quad w_2x_2+w_1x_1+b=-1
\end{aligned}
$$

두 직선 $w_2x_2+w_1x_1+b-1=0$과 $w_2x_2+w_1x_1+b+1=0$ 사이의 거리 $d$는 아래와 같이 정의된다.

$$
d=\frac{\lvert(b-1)-(b+1)\rvert}{\sqrt{w_2^2+w_1^2}}=\frac{2}{\lVert\mathbf{w}\rVert_2}
$$

---

</div>
</details>
<br>

즉, 우리의 목표는 이 마진을 최대화하는 것이기 때문에 목적 함수는 아래와 같다.

$$
\min{}{\frac{\lVert\mathbf{w}\rVert^2}{2}}
$$

SVM은 ~에 따라 Hard Margin과 Soft Margin으로 분류된다.

### Hard Margin

Hard Margin은 단 하나의 오분류도 허용하지 않는 방식이다.

데이터가 선형적으로 완벽하게 분리될 수 있을 때만 사용할 수 있습니다.

이상치(Outlier)에 매우 민감하여 조금의 노이즈에도 결정 경계가 크게 흔들릴 수 있다는 단점이 있습니다.

$$
\begin{aligned}
\min{}{\frac{\lVert\mathbf{w}\rVert^2}{2}}~~~~~~~\\
s.t.~y(\mathbf{w}^\top\mathbf{x}+b)\geq1
\end{aligned}
$$

![fig2](ml/4-2.png){: style="display:block; margin:0 auto; width:50%;"}
_[[출처]](https://www.surveypractice.org/article/2715-using-support-vector-machines-for-survey-research)_

### Soft Margin

어느 정도의 오분류나 마진 경계를 침범하는 것을 허용하는 유연한 방식입니다.

'슬랙 변수(Slack Variable)'라는 개념을 도입하여 마진을 위반한 정도를 측정합니다.

이상치가 있거나 데이터가 선형적으로 분리되지 않는 현실적인 문제에 더 효과적입니다. C 파라미터를 통해 마진 오류를 얼마나 허용할지 조절할 수 있습니다.

![fig3](ml/4-3.png){: style="display:block; margin:0 auto; width:50%;"}
_[[출처]](https://www.surveypractice.org/article/2715-using-support-vector-machines-for-survey-research)_

## Support Vector Regression

