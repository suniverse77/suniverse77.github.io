---
title: "[앙상블] Bias-Variance Decomposition"
date: 2025-06-10 00:00:00 +/-TTTT
categories: [AI, 머신러닝]
tags: [앙상블]
math: true
toc: true
author: sunho
---

## 데이터 생성 과정

어떤 $x$를 입력했을 때 완벽하게 $y$를 출력하는 이상적인 메커니즘 $F^*(x)$가 있다고 가정하더라도, 현실 세계에서 관측되는 $(x,y)$ 쌍에는 사람이 통제할 수 없는 노이즈 $\epsilon$이 포함된다.

$$
y=F^*(x)+\epsilon~~,~~\epsilon\sim\mathcal{N}(0,\sigma^2)
$$

- $F^*(x)$는 우리가 학습을 통해 찾아야 하는 target function이지만, 실제로는 알 수 없는 경우가 많다.
- 노이즈 $\epsilon$은 모든 샘플에서 i.i.d (Independent and Identically Distributed)하다고 가정한다.

예를 들어, 아래 그림에서 직선을 $F^*(x)=\beta_0+\beta_1 x$라고 해보자.

$F^*(x)$는 입력 $x$에 대해 항상 동일한 $y$값을 출력한다. 하지만 실제로 관측되는 $y$에는 노이즈 $\epsilon$가 포함되어 있으며, 따라서 특정 $x$에서의 $y$ 값은 단일 점이 아니라 분포의 형태를 띤다.

![fig1](ml/10-1.png){: style="display:block; margin:0 auto; width:50%;"}
_[[출처]](https://junstar92.github.io/mml-study-note/2022/08/15/ch8-1.html)_

## Bias-Variance Decomposition

노이즈가 어떻게 주어지냐에 따라서 데이터셋이 달라질 수 있다.

아래 그림에서 $\hat{F}(x)$는 특정 데이터셋에 대해서 학습한 함수를 의미하며, 여러 데이터셋에 대해 학습한 함수를 평균한 값을 $\bar{F}(x)$라고 표현한다.

$$
\bar{F}(x)=\mathbb{E}\left[\hat{F}_D(x)\right]
$$

![fig2](ml/10-2.png){: style="display:block; margin:0 auto; width:60%;"}
_[[출처]](https://junstar92.github.io/mml-study-note/2022/08/15/ch8-1.html)_

MSE (Mean Square Error)를 가정했을 때, 특정한 데이터 포인트에서의 에러는 아래와 같이 표현할 수 있다.

$$
Err(x_0)=\mathbb{E}\vphantom{\Bigg(}\left[\left(y-\hat{F}(x_0)\right)^2\right]=
\textcolor{red}{\text{Bias}^2\left(\hat{F}(x_0)\right)}+\textcolor{blue}{\text{Var}\left(\hat{F}(x_0)\right)}+\sigma^2
$$

이를 Bias-Variance Decomposition이라고 한다.

<details>
<summary><font color='#0000FF'>공식 유도</font></summary>
<div markdown="1">

**1. $y=F^*(x)+\epsilon$ 치환**

$$
\mathbb{E}\vphantom{\Bigg(}\left[\left(y-\hat{F}(x_0)\right)^2\right]
=\mathbb{E}\left[\left(F^*(x_0)+\epsilon-\hat{F}(x_0)\right)^2\right]
$$
<br>

**2. $\mathbb{E}\left[(A+B)^2\right]=\mathbb{E}\left[A^2\right]+\mathbb{E}\left[B^2\right]+2\mathbb{E}\left[AB\right]$ 공식 사용**

$A=F^*(x_0)-\hat{F}(x_0)$ , $B=\epsilon$로 가정하고 전개

이때, $\epsilon\sim N(0,\sigma)$이므로, $\mathbb{E}[\epsilon]=0$이다.

$$
\mathbb{E}\vphantom{\Bigg(}\left[\left(F^*(x_0)-\hat{F}(x_0)+\epsilon\right)^2\right]
=\mathbb{E}\left[\left(F^*(x_0)-\hat{F}(x_0)\right)^2\right]
+\mathbb{E}\left[\epsilon^2\right]
=\mathbb{E}\left[\left(F^*(x_0)-\hat{F}(x_0)\right)^2\right]+\sigma^2
$$
<br>

**3. $\bar{F}(x_0)$를 더하고 뺌**

$$
\mathbb{E}\left[\left(F^*(x_0)-\hat{F}(x_0)\right)^2\right]
=\mathbb{E}\left[\left(F^*(x_0)-\bar{F}(x_0)+\bar{F}(x_0)-\hat{F}(x_0)\right)^2\right]+\sigma^2
$$
<br>

**4. $\mathbb{E}\left[(A+B)^2\right]=\mathbb{E}\left[A^2\right]+\mathbb{E}\left[B^2\right]+2\mathbb{E}\left[AB\right]$ 공식 사용**

$A=F^*(x_0)-\bar{F}(x_0)$ , $B=\bar{F}(x_0)-\hat{F}(x_0)$로 가정하고 전개

이때, $\mathbb{E}\left[\bar{F}(x_0)-\hat{F}(x_0)\right]=0$이다.

$$
\mathbb{E}\vphantom{\Bigg(}\left[\left(F^*(x_0)-\bar{F}(x_0)+\bar{F}(x_0)-\hat{F}(x_0)\right)^2\right]+\sigma^2
=\mathbb{E}\vphantom{\Bigg(}\left[\left(F^*(x_0)-\bar{F}(x_0)\right)^2\right]
+\mathbb{E}\left[\left(\bar{F}(x_0)-\hat{F}(x_0)\right)^2\right]+\sigma^2
$$
<br>

**5. Bias와 Variance로 표현**

$$
\mathbb{E}\vphantom{\Bigg(}\left[\left(F^*(x_0)-\bar{F}(x_0)\right)^2\right]
+\mathbb{E}\left[\left(\bar{F}(x_0)-\hat{F}(x_0)\right)^2\right]+\sigma^2
=\textcolor{red}{\text{Bias}^2\left(\hat{F}(x_0)\right)}+\textcolor{blue}{\text{Var}^2\left(\hat{F}(x_0)\right)}+\sigma^2
$$

---

</div>
</details>
<br>

![fig3](ml/10-3.png){: style="display:block; margin:0 auto; width:50%;"}
_[[출처]](https://github.com/pilsung-kang/Business-Analytics-IME654-/blob/master/04%20Ensemble%20Learning/04-2_Ensemble%20Learning_Bias-Variance%20Decomposition.pdf)_

**편향 (Bias)**

Bias는 모델이 예측한 평균값과 실제 정답 사이의 차이를 나타낸다.

Bias가 크다는 것은 모델이 잘못 학습해서, 데이터가 아무리 주어져도 정답에 근접하기 어렵다는 것을 의미한다. (High bias implies a poor match)

**분산 (Variance)**

Variance는 개별 값들이 그들의 평균으로부터 얼마나 떨어져 있는지를 나타낸다.

Variance가 크다는 것은 데이터셋이 조금만 달라져도 모델의 예측 결과가 크게 달라진다는 것을 의미한다. (High variance implies a weak match)

아래 그림에서 $F^*(x)$와 $\bar{F}(x)$의 차이가 bias에 해당하며, $\bar{F}(x)$ 주변에 퍼져있는 노란색 반경이 variance에 해당한다고 볼 수 있다.

![fig4](ml/10-4.png){: style="display:block; margin:0 auto; width:50%;"}
_[[출처]](https://github.com/pilsung-kang/Business-Analytics-IME654-/blob/master/04%20Ensemble%20Learning/04-2_Ensemble%20Learning_Bias-Variance%20Decomposition.pdf)_

### Bias-Variance Dilemma

모델을 단순하게 만들면 variance는 작아지지만 bias가 커지고, 모델을 복잡하게 만들면 bias는 작아지지만 variance가 커진다.

즉, bias와 variance는 반비례 관계라서 항상 균형이 필요하다.

## 앙상블 (Ensemble)

앙상블은 개별 모델보다 더 높은 성능을 얻기 위해, 여러 개의 모델을 결합하여 하나의 최종 예측을 만드는 방법이다.

앙상블 기법은 주로 Bias와 Variance를 감소시키는 방법으로 나눌 수 있다.

- **Bagging**: 개별 모델의 변동성을 줄여 <span style="background-color:#fff5b1">variance를 감소</span>시킨다.
- **Boosting**: 약한 모델을 순차적으로 결합하면서 정답에 점점 더 가까워져 <span style="background-color:#fff5b1">bias를 감소</span>시킨다.

앙상블에서 중요한 것은 개별적인 모델이 충분한 다양성을 가지게 하는 것이다.

- **Implicit diversity**: 서로 다른 데이터셋을 사용하면 자연스럽게 모델 예측도 달라진다는 가정 (Bagging)
- **Explicit diversity**: 학습 과정에서 명시적으로 이전 모델과 다른 예측을 하도록 유도하는 방법 (Boosting)

아래 그림에서 왼쪽이 Bagging 계열의 알고리즘, 오른쪽이 Boosting 계열의 알고리즘을 나타낸다.

![fig5](ml/10-5.png){: style="display:block; margin:0 auto; width:80%;"}
_[[출처]](https://github.com/pilsung-kang/Business-Analytics-IME654-/blob/master/04%20Ensemble%20Learning/04-2_Ensemble%20Learning_Bias-Variance%20Decomposition.pdf)_

왼쪽은 병렬 처리가 가능하고, 오른쪽은 순차 처리만 가능하다.

하지만 개별 모델이 매우 클 수 있기 때문에, 반드시 병렬 처리를 하는 Bagging이 더 빠르다고 볼 수는 없다.

### 앙상블 사용 이유

앙상블에서 $m$번째 모델의 예측 결과를 $y_m(\mathbf{x})=f(\mathbf{x})+\epsilon_m(\mathbf{x})$으로 표현할 때, MSE는 아래와 같이 표현할 수 있다.

$$
\mathbb{E}_{\mathbf{x}}\left[y_m(\mathbf{x})-f(\mathbf{x})\right]^2
=\mathbb{E}_{\mathbf{x}}\left[\epsilon_m(\mathbf{x})\right]^2
$$

$M$개의 개별 모델의 예측에 대한 MSE는 아래와 같이 표현할 수 있다.

$$
E_{Avg}=\frac{1}{M}\sum_{m=1}^M\mathbb{E}_{\mathbf{x}}\left[\epsilon_m(\mathbf{x})\right]^2
$$

앙상블에서의 MSE는 아래와 같이 표현할 수 있다.

$$
E_{Ensemble}=\mathbb{E}_{\mathbf{x}}
\left[\left\{
\left(\frac{1}{M}\sum_{m=1}^My_m(\mathbf{x})\right)-f(\mathbf{x})
\right\}^2\right]_{\vphantom{\Bigg(}}
$$

이론적으로 개별 모델이 완전히 독립적이라면, 아래의 식이 성립한다.

$$
E_{Ensemble}=\frac{1}{M}E_{Avg}
$$

즉, 앙상블의 에러는 개별 모델을 단독으로 사용할 때보다 항상 작거나 같다.

$$\vphantom{\Big(}
E_{Ensemble}\leq E_{Avg}
$$

<details>
<summary><font color='#0000FF'>공식 유도</font></summary>
<div markdown="1">

앙상블에서의 MSE 식을 아래와 같이 정리할 수 있다.

$$
\begin{aligned}
\left(\frac{1}{M}\sum_{m=1}^My_m(\mathbf{x})\right)-f(\mathbf{x})
=\frac{1}{M}\sum_{m=1}^M\left(f(\mathbf{x})+\epsilon_m(\mathbf{x})\right)-f(\mathbf{x})\\
=f(\mathbf{x})+\frac{1}{M}\sum_{m=1}^M\epsilon_m(\mathbf{x})-f(\mathbf{x})~~~\\
=\frac{1}{M}\sum_{m=1}^M\epsilon_m(\mathbf{x})~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\end{aligned}
$$

$$
E_{Ensemble}=\mathbb{E}_{\mathbf{x}}
\left[\left(
\frac{1}{M}\sum_{m=1}^M\epsilon_m(\mathbf{x})
\right)^2\right]_{\vphantom{\Bigg(}}
$$

에러의 평균이 0이고, 에러끼리 상관관계가 없다고 가정한다.

$$
\mathbb{E}_{\mathbf{x}}\left[\epsilon_m(\mathbf{x})\right]=0
~~,~~\mathbb{E}_{\mathbf{x}}\left[\epsilon_m(\mathbf{x})\epsilon_l(\mathbf{x})\right]=0
$$

코시 슈바르츠 (Cauchy–Schwarz inequality) 부등식에 의해 아래가 성립한다.

$$
\left[\sum_{m=1}^M\epsilon_m(\mathbf{x})\right]^2
\leq M\sum_{m=1}^M\epsilon_m(\mathbf{x})^2
~\to~
\left[\frac{1}{M}\sum_{m=1}^M\epsilon_m(\mathbf{x})\right]^2
\leq \frac{1}{M}\sum_{m=1}^M\epsilon_m(\mathbf{x})^2
$$

---

</div>
</details>
