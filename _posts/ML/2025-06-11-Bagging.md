---
title: "[회귀] Bagging"
date: 2025-06-11 00:00:00 +/-TTTT
categories: [AI, 머신러닝]
tags: [앙상블]
math: true
toc: true
author: sunho
---

## K-fold Cross Validation

학습 데이터셋을 K개의 부분집합 (fold라고 부름)으로 나눈다.

학습을 K번 반복하면서 매번 1개의 fold는 검증용으로, 나머지 K-1개의 fold는 학습용으로 사용한다. 이후 최종 성능은 K번의 결과를 평균하여 평가한다.

![fig1](ml/Bagging-1.png){: style="display:block; margin:0 auto; width:90%;"}
_[[출처]](https://towardsdatascience.com/how-to-cross-validation-with-time-series-data-9802a06272c6/)_

데이터의 낭비가 없지만, 각 학습 세트가 항상 K-2개의 fold를 공유하기 때문에 모델 간 완전한 독립성이 부족하다는 단점이 있다.

## Bagging (Bootstrap Aggregating)

원본 데이터셋에서 복원 추출 (Bootstrap sampling)을 통해 여러 개의 학습 데이터셋을 생성한다.
즉, 한 번 뽑은 데이터를 다시 뽑을 수 있다.

생성된 각 데이터셋으로 독립적인 모델을 학습한 후, 최종적으로 결과를 합친다.

![fig2](ml/Bagging-2.png){: style="display:block; margin:0 auto; width:90%;"}
_[[출처]](https://github.com/pilsung-kang/Business-Analytics-IME654-/blob/master/04%20Ensemble%20Learning/04-3_Ensemble%20Learning_Bagging.pdf)_

각 모델은 서로 다른 데이터셋으로 학습하기 때문에 모델 간 상관관계가 줄어든다.

이러한 특징 때문에 분산이 큰 모델 (Decision Tree, SVM 등)에 효과적이다.

### Aggregating

각 모델의 결과를 집계하는 방식에는 여러 가지가 있다.

![fig3](ml/Bagging-3.png){: style="display:block; margin:0 auto; width:70%;"}
_[[출처]](https://github.com/pilsung-kang/Business-Analytics-IME654-/blob/master/04%20Ensemble%20Learning/04-3_Ensemble%20Learning_Bagging.pdf)_

**Majority Voting**



**Weighted Voting**



## Stacking

Stacking은 각 모델이 예측한 결과를 

![fig4](ml/Bagging-4.png){: style="display:block; margin:0 auto; width:90%;"}
_[[출처]](https://www.nb-data.com/p/comparing-model-ensembling-bagging)_

## Random Forest

Bagging의 특수한 형태로, Decision Tree를 기반으로 한다.
