---
title: "[앙상블] Bagging"
date: 2025-06-11 00:00:00 +/-TTTT
categories: [AI, 머신러닝]
tags: [앙상블]
math: true
toc: true
author: sunho
---

## K-fold Cross Validation

학습 데이터셋을 K개의 부분집합 (fold라고 부름)으로 나눈다.

학습을 K번 반복하면서 매번 1개의 fold는 검증용으로, 나머지 K-1개의 fold는 학습용으로 사용한다. 이후 최종 성능은 K번의 결과를 평균하여 평가한다.

![fig1](ml/11-1.png){: style="display:block; margin:0 auto; width:90%;"}
_[[출처]](https://towardsdatascience.com/how-to-cross-validation-with-time-series-data-9802a06272c6/)_

데이터의 낭비가 없지만, 각 학습 세트가 항상 K-2개의 fold를 공유하기 때문에 모델 간 완전한 독립성이 부족하다는 단점이 있다.

## Bagging (Bootstrap Aggregating)

원본 데이터셋에서 복원 추출을 통해 여러 개의 학습 데이터셋 (Bootstrap)을 생성한다.
즉, 한 번 뽑은 데이터를 다시 뽑을 수 있으며, 이를 Bootstrap Sampling이라고 부른다.

생성된 각 데이터셋으로 독립적인 모델을 학습한 후, 최종적으로 결과를 합친다.

아래 그림을 보면 Bootstrap마다 선택된 샘플이 다르며, 복원 추출을 하기 때문에 중복해서 뽑힌 샘플이 있는 것도 확인할 수 있다.

![fig2](ml/11.png){: style="display:block; margin:0 auto; width:80%;"}
_[[출처]](https://github.com/pilsung-kang/Business-Analytics-IME654-/blob/master/04%20Ensemble%20Learning/04-3_Ensemble%20Learning_Bagging.pdf)_

각 모델은 서로 다른 데이터셋으로 학습하기 때문에 모델 간 상관관계가 줄어든다.

이러한 특징 때문에 <span style="background-color:#fff5b1">분산이 큰 모델 (Decision Tree, SVM 등)에 효과적이다.</span>

### Aggregating

각 모델의 결과를 집계하는 방식에는 여러 가지가 있다.

아래 그림에서 빨간색 박스는 각 모델이 $y=1$로 예측한 확률을, 노란색 박스는 확률 $0.5$를 기준으로 클래스를 예측한 결과를 의미한다.

![fig3](ml/11-3.png){: style="display:block; margin:0 auto; width:50%;"}
_[[출처]](https://github.com/pilsung-kang/Business-Analytics-IME654-/blob/master/04%20Ensemble%20Learning/04-3_Ensemble%20Learning_Bagging.pdf)_

**Majority Voting (Hard Voting)**

단순하게, 가장 많은 표를 받은 클래스를 최종 예측으로 선택하는 방식이다.

$$
\hat{y}=\underset{i}{\arg\max}\left(\sum_{j=1}^n\delta(\hat{y}_j=i)\right)
$$

여기서 $\delta$는 입력값이 True면 1, False면 0을 출력하는 함수이다.

위의 예시에서 4개의 모델이 $\hat{y}=0$이라고 예측했고, 6개의 모델이 $\hat{y}=1$이라고 예측했으므로, Majority Voting에 의한 최종 예측 클래스는 $1$이다.

$$
\sum_{j=1}^{10}\delta(\hat{y}_j=0)=4~~,~~\sum_{j=1}^{10}\delta(\hat{y}_j=1)=6
~~\to~~\hat{y}=1
$$

**Weighted Voting (Soft Voting)**

각 모델의 투표 결과에 가중치를 적용한 후, 최종 예측을 하는 방식이다. 일반적으로 각 모델의 예측 확률을 가중치로 사용한다.

$$
\hat{y}=\underset{i}{\arg\max}\left(\frac{1}{n}\sum_{j=1}^nP(\hat{y}_j=i)\right)
$$

위의 예시에서 $\hat{y}=0$이라고 예측한 모델의 가중합은 $0.213$이고, $\hat{y}=1$이라고 예측한 모델의 가중합은 $0.787$이다. 따라서 Weighted Voting에 의한 최종 예측 클래스는 $1$이다.

$$
\frac{1}{10}\sum_{j=1}^{10}P(\hat{y}_j=0)=
\frac{0.10+0.08+0.13+0.34+0.41+0.16+0.14+0.32+0.02+0.43}{10}=0.213
$$

$$
\frac{1}{10}\sum_{j=1}^{10}P(\hat{y}_j=1)=\frac{0.90+0.92+0.87+0.66+0.59+0.84+0.86+0.68+0.98+0.57}{10}=0.787
$$

## Stacking

Stacking은 각 모델의 예측 결과를 이용해 메타 모델이 다시 학습해서 최종 예측 결과를 내는 것을 의미한다.

![fig4](ml/11-4.png){: style="display:block; margin:0 auto; width:70%;"}
_[[출처]](https://medium.com/data-and-beyond/stacking-ensemble-technique-in-ml-6d0a98a12607)_

## Random Forest

랜덤 포레스트는 Bagging의 특수한 형태로, 여러 개의 [Decision Tree](https://suniverse77.github.io/posts/DT/)를 학습시키는 앙상블 학습 방법이다.

랜덤 포레스트에서는 Bootstrap Sampling뿐만 아니라 Feature Sampling도 수행하여 개별 모델을 구성한다. 알고리즘 동작 원리는 아래와 같다.

1. **Bootstrap Sampling**

    원본 데이터셋에서 복원 추출을 통해, 각 Tree가 서로 다른 데이터셋을 통해 학습
2. **Feature Sampling**

    Tree의 각 분기에서 모든 특징을 사용하는 것이 아니라, 일부 특징만 무작위로 선택해서 사용
3. **Decision Tree 학습**

    샘플링된 데이터와 특징을 사용해 각각의 Decision Tree를 독립적으로 학습
4. **Aggregation**

    각 Tree의 예측 결과를 집계하여 최종 예측 수행

![fig5](ml/11-5.png){: style="display:block; margin:0 auto; width:70%;"}
_[[출처]](https://onceadayedu.tistory.com/107)_
