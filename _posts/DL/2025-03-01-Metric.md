---
title: "[평가지표] Metric"
date: 2025-03-01 00:00:00 +/-TTTT
categories: [AI, 딥러닝]
tags: [3D Vision]
math: true
toc: true
author: sunho
---

## Classification 평가 지표

분류 평가 지표는 모델의 예측이 실제 정답과 얼마나 일치하는지 측정하며, 혼동 행렬 (Confusion Matrix)을 기반으로 한다.

![fig1](dl/metric/1-1.png){: style="display:block; margin:0 auto; width:60%;"}
_[[출처]](https://www.blog.trainindata.com/confusion-matrix-precision-and-recall/)_

**Positive/Negative**는 모델이 예측한 값을, **True/False**는 그 예측이 맞았는지 틀렸는지를 의미한다.

- **TP (True Positive)** : 실제 **Positive**인 것을 **Positive**라고 올바르게 예측한 경우
- **TN (True Negative)** : 실제 **Negative**인 것을 **Negative**라고 올바르게 예측한 경우
- **FP (False Positive)** : 실제 **Negative**인 것을 **Positive**라고 잘못 예측한 경우
- **FN (False Negative)** : 실제 **Positive**인 것을 **Negative**라고 잘못 예측한 경우

<details>
<summary><font color='#FF0000'>Example 1</font></summary>
<div markdown="1">



</div>
</details>

![fig2](dl/metric/1-2.png){: style="display:block; margin:0 auto; width:80%;"}
_[[출처]](https://velog.io/@jjw9599/ConfusionMatrix-ClassificationEvaluation)_

### 정확도 (Accuracy)

Accuracy는 전체 데이터 중에서 모델이 올바르게 예측한 비율을 의미한다.

$$
\text{Accuracy}=\frac{\text{TP}+\text{TN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}}
$$

데이터가 불균형할 때 성능을 제대로 평가하기 어렵다는 단점이 있다.

예를 들어 99%가 정상, 1%가 불량인 경우, 모델이 전부 정상으로 예측해도 99%의 정확도가 나오게 된다.

### 정밀도 (Precision)

Precision은 모델이 Positive라고 예측한 것들 중에서, 실제로 Positive인 것의 비율을 의미한다.

$$
\text{Precision}=\frac{\text{TP}}{\text{TP}+\text{FP}}
$$

모델의 예측(Positive)을 신뢰해야 할 때 중요합니다.
    
예를 들어,  스팸 메일 분류기. FP가 치명적인 경우. 즉, 정상 메일을 스팸으로 분류하면 안 될 때)

### 재현율 (Recall, Sensitivity)

설명: 실제 "Positive"인 것들 중에서, 모델이 "Positive"라고 예측해낸(찾아낸) 비율입니다.

$$
\text{Recall}=\frac{\text{TP}}{\text{TP}+\text{FN}}
$$

실제 Positive인 데이터를 놓치면 안 될 때 중요합니다.

예를 들어, 암 진단 모델. FN이 치명적인 경우. 즉, 실제 암 환자를 정상으로 진단하면 안 될 때)

### F1 Score

F1 Score는 Precision과 Recall의 조화 평균이다.

$$
\text{F1 Score}=2\times\frac{\text{Precision}\times\text{Recall}}{\text{Precision}+\text{Recall}}
$$

데이터가 불균형할 때 Accuracy 대신 사용하며, Precision과 Recall 모두 중요할 때 사용한다.

### ROC-AUC (Area Under the Curve)

ROC 곡선은 모델의 임계값(Threshold) 변화에 따라 **FPR(False Positive Rate)**이 변할 때 **TPR(True Positive Rate, =Recall)**이 어떻게 변하는지를 그린 곡선입니다.

AUC는 이 ROC 곡선 아래의 면적을 의미하며, 1에 가까울수록 모델의 성능(판별력)이 좋다고 봅니다.

![fig3](dl/metric/1-3.png){: style="display:block; margin:0 auto; width:80%;"}
_[[출처]](https://www.blog.trainindata.com/auc-roc-analysis/)_

## Object Detection & Segmentation 평가 지표

### IoU (Intersection over Union)

### mAP (mean Average Precision)

## 생성 모델 평가 지표

### IS (Inception Score)

### FID (Fréchet Inception Distance)