---
title: "[평가지표] Metric"
date: 2025-03-01 00:00:00 +/-TTTT
categories: [AI, 딥러닝]
tags: [3D Vision]
math: true
toc: true
author: sunho
---

## Classification 평가 지표

분류 평가 지표는 모델의 예측이 실제 정답과 얼마나 일치하는지 측정하며, 혼동 행렬 (Confusion Matrix)을 기반으로 한다.

![fig1](dl/metric/1-1.png){: style="display:block; margin:0 auto; width:60%;"}
_[[출처]](https://www.blog.trainindata.com/confusion-matrix-precision-and-recall/)_

**Positive/Negative**는 모델이 예측한 값을, **True/False**는 그 예측이 맞았는지 틀렸는지를 의미한다.

- **TP (True Positive)** : 실제 **Positive**인 것을 **Positive**라고 올바르게 예측한 경우
- **TN (True Negative)** : 실제 **Negative**인 것을 **Negative**라고 올바르게 예측한 경우
- **FP (False Positive)** : 실제 **Negative**인 것을 **Positive**라고 잘못 예측한 경우
- **FN (False Negative)** : 실제 **Positive**인 것을 **Negative**라고 잘못 예측한 경우

<details>
<summary><font color='#FF0000'>Example 1</font></summary>
<div markdown="1">



</div>
</details>
<br>

![fig2](dl/metric/1-2.png){: style="display:block; margin:0 auto; width:80%;"}
_[[출처]](https://velog.io/@jjw9599/ConfusionMatrix-ClassificationEvaluation)_

### 정확도 (Accuracy)

Accuracy는 전체 데이터 중에서 모델이 올바르게 예측한 비율을 의미한다.

$$
\text{Accuracy}=\frac{\text{TP}+\text{TN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}}
$$

데이터가 불균형할 때 성능을 제대로 평가하기 어렵다는 단점이 있다.

예를 들어 99%가 정상, 1%가 불량인 경우, 모델이 전부 정상으로 예측해도 99%의 정확도가 나오게 된다.

### 정밀도 (Precision)

Precision은 모델이 **Positive**라고 예측한 것들 중에서, 실제로 **Positive**인 것 (정답을 맞춤)의 비율을 의미한다.

$$
\text{Precision}=\frac{\text{TP}}{\text{TP}+\text{FP}}
$$

이 지표는 모델이 **Positive** 예측을 얼마나 잘 수행했는지를 나타내며, <span style="background-color:#fff5b1">실제 **Negative**인 데이터를 잘못 잡으면 안될 때</span> 중요하다. 즉, $\text{FP}$가 치명적인 경우에 중요하다.
    
스팸 메일 분류기를 예로 들 수 있다.

메일 분류기는 정상 메일 (**Negative**)을 스팸 (**Positive**)으로 분류하면 안된다. 만약, 정상 메일을 스팸으로 분류한다면 $\text{FP}$가 증가하게 된다.

### 재현율 (Recall, Sensitivity)

Recall은 실제 **Positive**인 것들 중에서, 모델이 **Positive**라고 올바르게 찾아낸 비율을 의미한다.

$$
\text{Recall}=\frac{\text{TP}}{\text{TP}+\text{FN}}
$$

이 지표는 모델이 실제 **Positive** 데이터를 얼마나 빠짐없이 잘 찾아내는가를 나타내며, <span style="background-color:#fff5b1">실제 **Positive**인 데이터를 놓치면 안 될 때</span> 중요하다. 즉, 잘못 예측한 $\text{FN}$이 치명적인 경우에 중요하다.

암 진단 모델을 예로 들 수 있다.

암 진단 모델은 실제 암 환자 (**Positive**)를 정상 (**Negative**)으로 진단하면 안된다. 만약, 암 환자를 정상으로 진단한다면 $\text{FN}$이 증가하게 된다.

### F1 Score

F1 Score는 Precision과 Recall의 조화 평균이다.

$$
\text{F1 Score}=2\times\frac{\text{Precision}\times\text{Recall}}{\text{Precision}+\text{Recall}}
$$

데이터가 불균형할 때 Accuracy 대신 사용하며, Precision과 Recall 모두 중요할 때 사용한다.

### AUC-ROC Curve

ROC (Receiver Operating Characteristic) 곡선은 모델의 임계값 변화에 따라 **FPR (False Positive Rate)**이 변할 때 **TPR (True Positive Rate, Recall)**이 어떻게 변하는지를 그린 곡선이다.

AUC (Area Under the Curve)는 이 ROC 곡선 아래의 면적을 의미하며, 값이 $1$에 가까울수록 모델의 성능이 좋다고 본다.

![fig3](dl/metric/1-3.png){: style="display:block; margin:0 auto; width:80%;"}
_[[출처]](https://www.blog.trainindata.com/auc-roc-analysis/)_

## Object Detection & Segmentation 평가 지표

### IoU (Intersection over Union)

IoU는 모델이 예측한 영역과 실제 정답 영역이 얼마나 겹치는지를 나타내는 지표이며, Object Detection과 Semantic Segmentation에서 사용된다.

$$
\text{IoU}=\frac{\text{Area of Overlap}}{\text{Area of Union}}
$$

$0\sim1$ 사이의 값을 가지며, $1$이면 완벽히 일치한다는 의미이다.

Object Detection에서는 이 IoU 값이 특정 임계값을 넘어야 올바르게 탐지했다고 판단한다.

![fig4](dl/metric/2-1.png){: style="display:block; margin:0 auto; width:80%;"}
_[[출처]](https://glee1228.tistory.com/5)_

#### mIoU (mean IoU)

mIoU는 모든 클래스의 IoU 값들을 더해 평균낸 값을 의미한다.

$$
\text{mIoU}=\frac{1}{C}\sum_{i=1}^C\text{IoU}_i
$$

여기서 $C$는 클래스 개수를 의미한다.

### AP (Average Precision)

AP는 하나의 클래스에 대해 모델이 얼마나 정밀하게 탐지하는지를 종합적으로 나타내는 지표이며, Object Detection에서 사용된다.

AP는 PR 곡선 (Precision-Recall Curve) 곡선의 아래쪽 면적을 계산한 값이다.

![fig5](dl/metric/2-2.png){: style="display:block; margin:0 auto; width:80%;"}
_[[출처]](https://ctkim.tistory.com/entry/mAPMean-Average-Precision-%EC%A0%95%EB%A6%AC)_

#### mAP (mean Average Precision)

mAP는 모든 클래스의 AP 값들의 평균을 의미한다.

$$
\text{mAP}=\frac{1}{C}\sum_{i=1}^C\text{AP}_i
$$

여기서 $C$는 클래스 개수를 의미한다.

## 생성 모델 평가 지표

### IS (Inception Score)

### FID (Fréchet Inception Distance)