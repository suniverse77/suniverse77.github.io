---
title: "[경량화] PEFT (Parameter-Efficient Fine-Tuning)"
date: 2025-10-20 00:00:00 +/-TTTT
categories: [AI, 딥러닝]
tags: [경량화]
math: true
toc: true
author: sunho
---

## PEFT (Parameter-Efficient Fine-Tuning)

GPT-3, LLaMA, Stable Diffusion과 같은 수십억 개 이상의 파라미터를 가진  사전 학습 모델을 통째로 파인튜닝하면, 매우 많은 GPU 메모리와 긴 학습 시간이 필요하다.

또한 이미 학습된 모델을 새로운 task에 맞춰 파인튜닝하면, 이전에 학습했던 지식을 잊어버리는 치명적 망각 (Catastrophic Forgetting) 현상이 발생할 수 있다.

따라서 특정 task에 맞게 파인튜닝할 때, 모델의 모든 매개변수를 훈련시키는 대신 극히 일부의 매개변수만 수정하거나 소수의 새로운 매개변수를 추가하여 훈련시키는 기법들이 등장하였고, 이를 통칭하여 PEFT라고 부른다.

아래 그림은 PEFT 기법의 분류 (taxonomy)를 나타내며, 사용한 방법에 따라 크게 3가지로 분류하였다.

![fig1](dl/lightweight/1-1.png){: style="display:block; margin:0 auto; width:70%;"}
_출처: [Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.15647)_

- **Addition-based methods**: 기존 LLM의 가중치는 완전히 동결시키고, 학습 가능한 '새로운' 파라미터나 모듈을 모델에 '추가'하여 이 추가된 부분만 학습하는 방식
- **Selection-based methods**: 새로운 파라미터를 추가하지 않습니다. 대신, 모델이 원래 가지고 있던 수많은 파라미터 중 '일부'만 (예: 특정 레이어의 bias 항만) '선택'하여 해당 부분만 업데이트하는 방식
- **Reparametrization-based methods**: 기존 가중치의 변화량을 직접 학습하지 않고, 이를 더 적은 수의 파라미터로 효율적이게 표현(재매개변수화)하여 학습하는 방식

## LoRA (Low-Rank Adaptation)

[*LoRA: Low-Rank Adaptation of Large Language Models*](https://arxiv.org/abs/2106.09685) 연구에서 제안한 방법으로, LLM을 효율적으로 파인튜닝하기 위한 기법이다.

사전 학습된 가중치를 $W_0$라고 표현할 때, 파인튜닝 과정은 아래와 같이 표현할 수 있다.

$$
W_0+\Delta W
$$

만약 Full Fine-Tuning한다면, $\Delta W$는 $W_0$와 동일한 개수의 파라미터를 업데이트할 수 있기 때문에 계산 비용이 매우 커진다. 즉, $\Delta W$가 Full-Rank가 될 수 있다.

LoRA의 핵심 가정은 파인튜닝할 때 업데이트되는 가중치 변화량 $\Delta W$가 실제로는 복잡하지 않고, Low-Rank 구조를 가질 것이라는 점이다.

이러한 가정은 LLM이 이미 방대한 언어 지식을 보유하고 있고, 파인튜닝은 새로운 지식을 다시 학습하는 것이 아니라, 기존의 지식을 특정 목적에 맞게 조율만하는 과정이라는 아이디에서 기반하였다.

LoRA는 $\Delta W$를 직접 학습하는 대신, 이를 두 개의 Low-Rank 행렬 $B$와 $A$의 곱으로 근사한다.

$$
\begin{aligned}
W_0+\Delta W=W_0+BA~~~~~~~~~~~~~~~~~~~~\\
W_0\in\mathbb{R}^{d\times k},~B\in\mathbb{R}^{d\times r},~A\in\mathbb{R}^{r\times k},~r\leq\min(d,k)
\end{aligned}
$$

파인튜닝 과정에서는 원본 가중치 $W_0$는 동결되고 $A$와 $B$ 행렬만 학습되기 때문에, 학습 파라미터 수가 줄어든다.

- $\Delta W$는 전체를 학습하기 때문에, 업데이할 파라미터 개수가 $d\times k$개이다.
- $B$와 $A$는 업데이트할 파라미터 개수가 $r(d+k)$개이다.

해당 연구에서는 학습 안정성을 위해 $A$는 가우시안 분포로, $B$는 0으로 초기화하여 학습 초기에는 $\Delta W = 0$이 되도록 하였다.

또한 추론 시에는 $W' = W_0 + BA$ 연산을 통해 두 가중치를 미리 하나의 행렬로 병합할 수 있기 때문에, 원본 모델 대비 추가적인 추론 지연 (latency)이 발생하지 않는다.

![fig2](dl/lightweight/1-2.png){: style="display:block; margin:0 auto; width:70%;"}
_출처: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)_

## Adapter

거대한 원본 모델은 그대로 두고, 그 사이에 작은 plug-in 모듈을 삽입하여 훈련하는 기법이다. 이 plug-in 모듈을 Adapter라고 한다.

### Series Adapter

### Parallel Adapter

### Hybrid Adapter


