---
title: "[경량화] PEFT (Parameter-Efficient Fine-Tuning)"
date: 2025-10-20 00:00:00 +/-TTTT
categories: [AI, 딥러닝]
tags: [경량화]
math: true
toc: true
author: sunho
---

## PEFT (Parameter-Efficient Fine-Tuning)

GPT-3, LLaMA, Stable Diffusion과 같은 수십억 개 이상의 파라미터를 가진  사전 학습 모델을 통째로 파인튜닝하면, 매우 많은 GPU 메모리와 긴 학습 시간이 필요하다.

또한 이미 학습된 모델을 새로운 task에 맞춰 파인튜닝하면, 이전에 학습했던 지식을 잊어버리는 치명적 망각 (Catastrophic Forgetting) 현상이 발생할 수 있다.

따라서 특정 task에 맞게 파인튜닝할 때, 모델의 모든 매개변수를 훈련시키는 대신 극히 일부의 매개변수만 수정하거나 소수의 새로운 매개변수를 추가하여 훈련시키는 기법들이 등장하였고, 이를 통칭하여 PEFT라고 부른다.

PEFT에는 LoRA, Adapter 등의 기법이 존재한다.

## LoRA (Low-Rank Adaptation)

[*LoRA: Low-Rank Adaptation of Large Language Models*](https://arxiv.org/abs/2106.09685) 연구에서 제안한 방법이다.

LoRA에서는 파인튜닝 가중치 $\Delta W$는 복잡하지 않고 매우 단순할 것이라고 가정하였다. 즉, $\Delta W$가 실제로는 Low-Rank라고 가정하였다.

이는 LLM이 이미 언어에 대한 대부분의 지식을 알고 있고, 파인튜닝하는 것은 새로운 언어 지식을 다시 학습시키는 것이 아니라, 이미 아는 지식을 '특정 스타일'이나 '목적'에 맞게 조율(adapt)만하는 것이기 때문이다.

사전 학습된 가중치를 $W_0$라고 표현할 때, 파인튜닝 과정은 $W_0+\Delta W$로 표현할 수 있다. $\Delta W$를 2개의 Low-Rank 행렬 $A,B$로 분리하여, $A,B$만 학습시킨다.

$$
\begin{aligned}
W_0+\Delta W=W_0+BA~~~~~~~~~~~~~~~~~~~~\\
W_0\in\mathbb{R}^{d\times k},~B\in\mathbb{R}^{d\times r},~A\in\mathbb{R}^{r\times k},~r\leq\min(d,k)
\end{aligned}
$$

- $\Delta W$는 full-rank이기 때문에 업데이트할 파라미터 개수가 $d\times k$개이다.
- $B$와 $A$는 low-rank이기 때문에 업데이트할 파라미터 개수가 $r(d+k)$개이다.

![fig1](dl/lightweight/1-1.png){: style="display:block; margin:0 auto; width:70%;"}
_출처: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)_

## Adapter

### Series Adapter

### Parallel Adapter

### Hybrid Adapter


