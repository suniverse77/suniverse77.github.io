---
title: "[NN] 손실 함수 (Loss Function)"
date: 2025-02-01 06:00:00 +/-TTTT
categories: [AI, 딥러닝]
tags: [신경망]
math: true
toc: true
author: sunho
---

## 손실 함수 (Loss Function)

손실 함수는 

아래 손실 함수의 표기에서 $N$은 샘플의 개수, $i$는 샘플 인덱스를 의미한다.

## 오차 기반 손실 함수

두 값의 차이를 측정하며, 주로 <span style="background-color:#fff5b1">회귀 문제</span>에서 사용한다.

### MSE (Mean Squared Error)

오차 제곱의 평균이다.

$$
L_{MSE}=\frac{1}{N}\sum_{i=1}^N\left(y_i-\hat{y}_i\right)^2
$$

위의 식에서 $y$는 정답값, $\hat{y}$은 예측값을 의미한다.

제곱에 의해 오차가 클수록 loss값이 증가하기 때문에 이상치에 민감하다.

### MAE (Mean Absolute Error)

오차 절댓값의 평균이다.

$$
L_{MAE}=\frac{1}{N}\sum_{i=1}^N\lvert y_i-\hat{y}_i\rvert
$$

위의 식에서 $y$는 정답값, $\hat{y}$은 예측값을 의미한다.

오차가 커져도 loss값은 선형적으로 증가하여 이상치에 덜 민감하다.

## 마진 기반 손실 함수

결정 경계에서 얼마나 여유 있게 떨어져 있는가를 측정하며, 주로 <span style="background-color:#fff5b1">분류 문제</span>에서 사용한다.

### Hinge Loss

주로 [SVM](https://suniverse77.github.io/posts/SVM/)에서 사용된다. 이때 정답 클래스 라벨은 $-1,+1$로 설정한다.

$$
L_{Hinge}=\frac{1}{N}\sum_{i=1}^N\max(0,1-y_i\cdot\hat{y}_i)
$$

위의 식에서 $y$는 정답 라벨, $\hat{y}$은 예측 score를 의미한다.

정답을 맞추고, 경계선에서 1만큼의 마진을 확보한 경우 $(y\cdot\hat{y} \ge1)$, loss값은 $0$이다.

정답을 틀렸거나, 맞혀도 마진 안에 존재하는 경우, $1 - y \cdot \hat{y}$ 만큼의 손실을 받는다.

## 확률 기반 손실 함수

두 확률 분포의 차이를 측정하며, 주로 <span style="background-color:#fff5b1">분류 문제</span>에서 사용한다.

### Binatry Cross-Entropy Loss

이진 분류 문제에 사용된다. 이때 정답 클래스 라벨은 $0,1$로 설정한다.

$$
L_{BCE}=-\frac{1}{N}\sum_{i=1}^N\left[y_i\log(\hat{y}_i)-(1-y_i)\log(1-\hat{y}_i)\right]
$$

위의 식에서 $y$는 정답 라벨, $\hat{y}$은 모델이 $1$이라고 예측할 확률을 의미한다.

모델이 틀린 예측을 높은 확신으로 할수록, loss값이 기하급수적으로 증가한다.

<details>
<summary><font color='#FF0000'>Example 1</font></summary>
<div markdown="1">

**모델이 잘 맞춘 경우: 실제 정답이 $0$, 예측은 $0$**

$$
L_{BCE}=-0\log1-(1-0)\log(1-0)=\log(1)=0
$$


**모델이 틀린 경우:  실제 정답이 $0$, 예측은 $1$**

$$
L_{BCE}=-0\log1-(1-0)\log(1-1)=\log(0)=\infty
$$

---

</div>
</details>

### Categorical Cross-Entropy Loss

다중 클래스 분류 문제에 사용된다.

$$
L_{CCE}=-\frac{1}{N}\sum_{i=1}^N\sum_{k=1}^{C}y_{ik}\log(\hat{y}_{ik})
$$

위의 식에서 $y_k$는 $\hat{y}_k$는

위의 식에서 $p$는 정답 확률 분포, $q$는 예측 확률 분포를 의미한다. $C$는 전체 클래스의 개수를 의미한다.

<details>
<summary><font color='#FF0000'>Example 2</font></summary>
<div markdown="1">

**모델이 잘 맞춘 경우**

$$
p=\begin{bmatrix}1\\0\\0\end{bmatrix}~~,~~
q=\begin{bmatrix}0.8\\0.2\\0.1\end{bmatrix}
$$

$$
p\log(q)=1\cdot\log0.8+0\cdot\log0.2+0\cdot\log0.1=-0.097
$$

$$
L_{CCE}=-p\log(q)=0.097
$$

**모델이 틀린 경우**

$$
p=\begin{bmatrix}1\\0\\0\end{bmatrix}~~,~~
q=\begin{bmatrix}0.2\\0.4\\0.4\end{bmatrix}
$$

$$
p\log(q)=1\cdot\log0.2+0\cdot\log0.4+0\cdot\log0.4=-0.699
$$

$$
L_{CCE}=-p\log(q)=0.699
$$

---

</div>
</details>

### KL Divergence Loss

두 확률 분포 $p$와 $q$가 얼마나 다른지를 측정한다.

$$
D_{KL}(p||q)=\sum_{i=1}^Np_i\log\left(\frac{p_i}{q_i}\right)
$$

위의 식에서 $p$는 정답 확률 분포, $q$는 예측 확률 분포를 의미한다.

두 분포가 완전히 같으면 $0$이 된다.

## 기타 손실 함수

### Focal Loss

$$
L_{Focal}=
$$
