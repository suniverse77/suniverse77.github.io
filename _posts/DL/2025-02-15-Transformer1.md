---
title: "[트랜스포머] 트랜스포머 구조"
date: 2025-02-15 00:00:00 +/-TTTT
categories: [AI, 딥러닝]
tags: [트랜스포머]
math: true
toc: true
author: sunho
---

## Overview

트랜스포머의 아키텍처는 아래와 같으며, 크게 인코더와 디코더로 이루어져 있다.

![fig1](dl/transformer/1-1.png){: style="display:block; margin:0 auto; width:70%;"}
_출처: Attention Is All you Need_

인코더는 입력 문장을 이해하고 의미 정보를 요약하며, 디코더는 인코더가 전달한 의미 정보를 이용해 적절한 출력 문장을 생성한다.

예를 들어, 아래의 문제를 수행하는 트랜스포머 모델의 동작 방식은 아래와 같다.

**질문에 답을 해주는 모델**

`'오늘 날씨 어때?'`라는 질문을 하면 해당 문장은 인코더의 입력으로 들어간다. 인코더는 질문의 의미를 파악하여 하나의 요약본인 문맥 벡터 (Context Vector)를 출력한다.

디코더는 인코더가 전달한 문맥 벡터를 바탕으로, 답변의 가장 적절한 첫 번째 단어 `'오늘'`을 출력한다.

이후 디코더는 지금까지 생성된 단어 `'오늘'`을 다시 입력으로 받아, 질문과 현재까지의 출력 `'오늘 날씨 어때?' + '오늘'`을 함께 고려하여 다음에 올 단어 `'서울의'`를 출력한다.

그다음 디코더는 `'오늘 날씨 어때? + '오늘 서울의'`를 함께 고려하여 다음 단어 `'날씨는'`을 출력한다.

이러한 과정을 반복하여 최종적으로 `'오늘 서울의 날씨는 화창합니다.'`라는 답변을 완성한다.

**번역 모델**

번역 모델도 위의 과정과 동일하다.

한글 → 영어 번역에서 `'나는 학생이다.'`라는 문장이 인코더의 입력으로 들어오면, 디코더는 `'I'`를 출력한다.

이후 디코더는 `'나는 학생이다.' + 'I'`를 함께 고려하여 다음에 올 단어 `'am'`을 출력한다.

이러한 과정을 반복하여 최종적으로 `'I am a student.'`라는 번역본을 완성한다.

### 인코더 (Encoder)

인코더의 핵심 구성요소는 다음과 같다.

**Embedding**

입력 문장을 여러 개의 토큰으로 나누고, 각 토큰을 벡터 형태로 변환한다. 이 과정을 임베딩 (Embedding)이라고 한다. 토큰은 단어가 될 수도 있고, 문자가 될 수도 있다. (편의상 '단어=토큰'으로 설명)

이때 문장 내 단어의 순서 정보를 반영하기 위해, 각 단어 벡터에 위치 인코딩 (Positional Encoding)을 더한다.

**Self-Attention**

입력 문장 내 모든 단어가 서로 어떤 관계를 가지는지를 내적을 이용해 계산하며, 이 과정에서 단어 간의 문맥적 관계를 학습한다.

**Feed-Forward Network**

각 단어 벡터에 대해 개별적으로 수행되며, 비선형 변환을 통해 표현력을 강화한다.

### 디코더 (Decoder)

디코더의 핵심 구성요소는 다음과 같다.

**Embedding**

이전에 생성된 출력 단어를 벡터로 변환한 후, Positional Encoding을 더한다.

**Masked Self-Attention**

출력 문장 내 모든 단어가 서로 어떤 관계를 가지는지를 계산한다. 

인코더와는 달리, 학습 과정에서 현재 시점 이후의 단어는 보지 못하도록 미래 단어를 마스킹 (masking)한다.

**Encoder–Decoder Attention (Cross-Attention)**

인코더가 생성한 입력 문장의 의미 벡터를 참고하여, 현재 생성 중인 단어가 입력의 어떤 부분과 관련이 깊은지를 학습한다.

즉, 입력과 출력 간의 대응 관계를 학습하는 단계이다.

**Feed-Forward Network**

각 단어 벡터에 대해 개별적으로 수행되며, 비선형 변환을 통해 표현력을 강화한다.

**Softmax Layer**

각 단어 벡터를 소프트맥스 함수를 통해 확률 분포로 변환하며, 최종적으로 현재 시점에서 가장 확률이 높은 단어를 선택하여 출력한다.
