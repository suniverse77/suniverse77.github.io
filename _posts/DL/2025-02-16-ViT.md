---
title: "[트랜스포머] 비전 트랜스포머 (Vision Transformer)"
date: 2025-02-15 18:00:00 +/-TTTT
categories: [AI, 딥러닝]
tags: [트랜스포머]
math: true
toc: true
author: sunho
---

보기 전 앞선 포스트들을 보고 오시면 도움이 됩니다.

![fig0](dl/transformer/5-0.png){: style="display:block; margin:0 auto; width:70%;"}
_출처: [An Image is Worth 16X16 Words:
Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)_

## Overview

비전 트랜스포머 (ViT)는 이미지 분류 문제를 해결하기 위해 고안된 모델이다.

분류는 클래스를 예측하는 비생성적 문제이기 때문에 생성 과정이 필요하지 않다. 따라서 ViT는 트랜스포머의 인코더만 사용한다. 

전체 구조는 언어 모델의 인코더와 거의 동일하며, Embedding, Attention, MLP로 구성되어 있다. 단지 이미지를 처리하기 때문에, 단어 대신 이미지의 패치가 토큰 역할을 한다.

## 패치 임베딩 (Patch Embedding)

ViT에서는 이미지를 직접 픽셀 단위로 처리하지 않고, 먼저 이미지를 작은 패치 (patch) 단위로 분할하여 토큰 형태로 변환한다.

예를 들어 입력 이미지의 크기가 $48\times48$일 때, 이를 $16\times16$ 크기의 패치로 나눈다면 총 $N=9$개의 패치가 생긴다. 이미지는 RGB 3개의 채널을 가지고 있기 때문에, 하나의 패치는 $(C,H,W)=(3,16,16)$의 형태를 가진다.

이후 이 패치를 평탄화하여 하나의 벡터 $\mathbf{p}_i\in\mathbb{R}^{768}$로 변환한다. 벡터가 $768$ 차원인 이유는 하나의 패치에 $3\times16\times16=768$개의 픽셀값이 존재하기 때문이다. 이렇게 평탄화된 패치를 상단 그림에서 Flattened Patch라고 표현하였다.

이때 픽셀값은 $0\sim255$ 범위를 가지기 때문에, 정규화를 한다.

이 정규화된 벡터를 바로 인코더의 입력으로 사용하지 않고, 신경망에 통과시켜 $D$ 차원의 임베딩 벡터 (Embedded Patch)로 변환한다.

$$
\mathbf{e}_i=W_E~\mathbf{e}_i^{patch}\in\mathbb{R}^D
$$

이렇게 얻은 모든 패치 임베딩에 위치 임베딩 (positional embedding) $\mathbf{p}_i$를 더해, 모델이 각 패치의 공간적 위치 정보를 인식할 수 있도록 한다.

마지막으로, 클래스 토큰 (cls token) $\mathbf{e}_{cls}$를 시퀀스의 맨 앞에 추가한다. 이 토큰은 학습 과정에서 전체 이미지를 대표하며, 이후 분류 단계에서 최종적으로 사용된다.

따라서 인코더의 최종 입력은 다음과 같은 형태를 갖는다.

$$
E=\begin{bmatrix}
-\mathbf{e}_{cls}-\\-\mathbf{e}_1^{patch}+\mathbf{p}_1-\\-\mathbf{e}_2^{patch}+\mathbf{p}_2-\\\vdots\\-\mathbf{e}_N^{patch}+\mathbf{p}_N-
\end{bmatrix}\in\mathbb{R}^{(N+1)\times D}
$$

![fig1](dl/transformer/5-1.png){: style="display:block; margin:0 auto; width:80%;"}
_[[출처]](https://link.springer.com/article/10.1007/s00138-024-01609-0)_

### 위치 임베딩 (Positional Embedding)

아래 그림은 각 입력 패치의 위치 임베딩이 다른 모든 패치들의 위치 임베딩과 얼마나 유사한지를 시각화한 맵이다. 이는 트랜스포머 모델이 패치들 간의 공간적 관계를 어떻게 인코딩하고 있는지를 보여준다.

예를 들어, 아래 그림에서 중앙 4행 4열에 위치한 작은 히트맵은 $(4,4)$ 위치의 패치를 기준점으로 삼아, 다른 모든 패치들과의 유사도를 나타낸다. 

이 히트맵의 정중앙은 가장 밝은 노란색으로, 자기 자신과의 위치 임베딩 유사도가 가장 높음을 의미한다. 기준 패치에서 가까운 위치에 있는 패치들일수록 밝은 색상을 띠며 (유사도가 높음), 모서리나 가장자리로 갈수록 거리가 멀어지기 때문에 어두운 색상을 띠는 것 (유사도가 낮음)을 확인할 수 있다.

![fig2](dl/transformer/5-2.png){: style="display:block; margin:0 auto; width:50%;"}
_출처: [An Image is Worth 16X16 Words:
Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)_

## Multi-Head Self-Attention

이 단계는 입력이 패치 임베딩인 것을 제외하고는, 언어 모델에서 사용되는 Self-Attention과 거의 동일하다.

먼저 입력 임베딩 $E$에 가중치 행렬 $W_Q,W_K,W_V\in\mathbb{R}^{D\times D}$를 곱해 Query, Key, Value를 만든다.

$$
Q=EW_Q\in\mathbb{R}^{(N+1)\times D}~~,~~
K=EW_K\in\mathbb{R}^{(N+1)\times D}~~,~~
V=EW_V\in\mathbb{R}^{(N+1)\times D}
$$

또는 아래와 같이 $W_{QKV}\in\mathbb{R}^{D\times 3D}$을 곱해 동시에 계산한 후, Query, Key, Value로 나누는 방법을 사용할 수도 있다.

$$
QKV=W_{QKV}E\in\mathbb{R}^{(N+1)\times 3D}~\to~
Q=QKV[:D]~,~K=QKV[D:2D]~,~V=QKV[2D:]
$$

Attention 연산은 동일하게 아래와 같이 정의된다.

$$
\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

언어 모델에서는 Attention map이 단어 간의 관계를 파악하듯, ViT에서는 패치 간의 관계를 학습한다.

## 클래스 토큰 (Class Token)

클래스 토큰은 입력 이미지 전체의 정보를 종합하여, 최종적으로 '이 이미지는 오토바이다.'와 같은 결정을 내리는 역할을 한다.

트랜스포머 인코더를 통과한 임베딩 벡터 중, 클래스 토큰 벡터만 MLP Head를 통과하여 최종 클래스를 예측한다.

직관적으로, 클래스 토큰은 '이 이미지가 어떤 클래스인지 결론을 내리기 위해 어떤 패치의 정보가 가장 중요해?'라는 질문을 던지는 것으로 볼 수 있다.

이때 아래의 Attention map은 바로 그 질문에 대한 응답과 같다.

![fig3](dl/transformer/5-3.png){: style="display:block; margin:0 auto; width:60%;"}
_[[출처]](https://medium.com/@braian.d/understanding-representations-of-concepts-in-visual-transformers-by-analyzing-attention-maps-from-de00fce5bbdb)_

밝은 노란색 부분은 점수가 높은 영역으로, 클래스 토큰이 '너의 정보가 결정을 내리는 데 매우 중요해.'라고 강하게 주목하는 패치들이다.

반대로 어두운 보라색 부분은 점수가 낮은 영역으로, 클래스 토큰이 '너의 정보는 별로 중요하지 않으니 무시해도 되겠어.'라고 판단한 패치들이다.

클래스 토큰이 각 패치한테 질문을 던지고 응답을 받는 과정은 내적 연산으로 표현할 수 있다. 즉, $QK^\top$에서 아래의 빨간색 값들이 바로 그 값이다.

$$
QK^\top=
\begin{bmatrix}
\mathbf{q}_{cls}\mathbf{k}_{cls}^\top&
\color{red}\mathbf{q}_{cls}\mathbf{k}_1^\top&\color{red}\cdots&\color{red}\mathbf{q}_{cls}\mathbf{k}_N^\top\\
\mathbf{q}_1\mathbf{k}_{cls}^\top&\mathbf{q}_1\mathbf{k}_1^\top&\cdots&\mathbf{q}_1\mathbf{k}_N^\top\\
\vdots&\vdots&\ddots&\vdots\\
\mathbf{q}_N\mathbf{k}_{cls}^\top&\mathbf{q}_N\mathbf{k}_1^\top&\cdots&\mathbf{q}_N\mathbf{k}_N^\top
\end{bmatrix}\in\mathbb{R}^{(N+1)\times(N+1)}
$$

즉, 위의 Attention map 그림은 $QK^\top$ 행렬의 빨간색 부분을 아래와 같이 2차원 형태로 변환하여 시각화한 것이다. ($N=9$라고 가정)

$$
\begin{bmatrix}
\mathbf{q}_{cls}\mathbf{k}_1^\top&\mathbf{q}_{cls}\mathbf{k}_2^\top&\mathbf{q}_{cls}\mathbf{k}_3^\top\\
\mathbf{q}_{cls}\mathbf{k}_4^\top&\mathbf{q}_{cls}\mathbf{k}_5^\top&\mathbf{q}_{cls}\mathbf{k}_6^\top\\
\mathbf{q}_{cls}\mathbf{k}_7^\top&\mathbf{q}_{cls}\mathbf{k}_8^\top&\mathbf{q}_{cls}\mathbf{k}_9^\top\\
\end{bmatrix}\in\mathbb{R}^{3\times3}
$$

### 예측 (Prediction)

최종적으로, 클래스 토큰 $\mathbf{e}_{cls}$는 MLP Head를 통과하여 클래스 개수 $C$와 동일한 차원의 벡터로 변환된다.

이 벡터는 각 클래스에 대한 로짓 (logit)값을 나타내며, 소프트맥스 함수를 적용하여
클래스별 확률 분포로 변환한다.

$$
\mathbf{z}=\text{MLP}(\mathbf{e}_{cls})\in\mathbb{R}^C~\to~
\mathbf{y}=\text{softmax}(\mathbf{z})
=\begin{bmatrix}y_1\\y_2\\\vdots\\y_C\end{bmatrix}\in\mathbb{R}^C
$$

여기서 $y_i$는 해당 클래스에 속할 확률을 의미하며, 모델은 이 확률들 중 가장 높은 값을 가진 클래스를 최종 예측 결과로 선택한다.

$$
\hat{y}=\underset{i}{\arg\max}(y_i)
$$

## Inductive Bias

Inductive Bias는 모델이 처음 보는 데이터에 대해 예측을 수행할 때 사용하는 내장된 기본 가정이다. 

모델이 태어날 때부터 가지고 있는 일종의 '편견'이나 '세상을 보는 방식'이라고 생각하면 된다.

### CNN

CNN은 이미지 처리에 특화된 두 가지 강력한 가정을 가지고 있다. (Inductive Bias가 큼)

이 가정들은 이미지 데이터의 구조적 특성에 매우 잘 맞기 때문에, 비교적 적은 데이터로도 빠르고 효율적으로 학습할 수 있다.

**1. 지역성 (Locality)**

이는 <span style="background-color:#fff5b1">'중요한 정보는 보통 서로 가까이 붙어있다.'</span>는 가정이다.

CNN의 필터는 이미지의 작은 영역만을 관찰하면서, 그 영역 안에서 특징을 추출한다.

예를 들어 고양이의 눈, 코, 입이 서로 가까이 붙어있는 것처럼, 이미지의 주요 시각적 패턴은 local pattern을 가진다고 보는 것이다.

**2. 이동 불변성 (Translation Invariance)**

이는 <span style="background-color:#fff5b1">'특징의 위치가 달라져도 동일하게 인식할 수 있다.'</span>라는 가정이다.

하나의 필터가 이미지 전체를 슬라이딩하며 동일한 연산을 적용하기 때문에, 모델은 특정 패턴이 이미지의 어디에 있든 동일한 특징으로 인식할 수 있다.

예를 들어 고양이가 이미지의 왼쪽에 있든 오른쪽에 있든, CNN은 그것을 같은 '고양이'로 인식한다.

### Transformer

반면 트랜스포머는 이러한 사전 가정이 없고, 모델이 이미지 패치들 간의 관계를 Self-Attention을 통해 데이터로부터 직접 학습해야만 한다. (Inductive Bias가 작음)

트랜스포머는 <span style="background-color:#fff5b1">'모든 요소는 다른 모든 요소와 어떤 식으로든 관련이 있을 수 있다.'</span>라는 기본 전제를 가지고 있다.

즉, 모델은 입력 이미지의 한 패치가 다른 모든 패치와 어떤 관계를 맺고 있는지를 전역적으로 계산하며, 이는 Locality 같은 제한적인 가정을 하지 않기 때문에
이미지의 왼쪽 끝 픽셀과 오른쪽 끝 픽셀 사이에도 직접적인 상호작용이 가능하다.

**트랜스포머가 대규모 데이터셋을 필요로 하는 이유**

이처럼 트랜스포머는 약한 Inductive Bias를 가지기 때문에, 데이터의 구조나 패턴에 대한 사전 지식이 거의 없다.

따라서 모델이 스스로 모든 관계와 구조를 처음부터 학습해야 하므로, 효과적으로 학습하기 위해서는 방대한 양의 데이터가 필요한 것이다.

아래 그림에서 회색 영역은 CNN 기반의 모델의 성능 범위를 나타낸다. 적은 데이터셋인 ImageNet에 대해 학습했을 때는 ViT의 성능이 CNN보다 낮지만, 대규모 데이터셋인 JFT-300M에 대해 학습했을 때는 ViT의 성능이 CNN을 능가하는 것을 확인할 수 있다.

![fig4](dl/transformer/5-4.png){: style="display:block; margin:0 auto; width:60%;"}
_출처: [An Image is Worth 16X16 Words:
Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)_
