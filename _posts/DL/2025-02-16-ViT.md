---
title: "[트랜스포머] 비전 트랜스포머 (Vision Transformer)"
date: 2025-02-15 18:00:00 +/-TTTT
categories: [AI, 딥러닝]
tags: [트랜스포머]
math: true
toc: true
author: sunho
---

보기 전 앞선 포스트들을 보고 오시면 도움이 됩니다.

![fig0](dl/transformer/5-0.png){: style="display:block; margin:0 auto; width:70%;"}
_출처: [An Image is Worth 16X16 Words:
Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)_

## Overview

비전 트랜스포머 (ViT)는 이미지 분류 문제를 해결하기 위해 고안된 모델이다.

분류는 클래스를 예측하는 비생성적 문제이기 때문에 생성 과정이 필요하지 않다. 따라서 ViT는 트랜스포머의 인코더만 사용한다. 

전체 구조는 언어 모델의 인코더와 거의 동일하며, Embedding, Attention, MLP로 구성되어 있다. 단지 이미지를 처리하기 때문에, 단어 대신 이미지의 패치가 토큰 역할을 한다.

## 패치 임베딩 (Patch Embedding)

ViT에서는 이미지를 직접 픽셀 단위로 처리하지 않고, 먼저 이미지를 작은 패치 (patch) 단위로 분할하여 토큰 형태로 변환한다.

예를 들어 입력 이미지의 크기가 $48\times48$일 때, 이를 $16\times16$ 크기의 패치로 나눈다면 총 $N=9$개의 패치가 생긴다. 이미지는 RGB 3개의 채널을 가지고 있기 때문에, 하나의 패치는 $(C,H,W)=(3,16,16)$의 형태를 가진다.

이후 이 패치를 평탄화하여 하나의 벡터 $\mathbf{p}_i\in\mathbb{R}^{768}$로 변환한다. 벡터가 $768$ 차원인 이유는 하나의 패치에 $3\times16\times16=768$개의 픽셀값이 존재하기 때문이다. 이렇게 평탄화된 패치를 상단 그림에서 Flattened Patch라고 표현하였다.

이때 픽셀값은 $0\sim255$ 범위를 가지기 때문에, 정규화를 한다.

이 정규화된 벡터를 바로 인코더의 입력으로 사용하지 않고, 신경망에 통과시켜 $D$ 차원의 임베딩 벡터 (Embedded Patch)로 변환한다.

$$
\mathbf{e}_i=W_E\mathbf{e}_i^{patch}\in\mathbb{R}^D
$$

이렇게 얻은 모든 패치 임베딩에 위치 임베딩 (positional embedding) $\mathbf{p}_i$를 더해, 모델이 각 패치의 공간적 위치 정보를 인식할 수 있도록 한다.

마지막으로, 클래스 토큰 (cls token) $\mathbf{e}_{cls}$를 시퀀스의 맨 앞에 추가한다. 이 토큰은 학습 과정에서 전체 이미지를 대표하며, 이후 분류 단계에서 최종적으로 사용된다.

따라서 인코더의 최종 입력은 다음과 같은 형태를 갖는다.

$$
E=\begin{bmatrix}
-\mathbf{e}_{cls}-\\-\mathbf{e}_1^{patch}+\mathbf{p}_1-\\-\mathbf{e}_2^{patch}+\mathbf{p}_2-\\\vdots\\-\mathbf{e}_N^{patch}+\mathbf{p}_N-
\end{bmatrix}\in\mathbb{R}^{(N+1)\times D}
$$

![fig1](dl/transformer/5-1.png){: style="display:block; margin:0 auto; width:70%;"}
_[[출처]](https://link.springer.com/article/10.1007/s00138-024-01609-0)_

### 위치 임베딩 (Positional Embedding)

![fig2](dl/transformer/5-2.png){: style="display:block; margin:0 auto; width:70%;"}
_출처: [An Image is Worth 16X16 Words:
Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)_

## Multi-Head Self-Attention

이 단계는 입력이 패치 임베딩인 것을 제외하고는, 언어 모델에서 사용되는 Self-Attention과 거의 동일하다.

먼저 입력 임베딩 $E$에 가중치 행렬 $W_Q,W_K,W_V\in\mathbb{R}^{D\times D}$를 곱해 Query, Key, Value를 만든다.

$$
Q=EW_Q\in\mathbb{R}^{(N+1)\times D}~~,~~
K=EW_K\in\mathbb{R}^{(N+1)\times D}~~,~~
V=EW_V\in\mathbb{R}^{(N+1)\times D}
$$

또는 아래와 같이 $W_{QKV}\in\mathbb{R}^{D\times 3D}$을 곱해 동시에 계산한 후, Query, Key, Value로 나누는 방법을 사용할 수도 있다.

$$
QKV=W_{QKV}E\in\mathbb{R}^{(N+1)\times 3D}~\to~
Q=QKV[:D]~,~K=QKV[D:2D]~,~V=QKV[2D:]
$$

Attention 연산은 동일하게 아래와 같이 정의된다.

$$
\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

언어 모델에서는 Attention map이 단어 간의 관계를 파악하듯, ViT에서는 패치 간의 관계를 학습한다.

## 클래스 토큰 (Class Token)

![fig3](dl/transformer/5-3.png){: style="display:block; margin:0 auto; width:70%;"}
_[[출처]](https://medium.com/@braian.d/understanding-representations-of-concepts-in-visual-transformers-by-analyzing-attention-maps-from-de00fce5bbdb)_

## CNN vs Transformer

CNN보다 inductive bias가 적다. 
