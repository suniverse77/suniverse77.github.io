---
title: "[CS231n] Neural Networks and Backpropagation"
date: 2025-09-07 00:00:00 +/-TTTT
categories: [AI, CS231n]
tags: [CS231n]
math: true
toc: true
author: sunho
description: 📖 Stanford CS231n | Spring 2025 | Lecture 4
---

## Neural Networks

하나의 layer로 구성된 신경망은 아래와 같이 정의된다.

$$
f=Wx
$$

2개의 layer를 쌓으면, 아래와 같이 표현할 수 있다. (ReLU 함수 사용)

$$
f=W_2\max(0,W_1x)
$$

만약 layer 사이에 비선형 함수를 넣지 않는다면, 여러 layer를 쌓아도 결국 하나의 선형 변환과 동일해진다. 

비선형 함수는 입력 공간을 변형하여 원래 선형적으로 분리되지 않던 데이터를 구분 가능하게 만드는 역할을 한다.

예를 들어, 아래의 그림은 유클리디안 좌표계에서 선형 분리가 불가능했던 데이터가, 극좌표계로 변환되면서 쉽게 분리가 가능해진 상황을 보여준다.

![fig1](cs231n/04-1.png){: style="display:block; margin:0 auto; width:90%;"}

신경망의 구조는 아래처럼 간단히 도식화할 수 있으며, 그림에서 보이는 동그라미는 뉴런 (neuron)을 의미한다.

입력층은 layer로 세지 않기 때문에 아래 그림에서 신경망은 3개의 layer로 구성되어 있다고 할 수 있다. (hiddne layer 2개 + 출력 layer 1개)

![fig2](cs231n/04-2.png){: style="display:block; margin:0 auto; width:90%;"}

일반적으로 뉴런의 개수가 많아질수록 분리 능력이 향상된다.

![fig3](cs231n/04-3.png){: style="display:block; margin:0 auto; width:90%;"}

## Backpropagation

매번 새로운 손실 함수에 대해 $\nabla_WL$을 계산하는 것은 매우 비효율적이다.

그렇다면 신경망에서 어떻게 효율적으로 그라디언트를 계산할 수 있을까?

### Computational Graphs

신경망의 연산은 덧셈, 곱셈 등의 일련의 기본 연산으로 구성되기 때문에 이를 계산 그래프로 표현할 수 있다.

- **순전파 (Forward pass)**에서는 입력에서 출발해 그래프를 따라가면서 각 노드의 출력을 차례대로 계산하며, 최종적으로 손실 함수 값 $L$을 얻는다.
- **역전파 (Backward pass)**에서는 출력에서 입력 방향으로 그래프를 거슬러 올라가면서 chain rule을 이용해 각 파라미터에 대한 손실의 기울기 $\nabla_WL$를 계산한다.

![fig4](cs231n/04-4.png){: style="display:block; margin:0 auto; width:100%;"}

### Backprop with Scalars

스칼라 함수

![fig5](cs231n/04-5.png){: style="display:block; margin:0 auto; width:80%;"}

<details>
<summary><font color='#FF0000'>Example 1</font></summary>
<div markdown="1">



</div>
</details>
<br>

<details>
<summary><font color='#FF0000'>Example 2</font></summary>
<div markdown="1">



</div>
</details>
<br>



### Backprop with Vectors

벡터 함수

### Backprop with Matrices (Tensors)
