---
title: "[CS231n] Regularization and Optimization"
date: 2025-09-06 00:00:00 +/-TTTT
categories: [딥러닝]
tags: [CS231n]
math: true
toc: true
author: sunho
description: 👨‍👧‍👧 Stanford CS231n | Spring 2025 | Lecture 3 
---

## Regularization

$$
L(W)=\underbrace{\frac{1}{N}\sum_{i=1}^NL_i(f(x_i,W),y_i)}_{\text{Data loss}}+\underbrace{\lambda R(W)}_{\text{Regularization}}
$$

**Data loss**

- 모델이 학습 데이터에 대해 올바른 예측을 하도록 유도한다.

**Regularization**

- 모델이 학습 데이터에 오버피팅되는 것을 방지해 일반화 성능을 높이는 역할을 한다.
- 입력 데이터와는 독립적인 항으로, 가중치 $W$의 크기나 형태만 제한한다.
- 가중치의 복잡도를 제어해 단순하고 안정적인 모델을 선호하도록 만듦.

### L1 Regularization (Lasso)

$$
R(W)=\sum_k\sum_l\lvert W_{k,l}\rvert
$$

Nearest Neighbor는 새로운 샘플이 들어오면 저장된 모든 샘플과의 거리를 계산해서 가장 가까운 샘플의 라벨을 따르는 방법이다. 학습과 예측의 과정은 아래와 같다.


### L2 Regularization (Lidge)

$$
R(W)=\sum_k\sum_lW^2_{k,l}
$$

새로운 입력이 들어오면 모든 샘플과의 거리를 비교하고, 그 중 가장 가까운 K개의 이웃을 선택한다.

선택된 K개의 이웃 라벨 중 일반적으로 다수결 투표 (Majority Voting)를 통해 라벨을 결정한다.

## Optimization

![fig7](cs231n/02-7.png){: width="100%"}

### Interpreting a Linear Classifier

**Algebraic Viewpoint**

![fig8](cs231n/02-8.png){: width="100%"}

$W$의 각 행은 특정 클래스에 대응하는 가중치 벡터로, 입력 $x$와 내적하여 가장 점수가 큰 클래스로 분류한다.

**Visual Viewpoint**

가중치 행렬의 각 행은 해당 클래스가 어떻게 생겼는지를 대략적으로 나타내는 일종의 템플릿이다.

![fig9](cs231n/02-9.png){: width="100%"}

위에서 아래 10개의 그림은 각 클래스의 가중치 행을 시각화한 것으로, car의 경우 자동차의 패턴이 보인다.

**Geometric Viewpoint**

이미지를 공간 위의 하나의 점으로 생각하고, Linear Classifier를 각 점을 구분하는 경계선으로 본다.

![fig10](cs231n/02-10.png){: width="100%"}

**고차원 공간에서의 이미지**

- 32x32x3 크기의 이미지의 경우, 이 이미지는 총 3,072개의 숫자들로 이루어진 데이터로 볼 수 있다.
- 3,072개의 숫자 하나하나를 각 차원의 좌표 값으로 생각하면, 이 이미지는 3,072차원 공간에 있는 하나의 점으로 표현할 수 있다.
- 같은 클래스의 이미지들은 고차원 공간에서 비슷한 영역에 모여 분포한다.

**분류기의 기하학적 의미**

- 서로 다른 클래스의 점들을 잘 나눌 수 있는 결정 경계(Decision Boundary)를 찾는 것.
- 이 결정 경계는 **고차원 공간 속의 초평면(hyperplane)**이며, 경계를 기준으로 한쪽은 고양이, 다른 쪽은 강아지로 분류됨.
- Weight $W$는 결정 경계의 기울기를 결정하며, 결정 경계에 수직인 방향을 가리킨다. 이 방향으로 갈수록 해당 클래스일 점수가 높아짐을 의미한다.
- Bias $b$는 결정 경계의 위치를 결정하며, 경계선을 데이터 분포에 맞게 평행 이동시켜 최적의 위치에 놓는 역할을 한다.

**학습 과정**

- 주어진 데이터를 바탕으로 정답을 가장 잘 분리할 수 있도록 최적의 $W$와 $b$ 값을 찾는 과정이다.
- 직관적으로는, 데이터 점들이 찍힌 그래프 위에 선을 이리저리 그어보며 두 그룹을 가장 잘 나누는 위치를 찾는 것과 같다.

### Softmax Classifier

$$
\text{Softmax}(s_i)=\frac{\exp(s_i)}{\sum_{j}\exp(s_j)}
$$

Softmax 함수는 위와 같이 정의되며, classifier의 raw score를 확률 분포로 변환할 때 사용한다.

![fig11](cs231n/02-11.png){: width="100%"}
![fig12](cs231n/02-12.png){: width="100%"}
![fig12](cs231n/02-13.png){: width="100%"}

Softmax 함수를 통해 얻은 확률 값은 MLE로 해석할 수도 있고, 실제 확률 분포와 비교하여 KL Divergence나 Cross Entropy를 손실 함수로 사용할 수도 있다.

### Hinge Loss (SVM Loss)

정답 점수와 오답 점수가 margin 이상 차이나지 않으면 패널티를 주는 손실 함수이다.

$$
L_i=\sum_{j\not=y_i}\max(0,s_j-s_{y_i}+1)
$$

Hinge Loss는 위와 같이 정의되며, $s_j$는 클래스 $j$에 대한 score, $s_{y_i}$는 정답 클래스 $y_i$에 대한 score를 의미한다.

$+1$은 margin으로 정답 클래스가 오답 클래스보다 최소한 1만큼은 크게 점수를 가져야 한다는 것을 의미한다.

![fig14](cs231n/02-14.png){: width="100%"}

위에서 고양이 이미지의 hinge loss를 계산하면 아래와 같다.

![fig15](cs231n/02-15.png){: width="100%"}
