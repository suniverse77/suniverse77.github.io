---
title: "[CS231n] Regularization and Optimization"
date: 2025-09-06 00:00:00 +/-TTTT
categories: [ë”¥ëŸ¬ë‹]
tags: [CS231n]
math: true
toc: true
author: sunho
description: ğŸ‘¨â€ğŸ‘§â€ğŸ‘§ Stanford CS231n | Spring 2025 | Lecture 3 
---

## Regularization

$$
L(W)=\underbrace{\frac{1}{N}\sum_{i=1}^NL_i(f(x_i,W),y_i)}_{\text{Data loss}}+\underbrace{\lambda R(W)}_{\text{Regularization}}
$$

**Data loss**

- ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ ì˜¬ë°”ë¥¸ ì˜ˆì¸¡ì„ í•˜ë„ë¡ ìœ ë„í•œë‹¤.

**Regularization**

- ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì— ì˜¤ë²„í”¼íŒ…ë˜ëŠ” ê²ƒì„ ë°©ì§€í•´ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ëŠ” ì—­í• ì„ í•œë‹¤.
- ì…ë ¥ ë°ì´í„°ì™€ëŠ” ë…ë¦½ì ì¸ í•­ìœ¼ë¡œ, ê°€ì¤‘ì¹˜ $W$ì˜ í¬ê¸°ë‚˜ í˜•íƒœë§Œ ì œí•œí•œë‹¤.
- ê°€ì¤‘ì¹˜ì˜ ë³µì¡ë„ë¥¼ ì œì–´í•´ ë‹¨ìˆœí•˜ê³  ì•ˆì •ì ì¸ ëª¨ë¸ì„ ì„ í˜¸í•˜ë„ë¡ ë§Œë“¦.

### L1 Regularization (Lasso)

$$
R(W)=\sum_k\sum_l\lvert W_{k,l}\rvert
$$

Nearest NeighborëŠ” ìƒˆë¡œìš´ ìƒ˜í”Œì´ ë“¤ì–´ì˜¤ë©´ ì €ì¥ëœ ëª¨ë“  ìƒ˜í”Œê³¼ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•´ì„œ ê°€ì¥ ê°€ê¹Œìš´ ìƒ˜í”Œì˜ ë¼ë²¨ì„ ë”°ë¥´ëŠ” ë°©ë²•ì´ë‹¤. í•™ìŠµê³¼ ì˜ˆì¸¡ì˜ ê³¼ì •ì€ ì•„ë˜ì™€ ê°™ë‹¤.


### L2 Regularization (Lidge)

$$
R(W)=\sum_k\sum_lW^2_{k,l}
$$

ìƒˆë¡œìš´ ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´ ëª¨ë“  ìƒ˜í”Œê³¼ì˜ ê±°ë¦¬ë¥¼ ë¹„êµí•˜ê³ , ê·¸ ì¤‘ ê°€ì¥ ê°€ê¹Œìš´ Kê°œì˜ ì´ì›ƒì„ ì„ íƒí•œë‹¤.

ì„ íƒëœ Kê°œì˜ ì´ì›ƒ ë¼ë²¨ ì¤‘ ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìˆ˜ê²° íˆ¬í‘œ (Majority Voting)ë¥¼ í†µí•´ ë¼ë²¨ì„ ê²°ì •í•œë‹¤.

## Optimization

Numerical gradient

Analytic gradient

### SGD (Stochastic Gradient Descent)

ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰ë§ˆë‹¤ ë°ì´í„° subsetì„ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§í•˜ê¸° ë•Œë¬¸ì— stochasticì´ë¼ê³  ë¶€ë¥¸ë‹¤.

$$
x_{t+1}=x_t-\alpha\nabla f(x_t)
$$

SGDì—ëŠ” ëª‡ ê°€ì§€ ë¬¸ì œì ì´ ì¡´ì¬í•œë‹¤.

1. Oscillation
    
    Loss functionì´ ì–´ë–¤ ë°©í–¥ì—ì„œëŠ” ê¸‰ê²©íˆ ë³€í•˜ê³ , ë‹¤ë¥¸ ë°©í–¥ì—ì„œëŠ” ì™„ë§Œí•˜ê²Œ ë³€í•˜ëŠ” ê²½ìš°ê°€ ìˆìœ¼ë©°, ì´ë•Œ Gradient DescentëŠ” ê²½ì‚¬ë¥¼ ë”°ë¼ ì§€ê·¸ì¬ê·¸ë¡œ ì›€ì§ì´ë©° ì§„ë™í•˜ë©´ì„œ ì²œì²œíˆ ìˆ˜ë ´í•˜ê²Œ ëœë‹¤.
    
    ![fig2](cs231n/03-2.png){: .w-100}
    
    í–‰ë ¬ì˜ condition numberëŠ” ê°€ì¥ í° singular valueì™€ ê°€ì¥ ì‘ì€ singular valueì˜ ë¹„ìœ¨ì„ ì˜ë¯¸í•œë‹¤.
    
    Hessianì€ loss í•¨ìˆ˜ì˜ ê³¡ë¥ ì„ ë‚˜íƒ€ë‚´ëŠ” í–‰ë ¬ë¡œ, Hessianì˜ condition numberì´ í¬ë‹¤ëŠ” ê²ƒì€ ì–´ë–¤ ë°©í–¥ì€ ê°€íŒŒë¥´ê³ , ì–´ë–¤ ë°©í–¥ì€ í‰í‰í•˜ë‹¤ëŠ” ëœ»ì´ë‹¤. â†’ ìµœì í™”ê°€ ë¶ˆì•ˆì •í•´ì§

2. Local minima or Saddle point
    - Loss surfaceì—ëŠ” ì—¬ëŸ¬ ê°œì˜ local minimaì™€ saddle pointê°€ ì¡´ì¬í•œë‹¤.
    - Saddle pointì—ì„œëŠ” gradientê°€ 0ì— ê°€ê¹Œì›Œì ¸ í•™ìŠµì´ ì •ì²´ë  ìˆ˜ ìˆë‹¤.
    - íŠ¹íˆ ê³ ì°¨ì› ê³µê°„ì—ì„œëŠ” local minimaë³´ë‹¤ saddle pointê°€ ë” í”í•˜ë‹¤.
    
    ![fig3](cs231n/03-3.png){: .w-70}
    
3. Stochasticity
    
    ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— loss ê°’ì´ ë§¤ ìŠ¤í…ë§ˆë‹¤ í”ë“¤ë¦°ë‹¤.
    
    ![fig4](cs231n/03-4.png){: .w-30}

### SGD + Momentum

ë‹¨ìˆœíˆ gradientë§Œ ë³´ëŠ” ëŒ€ì‹ , ì†ë„ë¥¼ ê³ ë ¤í•œ ë°©ë²•ì´ë‹¤.

$$
v_{t+1}=\rho v_t+\alpha\nabla f(x_t)
\\
x_{t+1}=x_t-\alpha v_{t+1}
$$

- $v$ (velocity): ì´ì „ê¹Œì§€ì˜ ì—¬ëŸ¬ stepì˜ ëˆ„ì  ì—…ë°ì´íŠ¸ ë°©í–¥ì„ ë‚˜íƒ€ë‚¸ë‹¤.
- $\rho$ (momentum): ê³¼ê±° ì†ë„ë¥¼ ì–¼ë§ˆë‚˜ ë°˜ì˜í• ì§€ë¥¼ ê²°ì •í•œë‹¤. (í´ìˆ˜ë¡ ê³¼ê±° ì†ë„ë¥¼ ë” ë§ì´ ë°˜ì˜í•œë‹¤.)

ê°™ì€ ë°©í–¥ì˜ gradientê°€ ë°˜ë³µë˜ë©´ ì ì°¨ ì†ë„ê°€ ë¶™ì–´ ë” ë¹¨ë¼ì§„ë‹¤.

ê³¡ë¥  ì°¨ì´ê°€ í° ì§€í˜•ì—ì„œ ì§„ë™ì„ ì¤„ì´ê³  ë” ì•ˆì •ì ì¸ ë°©í–¥ìœ¼ë¡œ ì´ë™í•œë‹¤.

ë°˜ë“œì‹œ ë¹ ë¥¸ ìˆ˜ë ´ì„ ë³´ì¥í•˜ì§„ ì•Šì§€ë§Œ, ë” ë‚˜ì€ minimumì„ ì°¾ë„ë¡ ë„ì™€ì¤€ë‹¤.

### RMSprop

gradientì˜ í¬ê¸°ë¥¼ ìš”ì†Œë³„ë¡œ ì¡°ì •í•˜ì—¬ í•™ìŠµë¥ ì„ ìŠ¤ì¼€ì¼ë§í•œ ë°©ë²•ì´ë‹¤.

$$
g_{t+1}=\gamma g_t+(1-\gamma)\left(\nabla f(x_t)\right)^2
\\
x_{t+1}=x_t-\frac{\alpha}{\sqrt{g_{t+1}+\epsilon}}\nabla f(x_t)
$$

gradientê°€ í° íŒŒë¼ë¯¸í„°ëŠ” ì—…ë°ì´íŠ¸ í­ì„ ì¤„ì´ê³ , ì‘ì€ íŒŒë¼ë¯¸í„°ëŠ” ë” í¬ê²Œ ì—…ë°ì´íŠ¸.

í‰í‰í•œ ì˜ì—­(gradientê°€ ì‘ì€ ì˜ì—­)ì—ì„œëŠ” ì´ë™ì„ ê°€ì†, ê°€íŒŒë¥¸ ì˜ì—­(gradientê°€ í° ì˜ì—­)ì—ì„œëŠ” ì´ë™ì„ ì™„í™”

ë°©í–¥ë§ˆë‹¤ ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ì„ ìë™ìœ¼ë¡œ ë³´ì •í•´ í•™ìŠµì„ ì•ˆì •í™”í•œë‹¤.

### Adam

Momentum + RMSpropì„ ê²°í•©í•œ ë°©ì‹ì´ë‹¤.

- **Momentum**: 1ì°¨ ëª¨ë©˜íŠ¸(gradient í‰ê· )ë¥¼ ì¶”ì 
- **RMSprop**: 2ì°¨ ëª¨ë©˜íŠ¸(gradient ì œê³± í‰ê· )ë¥¼ ì¶”ì 

$$
m_{t+1}=\beta_1m_t+(1-\beta_2)\nabla f(x_t)~,~v_{t+1}=\beta_1v_t+(1-\beta_2)\left(\nabla f(x_t)\right)^2
\\
\hat{m}_{t+1}=\frac{m_{t+1}}{1-(\beta_1)^{t+1}}~,~\hat{v}_{t+1}=\frac{v_{t+1}}{1-(\beta_2)^{t+1}}
\\
x_{t+1}=x_t-\alpha\frac{\hat{m}_t}{\sqrt{\hat{v}_t}+\epsilon}
$$

### AdamW

Adamì˜ ë³€í˜•ìœ¼ë¡œ, Regularizationë¬¸ì œë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ ê³ ì•ˆë˜ì—ˆë‹¤.

## Learning Rate Decay

í•™ìŠµë¥  (learning rate)ì„ ê³ ì •í•˜ëŠ” ë°©ë²• ì™¸ì—ë„ ë‹¤ì–‘í•œ ì „ëµì´ ì¡´ì¬í•œë‹¤.

ê²½í—˜ì ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸°ë¥¼ $k$ë°° ì¦ê°€ì‹œí‚¤ë©´ í•™ìŠµë¥ ë„ $k$ë°° ì¦ê°€ì‹œí‚¤ëŠ” ê²ƒì´ íš¨ê³¼ì ì´ë¼ê³  í•œë‹¤. (ì´ë¥¼ linear scaling ruleì´ë¼ê³  í•¨)

### Step

íŠ¹ì • epochë§ˆë‹¤ learning rateë¥¼ ê°ì†Œì‹œí‚¤ëŠ” ë°©ì‹

![fig5](cs231n/03-5.png){: style="display:block; margin:0 auto; width:40%;"}

### Cosine

$$
\alpha_t=\frac{1}{2}\alpha_0(1+\cos(t\pi/T))
$$

![fig6](cs231n/03-6.png){: style="display:block; margin:0 auto; width:40%;"}

### Linear

$$
\alpha_t=\alpha_0(1-t/T)
$$

![fig7](cs231n/03-7.png){: style="display:block; margin:0 auto; width:40%;"}

### Invert Sqrt

$$
\alpha_t=\frac{\alpha_0}{\sqrt{t}}
$$

![fig8](cs231n/03-8.png){: style="display:block; margin:0 auto; width:40%;"}

## Linear Warmup

ì´ˆê¸°ì— ì„ í˜•ì ìœ¼ë¡œ learning rateë¥¼ ì¦ê°€ì‹œëŠ” ë°©ì‹

![fig9](cs231n/03-9.png){: style="display:block; margin:0 auto; width:40%;"}
