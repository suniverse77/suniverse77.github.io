---
title: "[CS231n] Regularization and Optimization"
date: 2025-09-06 00:00:00 +/-TTTT
categories: [딥러닝]
tags: [CS231n]
math: true
toc: true
author: sunho
description: 👨‍👧‍👧 Stanford CS231n | Spring 2025 | Lecture 3 
---

## Regularization

$$
L(W)=\underbrace{\frac{1}{N}\sum_{i=1}^NL_i(f(x_i,W),y_i)}_{\text{Data loss}}+\underbrace{\lambda R(W)}_{\text{Regularization}}
$$

**Data loss**

- 모델이 학습 데이터에 대해 올바른 예측을 하도록 유도한다.

**Regularization**

- 모델이 학습 데이터에 오버피팅되는 것을 방지해 일반화 성능을 높이는 역할을 한다.
- 입력 데이터와는 독립적인 항으로, 가중치 $W$의 크기나 형태만 제한한다.
- 가중치의 복잡도를 제어해 단순하고 안정적인 모델을 선호하도록 만듦.

### L1 Regularization (Lasso)

$$
R(W)=\sum_k\sum_l\lvert W_{k,l}\rvert
$$

Nearest Neighbor는 새로운 샘플이 들어오면 저장된 모든 샘플과의 거리를 계산해서 가장 가까운 샘플의 라벨을 따르는 방법이다. 학습과 예측의 과정은 아래와 같다.


### L2 Regularization (Lidge)

$$
R(W)=\sum_k\sum_lW^2_{k,l}
$$

새로운 입력이 들어오면 모든 샘플과의 거리를 비교하고, 그 중 가장 가까운 K개의 이웃을 선택한다.

선택된 K개의 이웃 라벨 중 일반적으로 다수결 투표 (Majority Voting)를 통해 라벨을 결정한다.

## Optimization

Numerical gradient

Analytic gradient

### SGD (Stochastic Gradient Descent)

알고리즘 실행마다 데이터 subset을 무작위로 샘플링하기 때문에 stochastic이라고 부른다.

$$
x_{t+1}=x_t-\alpha\nabla f(x_t)
$$

SGD에는 몇 가지 문제점이 존재한다.

1. Oscillation
    
    Loss function이 어떤 방향에서는 급격히 변하고, 다른 방향에서는 완만하게 변하는 경우가 있으며, 이때 Gradient Descent는 경사를 따라 지그재그로 움직이며 진동하면서 천천히 수렴하게 된다.
    
    ![fig2](cs231n/03-2.png){: .w-100}
    
    행렬의 condition number는 가장 큰 singular value와 가장 작은 singular value의 비율을 의미한다.
    
    Hessian은 loss 함수의 곡률을 나타내는 행렬로, Hessian의 condition number이 크다는 것은 어떤 방향은 가파르고, 어떤 방향은 평평하다는 뜻이다. → 최적화가 불안정해짐

2. Local minima or Saddle point
    - Loss surface에는 여러 개의 local minima와 saddle point가 존재한다.
    - Saddle point에서는 gradient가 0에 가까워져 학습이 정체될 수 있다.
    - 특히 고차원 공간에서는 local minima보다 saddle point가 더 흔하다.
    
    ![fig3](cs231n/03-3.png){: .w-70}
    
3. Stochasticity
    
    미니배치를 사용하기 때문에 loss 값이 매 스텝마다 흔들린다.
    
    ![fig4](cs231n/03-4.png){: .w-30}

### SGD + Momentum

단순히 gradient만 보는 대신, 속도를 고려한 방법이다.

$$
v_{t+1}=\rho v_t+\alpha\nabla f(x_t)
\\
x_{t+1}=x_t-\alpha v_{t+1}
$$

- $v$ (velocity): 이전까지의 여러 step의 누적 업데이트 방향을 나타낸다.
- $\rho$ (momentum): 과거 속도를 얼마나 반영할지를 결정한다. (클수록 과거 속도를 더 많이 반영한다.)

같은 방향의 gradient가 반복되면 점차 속도가 붙어 더 빨라진다.

곡률 차이가 큰 지형에서 진동을 줄이고 더 안정적인 방향으로 이동한다.

반드시 빠른 수렴을 보장하진 않지만, 더 나은 minimum을 찾도록 도와준다.

### RMSprop

gradient의 크기를 요소별로 조정하여 학습률을 스케일링한 방법이다.

$$
g_{t+1}=\gamma g_t+(1-\gamma)\left(\nabla f(x_t)\right)^2
\\
x_{t+1}=x_t-\frac{\alpha}{\sqrt{g_{t+1}+\epsilon}}\nabla f(x_t)
$$

gradient가 큰 파라미터는 업데이트 폭을 줄이고, 작은 파라미터는 더 크게 업데이트.

평평한 영역(gradient가 작은 영역)에서는 이동을 가속, 가파른 영역(gradient가 큰 영역)에서는 이동을 완화

방향마다 다른 스케일을 자동으로 보정해 학습을 안정화한다.

### Adam

Momentum + RMSprop을 결합한 방식이다.

- **Momentum**: 1차 모멘트(gradient 평균)를 추적
- **RMSprop**: 2차 모멘트(gradient 제곱 평균)를 추적

$$
m_{t+1}=\beta_1m_t+(1-\beta_2)\nabla f(x_t)~,~v_{t+1}=\beta_1v_t+(1-\beta_2)\left(\nabla f(x_t)\right)^2
\\
\hat{m}_{t+1}=\frac{m_{t+1}}{1-(\beta_1)^{t+1}}~,~\hat{v}_{t+1}=\frac{v_{t+1}}{1-(\beta_2)^{t+1}}
\\
x_{t+1}=x_t-\alpha\frac{\hat{m}_t}{\sqrt{\hat{v}_t}+\epsilon}
$$

### AdamW

Adam의 변형으로, Regularization문제를 개선하기 위해 고안되었다.

## Learning Rate Decay

학습률 (learning rate)을 고정하는 방법 외에도 다양한 전략이 존재한다.

경험적으로 배치 크기를 $k$배 증가시키면 학습률도 $k$배 증가시키는 것이 효과적이라고 한다. (이를 linear scaling rule이라고 함)

### Step

특정 epoch마다 learning rate를 감소시키는 방식

![fig5](cs231n/03-5.png){: style="display:block; margin:0 auto; width:40%;"}

### Cosine

$$
\alpha_t=\frac{1}{2}\alpha_0(1+\cos(t\pi/T))
$$

![fig6](cs231n/03-6.png){: style="display:block; margin:0 auto; width:40%;"}

### Linear

$$
\alpha_t=\alpha_0(1-t/T)
$$

![fig7](cs231n/03-7.png){: style="display:block; margin:0 auto; width:40%;"}

### Invert Sqrt

$$
\alpha_t=\frac{\alpha_0}{\sqrt{t}}
$$

![fig8](cs231n/03-8.png){: style="display:block; margin:0 auto; width:40%;"}

## Linear Warmup

초기에 선형적으로 learning rate를 증가시는 방식

![fig9](cs231n/03-9.png){: style="display:block; margin:0 auto; width:40%;"}
