---
title: "[CS231n] CNN Architectures"
date: 2025-09-09 00:00:00 +/-TTTT
categories: [딥러닝]
tags: [CS231n]
math: true
toc: true
author: sunho
description: 👨‍👧‍👧 Stanford CS231n | Spring 2025 | Lecture 6
---

# How to build CNNs?

## Layers in CNNs

### Normalization Layer

![fig1](cs231n/06-1.png){: style="display:block; margin:0 auto; width:100%;"}

**Batch Normalization**



**Layer Normalization**



### Dropout

Dropout은 학습 중에 확률적으로 뉴런을 꺼서 모델이 특정 뉴런에 오버피팅되는 것을 막는다.

매번 다른 뉴런이 꺼지기 때문에 일종의 앙상블 학습 효과를 낸다.

![fig2](cs231n/06-2.png){: style="display:block; margin:0 auto; width:100%;"}

테스트 때도 dropout을 적용하면 같은 이미지를 넣어도 매번 출력이 달라지기 때문에 예측의 일관성이 없어진다. 따라서 dropout은 학습에서만 사용하고 테스트에서는 사용하지 않는다.

하지만 학습에서만 사용할 경우 학습과 테스트 때 활성된 뉴런의 개수가 달라지기 때문에 각 뉴런의 출력값의 스케일이 달라지는 현상이 발생한다.

아래처럼 

$$
\mathbf{x}^\top\mathbf{w}=
\begin{bmatrix}1\\1\\1\\1\end{bmatrix}
\begin{bmatrix}1&1&1&1\end{bmatrix}=4
~~~,~~~
\mathbf{x}^\top\mathbf{w}=
\begin{bmatrix}1\\1\\1\\1\end{bmatrix}
\begin{bmatrix}1&0&1&0\end{bmatrix}=2
$$

이러한 현상을 방지하기 위해 학습 때 출력값에 $p$를 곱해서 출력 분포가 동일하도록 맞춘다.

## Activation Functions

활성화 함수는 모델에 비선형성을 도입하며, 여러 종류의 함수가 존재한다.

### Sigmoid

$$
\sigma(x)=\frac{1}{1+e^{-x}}
$$

Sigmoid 함수는 위와 같이 정의되며, 출력값의 범위를 0과 1사이로 매핑하기 때문에 확률로 해석할 때 주로 사용한다.

하지만 입력이 매우 큰 양수거나 음수일 경우 기울기가 0에 가까워진다는 문제점이 존재한다.

![fig3](cs231n/06-3.png){: style="display:block; margin:0 auto; width:60%;"}

### ReLU (Rectified Linear Unit)

$$
f(x)=\max(0,x)
$$

ReLU 함수는 위와 같이 정의되며, 음수 입력은 0으로, 양수 입력은 그대로 출력한다.

양수 영역에서는 기울기가 1이므로 saturation 문제가 없으며, sigmoid나 tanh보다 학습이 훨씬 빠르다.

하지만 여전히 음수 입력에 대해 기울기가 0이라는 문제점이 존재한다.

![fig4](cs231n/06-4.png){: style="display:block; margin:0 auto; width:60%;"}

### GeLU (Gaussian Error Linear Unit)

$$
f(x)=x\cdot\Phi(x)
$$

GeLU 함수는 위와 같이 정의된다.

ReLU에서 0에서 갑자기 꺾이는 불연속적인 변화를 부드럽게 만들었으며, 음수 구간에서도 0이 아닌 기울기를 가지기 때문에 학습 안정성이 개선되었다.

최근 트랜스포머 계열 모델에서 많이 사용되는 함수이다.

![fig5](cs231n/06-5.png){: style="display:block; margin:0 auto; width:60%;"}

### Others

그 외에도 다양한 활성화 함수가 존재한다.

![fig6](cs231n/06-6.png){: style="display:block; margin:0 auto; width:100%;"}

## CNN Architectures

### VGGNet

3x3 conv 장점은 각 레이어에서 수용영역을 항상 2를 더함
레이어 3개를 쌓으면 7x7 레이어 1개와 동일한 effective field가 있음, 매개변수도 더 적음
A3의 한 칸이 보는 영역은 input의 7x7

### ResNet

깊은 신경망은 얕은 신경망보다 표현력이 더 뛰어나야 하지만, 실제로 층이 깊어질수록 오히려 학습이 어려워지고 성능이 낮아지는 현상이 발생하였다. ResNet은 깊은 모델일수록 최적화하기 어렵다는 문제를 해결하기 위해 고안된 구조다.

항등함수 (Identity Function)를 넣으면 모델을 최소한 얕은 모델만큼 좋아야한다. 이를 이용해 항등 함수를 쉽게 학습할 수 있도록 skip connection을 추가하였다.

신경망은 여러 층을 통과하여 입력 $x$를 원하는 출력 $H(x)$로 매핑하는 것을 학습한다. ResNet에서는 $H(x)$를 직접 학습하는 대신 skip connection을 도입하여 잔차 $F(x)=H(x)-x$만 학습하도록 하였다.

잔차를 학습하는 것이 더 쉬운 이유는 2가지 관점으로 볼 수 있다.

**1. 최적화 관점**

만약 여러 층을 거쳐도 입력과 출력이 거의 같아야 하는 상황 (더 깊은 층이 필요 없는 상황)이라면, $H(x)$는 항등 함수에 가까워야 한다. 하지만 여러 층을 통과하며 이 항등 함수를 학습하는 것은 어렵다.

반면 ResNet은 $F(x)=H(x)−x$를 학습하므로 위와 동일한 상황에서 신경망은 단순히 $F(x)=0$을 학습하면 되며, 이는 최적화 관점에서 매우 쉬운 목표이다.

예를 들어, 출력이 입력과 똑같아야되는 상황에서 skip connection이 없다면 $x\to+2\to-2\to H(x)=x$ 이런 복잡한 매핑을 학습해야할 수도 있다. 이 경우 모든 층의 가중치가 정교하게 맞아야 하며, 복잡한 과정으로 항등 함수를 흉내내야하기 때문에 매우 어렵다.

하지만 skip connection이 있다면 이미 이 경로가 항등 함수를 제공하기 때문에 $x\to+0\to+0\to H(x)=F(x)+x$ 이렇게 간단히 $F(x)=0$만 학습해도 입력과 동일한 출력이 보장된다.

**2. 기울기 관점**

역전파로 전달되는 기울기는 연속적인 미분 값의 곱으로 표현되는데, 각 층에서 미분 값이 1보다 작다면 네트워크가 깊어질수록 점점 0에 가까워져 앞쪽 레이어까지 기울기가 잘 전달되지 않는 <span style="background-color:#fff5b1">기울기 소실 (Vanishing Gradient)</span> 현상이 발생한다.

ResNet에서는 skip connection에 의해 기울기가 아래와 같이 흐른다.

$$
y=F(x)+x~\to~\frac{\partial L}{\partial x}=\frac{\partial L}{\partial y}\left(\frac{\partial F(x)}{\partial x}+I\right)
$$

덧셈 구조 덕분에 $F(x)$의 기울기가 0에 가까워져도 $I$를 통해 기울기가 그대로 전달되기 때문에 기울기가 소실되지 않는다.

## Weight Initialization

### Kaiming / MSRA Initialization



# How to train CNNs?

## Data Preprocessing

## Data Augmentation

## Transfer Learning

## Hyperparameter Selection

