---
title: "[CS231n] CNN Architectures"
date: 2025-09-09 00:00:00 +/-TTTT
categories: [딥러닝]
tags: [CS231n]
math: true
toc: true
author: sunho
description: 📖 Stanford CS231n | Spring 2025 | Lecture 6
---

# How to build CNNs?

## Layers in CNNs

### Normalization Layer

Normalization은 데이터의 크기와 분포를 일정한 범위로 조정하는 task를 의미한다.

자세한 내용은 [Normalization]() 포스트에 설명하였다.

![fig1](cs231n/06-1.png){: style="display:block; margin:0 auto; width:100%;"}

보통 normalization을 설명할 때 위의 그림을 자주 사용한다. 

하지만 아래의 그림이 이해하기에는 더 쉬운 것 같다.

설명하기 전, 채널 크기를 $C$, 세로와 가로의 크기를 $H$와 $W$라고 할 때, 앞으로 입력의 크기를 $C\times H\times W$로 표현한다. 이러한 샘플이 $N$개 존재한다면 $N\times C\times H\times W$ 크기로 표현한다.

**Batch Normalization**

![fig2](cs231n/06-2.png){: style="display:block; margin:0 auto; width:80%;"}
_[[그림 출처]](https://blog.csdn.net/weixin_38346042/article/details/131882490)_

Batch Norm은 하나의 배치 내에서 각 채널 별로 정규화를 수행한다.

$N\times C\times H\times W$ 크기의 텐서에 대해 batch norm을 수행하면 결과의 크기는 $1\times C\times H\times W$가 된다.

**Layer Normalization**

![fig3](cs231n/06-3.png){: style="display:block; margin:0 auto; width:80%;"}
_[[그림 출처]](https://blog.csdn.net/weixin_38346042/article/details/131882490)_

Layer Norm은 하나의 샘플에 대해서 정규화를 수행한다.

$N\times C\times H\times W$ 크기의 텐서에 대해 layer norm을 수행하면 결과의 크기는 $N\times 1\times 1\times 1$가 된다.

**Instance Normalization**

![fig4](cs231n/06-4.png){: style="display:block; margin:0 auto; width:80%;"}
_[[그림 출처]](https://blog.csdn.net/weixin_38346042/article/details/131882490)_

Istance Norm은 각 샘플마다 채널 별로 정규화를 수행한다.

$N\times C\times H\times W$ 크기의 텐서에 대해 instance norm을 수행하면 결과의 크기는 $N\times 1\times 1\times C$가 된다.

**Group Normalization**

![fig5](cs231n/06-5.png){: style="display:block; margin:0 auto; width:80%;"}
_[[그림 출처]](https://blog.csdn.net/weixin_38346042/article/details/131882490)_

Group Norm은 각 샘플마다 채널을 $G$개의 그룹으로 묶어 정규화를 수행한다.

$G=C$이면 그룹의 개수가 총 $C$개라는 뜻이므로 instance norm과 동일한 기능을 수행하며, $G=1$이면 하나의 샘플을 통채로 정규화한다는 뜻이므로 layer norm과 동일한 기능을 수행한다.

그룹의 개수가 $G$일 때, $N\times C\times H\times W$ 크기의 텐서에 대해 group norm을 수행하면 결과의 크기는 $N\times 1\times 1\times G$가 된다.

---

위는 ConvNet에서의 동작 방식이었고, MLP에서의 Batch norm과 Layer norm의 동작은 아래와 같다. 기본적으로 매커니즘은 동일하다.

![fig6](cs231n/06-6.png){: style="display:block; margin:0 auto; width:80%;"}
_[[그림 출처]](https://yonghyuc.wordpress.com/2020/03/04/batch-norm-vs-layer-norm/)_

- **Batch Normalization**: 배치 내에서 각 feature 별로 정규화를 수행
- **Layer Normalization**: 하나의 샘플에 대해 정규화 수행

### Dropout

Dropout은 학습 중에 확률적으로 뉴런을 꺼서 모델이 특정 뉴런에 오버피팅되는 것을 막는다.

매번 다른 뉴런이 꺼지기 때문에 일종의 앙상블 학습 효과를 낸다.

![fig7](cs231n/06-7.png){: style="display:block; margin:0 auto; width:90%;"}

테스트 때도 dropout을 적용하면 같은 이미지를 넣어도 매번 출력이 달라지기 때문에 예측의 일관성이 없어진다. 따라서 dropout은 학습에서만 사용하고 테스트에서는 사용하지 않는다.

하지만 학습에서만 사용할 경우 학습과 테스트 때 활성된 뉴런의 개수가 달라지기 때문에 각 뉴런의 출력값의 스케일이 달라지는 현상이 발생한다.

아래처럼 

$$
\mathbf{x}^\top\mathbf{w}=
\begin{bmatrix}1\\1\\1\\1\end{bmatrix}
\begin{bmatrix}1&1&1&1\end{bmatrix}=4
~~~,~~~
\mathbf{x}^\top\mathbf{w}=
\begin{bmatrix}1\\1\\1\\1\end{bmatrix}
\begin{bmatrix}1&0&1&0\end{bmatrix}=2
$$

이러한 현상을 방지하기 위해 학습 때 출력값에 $p$를 곱해서 출력 분포가 동일하도록 맞춘다.

## Activation Functions

활성화 함수는 모델에 비선형성을 도입하며, 여러 종류의 함수가 존재한다.

### Sigmoid

$$
\sigma(x)=\frac{1}{1+e^{-x}}
$$

Sigmoid 함수는 위와 같이 정의되며, 출력값의 범위를 0과 1사이로 매핑하기 때문에 확률로 해석할 때 주로 사용한다.

하지만 입력이 매우 큰 양수거나 음수일 경우 기울기가 0에 가까워진다는 문제점이 존재한다.

![fig8](cs231n/06-3.png){: style="display:block; margin:0 auto; width:80%;"}

### ReLU (Rectified Linear Unit)

$$
f(x)=\max(0,x)
$$

ReLU 함수는 위와 같이 정의되며, 음수 입력은 0으로, 양수 입력은 그대로 출력한다.

양수 영역에서는 기울기가 1이므로 saturation 문제가 없으며, sigmoid나 tanh보다 학습이 훨씬 빠르다.

하지만 여전히 음수 입력에 대해 기울기가 0이라는 문제점이 존재한다.

![fig9](cs231n/06-9.png){: style="display:block; margin:0 auto; width:80%;"}

### GeLU (Gaussian Error Linear Unit)

$$
f(x)=x\cdot\Phi(x)
$$

GeLU 함수는 위와 같이 정의된다.

ReLU에서 0에서 갑자기 꺾이는 불연속적인 변화를 부드럽게 만들었으며, 음수 구간에서도 0이 아닌 기울기를 가지기 때문에 학습 안정성이 개선되었다.

최근 트랜스포머 계열 모델에서 많이 사용되는 함수이다.

![fig10](cs231n/06-10.png){: style="display:block; margin:0 auto; width:80%;"}

### Others

그 외에도 다양한 활성화 함수가 존재한다.

![fig11](cs231n/06-11.png){: style="display:block; margin:0 auto; width:100%;"}

## CNN Architectures

아래 그림은 연도별 ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 대회의 우승 모델의 에러율과 layer 깊이를 나타낸다.

ResNet을 기점으로 신경망의 layer가 매우 깊어진 것을 볼 수 있다.

![fig12](cs231n/06-12.png){: style="display:block; margin:0 auto; width:80%;"}

### VGGNet

VGGNet의 아키텍처는 아래와 같다.

<span style="background-color:#fff5b1">3x3 conv (stride=1)만 사용</span>한 것이 가장 큰 특징이며, conv 연산 때는 공간적 크기를 보존하고 크기를 줄일 때는 pooling 연산을 수행하였다.

![fig13](cs231n/06-13.png){: style="display:block; margin:0 auto; width:80%;"}

아래 그림에서 3x3 conv를 3번 사용하면 A3의 한 칸이 보는 영역이 Input의 7x7 영역인 것을 확인할 수 있다. 이는 7x7 conv를 1번 사용하는 것과 동일한 receptive field를 가지는 효과를 낸다.

![fig14](cs231n/06-14.png){: style="display:block; margin:0 auto; width:100%;"}

하지만 왜 3x3 conv를 3번 사용했을까? 정답은 동일한 receptive field를 가지지만 필요한 파라미터 수는 더 적기 때문이다.

입력과 출력의 채널 크기가 $C$라면 필터가 $C$개 있어야 하므로, 7x7 conv 1-layer는 $1\times[C\times (7\times7\times C)]=49C^2$개의 파라미터가 필요하다. 반면 3x3 conv 3-layer는 $3\times[C\times (3\times3\times C)]=9C^2$개의 파라미터만 필요하므로 더 적다. 

또한 layer가 더 깊어지므로 비선형성이 더 증가한다는 이점이 있다.

### ResNet

깊은 신경망은 얕은 신경망보다 표현력이 더 뛰어나야 하지만, 실제로 층이 깊어질수록 오히려 학습이 어려워지고 성능이 낮아지는 현상이 발생하였다.
ResNet은 깊은 모델일수록 최적화하기 어렵다는 문제를 해결하기 위해 고안된 구조다.

![fig15](cs231n/06-15.png){: style="display:block; margin:0 auto; width:100%;"}

항등함수 (Identity Function)를 넣으면 모델을 최소한 얕은 모델만큼 좋아야한다. 

![fig16](cs231n/06-16.png){: style="display:block; margin:0 auto; width:100%;"}

이를 이용해 항등 함수를 쉽게 학습할 수 있도록 skip connection을 추가하였다.

![fig17](cs231n/06-17.png){: style="display:block; margin:0 auto; width:80%;"}

신경망은 여러 층을 통과하여 입력 $x$를 원하는 출력 $H(x)$로 매핑하는 것을 학습한다. ResNet에서는 $H(x)$를 직접 학습하는 대신 skip connection을 도입하여 잔차 $F(x)=H(x)-x$만 학습하도록 하였다.

![fig18](cs231n/06-18.png){: style="display:block; margin:0 auto; width:80%;"}

잔차를 학습하는 것이 더 쉬운 이유는 2가지 관점으로 볼 수 있다.

**1. 최적화 관점**

만약 여러 층을 거쳐도 입력과 출력이 거의 같아야 하는 상황 (더 깊은 층이 필요 없는 상황)이라면, $H(x)$는 항등 함수에 가까워야 한다. 하지만 여러 층을 통과하며 이 항등 함수를 학습하는 것은 어렵다.

반면 ResNet은 $F(x)=H(x)−x$를 학습하므로 위와 동일한 상황에서 신경망은 단순히 $F(x)=0$을 학습하면 되며, 이는 최적화 관점에서 매우 쉬운 목표이다.

예를 들어, 출력이 입력과 똑같아야되는 상황에서 skip connection이 없다면 $x\to+2\to-2\to H(x)=x$ 이런 복잡한 매핑을 학습해야할 수도 있다. 이 경우 모든 층의 가중치가 정교하게 맞아야 하며, 복잡한 과정으로 항등 함수를 흉내내야하기 때문에 매우 어렵다.

하지만 skip connection이 있다면 이미 이 경로가 항등 함수를 제공하기 때문에 $x\to+0\to+0\to H(x)=F(x)+x$ 이렇게 간단히 $F(x)=0$만 학습해도 입력과 동일한 출력이 보장된다.

**2. 기울기 관점**

역전파로 전달되는 기울기는 연속적인 미분 값의 곱으로 표현되는데, 각 층에서 미분 값이 1보다 작다면 네트워크가 깊어질수록 점점 0에 가까워져 앞쪽 레이어까지 기울기가 잘 전달되지 않는 <span style="background-color:#fff5b1">기울기 소실 (Vanishing Gradient)</span> 현상이 발생한다.

ResNet에서는 skip connection에 의해 기울기가 아래와 같이 흐른다.

$$
y=F(x)+x~\to~\frac{\partial L}{\partial x}=\frac{\partial L}{\partial y}\left(\frac{\partial F(x)}{\partial x}+I\right)
$$

덧셈 구조 덕분에 $F(x)$의 기울기가 0에 가까워져도 $I$를 통해 기울기가 그대로 전달되기 때문에 기울기가 소실되지 않는다.

## Weight Initialization

### Kaiming / MSRA Initialization

![fig19](cs231n/06-19.png){: style="display:block; margin:0 auto; width:100%;"}

![fig20](cs231n/06-20.png){: style="display:block; margin:0 auto; width:100%;"}

![fig21](cs231n/06-21.png){: style="display:block; margin:0 auto; width:100%;"}

폭발하는 문제를 해결할 수는 있지만 최적화는 여전히 어려울 수 있다.

# How to train CNNs?

## Data Preprocessing for Image

이미지에 대한 전처리는 매우 간단하다.

각 RGB 채널의 평균과 표준편차를 계산해 normalizaiton을 수행하면 된다.

데이터셋에서 미리 계산된 값을 이용해서 할 수도 있음
가장 일반적인 것은 ImageNet의 평균과 표준편차 이용

## Data Augmentation

regularization 효과, 오버피팅 방지에 효과적

데이터 세트의 크기를 효과적으로 늘릴 수 있다.

![fig22](cs231n/06-22.png){: style="display:block; margin:0 auto; width:100%;"}

데이터를 증강하는 방법에는 여러 가지가 있다.

### Horizontal Flips

이미지를 수평으로 뒤집는 방법이다.

![fig23](cs231n/06-23.png){: style="display:block; margin:0 auto; width:60%;"}

### Random crops and scales

원본 이미지의 크기를 다양하게 조절한 뒤, 그중 일부를 무작위로 잘라내는 방법이다. 과정은 아래와 같다.

1. $[256,480]$ 사이에서 랜덤하게 $L$ 값을 선택한다.
2. 이미지의 가로, 세로 중 짧은 부분을 $L$로 설정하고, 긴 부분을 비율에 맞게 조정한다. 

    예를 들어 $L=300$일 때, $240\times360$ 크기의 이미지는 $300\times450$으로 scaling된다.
3. 랜덤한 위치를 선택해 특정 크기로 (예를 들어 $224\times 224$) 잘라낸다.

이 방법을 통해서 모델은 고양이의 얼굴만 크게 확대된 이미지, 몸통 일부만 보이는 이미지 등 다양한 상황을 학습할 수 있다.

![fig24](cs231n/06-24.png){: style="display:block; margin:0 auto; width:60%;"}

### Color Jitter

이미지의 대비와 밝기 등 색상과 관련된 속성값을 랜덤하게 조절하는 방법이다.

![fig25](cs231n/06-25.png){: style="display:block; margin:0 auto; width:60%;"}

### Cutout

이미지의 특정 영역을 랜덤으로 선택하여 검은색 또는 회색 사각형으로 가려버리는 방법이다.

![fig26](cs231n/06-26.png){: style="display:block; margin:0 auto; width:60%;"}

## Transfer Learning

![fig27](cs231n/06-27.png){: style="display:block; margin:0 auto; width:60%;"}

![fig28](cs231n/06-28.png){: style="display:block; margin:0 auto; width:60%;"}

## Hyperparameter Selection

![fig29](cs231n/06-29.png){: style="display:block; margin:0 auto; width:60%;"}

![fig30](cs231n/06-30.png){: style="display:block; margin:0 auto; width:60%;"}

![fig31](cs231n/06-31.png){: style="display:block; margin:0 auto; width:60%;"}

![fig32](cs231n/06-32.png){: style="display:block; margin:0 auto; width:60%;"}
