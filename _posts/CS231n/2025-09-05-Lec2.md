---
title: "[CS231n] Image Classification with Linear Classifiers"
date: 2025-09-05 00:00:00 +/-TTTT
categories: [딥러닝]
tags: [CS231n]
math: true
toc: true
author: sunho
description: 👨‍👧‍👧 Stanford CS231n | Spring 2025 | Lecture 2 
---

## Data-Driven Approach

사진에서는 피사체의 자세, 카메라의 각도, 조명 세기와 같은 작은 변화에도 픽셀값들이 크게 달라질 수 있기 때문에 미리 정해진 규칙만으로 이미지를 분석하는 것은 어렵다.

따라서 대규모의 이미지 데이터와 그에 대응하는 레이블을 컴퓨터에 학습시켜 스스로 분류 규칙을 학습하도록 하는 접근법이 필요하며, 이를 data-driven approach라고 부른다.

이미지 분류를 위한 대표적인 데이터 기반 접근법으로는 Nearest Neighbor Classifier와 Linear Classifier가 있다.

## Nearest Neighbor

Nearest Neighbor는 새로운 샘플이 들어오면 저장된 모든 샘플과의 거리를 계산해서 가장 가까운 샘플의 라벨을 따르는 방법이다. 학습과 예측의 과정은 아래와 같다.

**Training**

- 나중에 새로운 이미지와 비교하기 위해 단순히 이미지와 라벨을 저장한다.
- 사본만 보관하기 때문에 O(1)의 시간이 소요된다.

**Predict**

- 새로운 샘플이 들어오면 보유하고 있는 모든 샘플과 거리를 비교하여 가장 가까운 샘플을 찾고, 해당 샘플의 라벨로 분류한다.
- 샘플이 N개 있을 때, 비교를 N번 하기 때문에 O(N)의 시간이 소요된다.

### K-Nearest Neighbor (KNN)

새로운 입력이 들어오면 모든 샘플과의 거리를 비교하고, 그 중 가장 가까운 K개의 이웃을 선택한다.

선택된 K개의 이웃 라벨 중 일반적으로 다수결 투표 (Majority Voting)를 통해 라벨을 결정한다.

![fig1](cs231n/02-1.png){: width="100%"}

### Distance Metric

Distance Metric은 두 데이터 사이의 유사도를 수치로 표현하는 방법으로, 일반적으로 두 점 사이의 거리를 계산하여 정의된다.

Distance Metric에는 크게 L1 Distance와 L2 Distance가 있다.

![fig2](cs231n/02-2.png){: width="100%"}

**L1 Distance (Manhattan Distance)**

좌표축에 평행한 직각 경로를 따라 측정한 거리를 의미한다.

$$
d(\mathbf x,\mathbf y)=\sum_i\lvert x_i-y_i\rvert
$$

- 좌표축 기준으로 계산하기 때문에, 특징 축이 회전된다면 동일한 점이라도 거리가 달라진다. → 데이터를 어떤 축 기준에서 보느냐에 따라 거리가 달라진다.
- 절댓값을 단순히 합산하기 때문에 outlier에 덜 민감하다.

**L2 Distance (Euclidean Distance)**

두 점을 직선으로 연결한 최단 거리를 의미한다.

$$
d(\mathbf x,\mathbf y)=\sum_i\sqrt{(x_i-y_i)^2}
$$

- 특징 축을 회전해도 거리가 달라지지 않는다.
- 제곱 때문에 큰 차이가 있으면 그 부분이 전체 계산에서 차지하는 비중이 압도적으로 커지기 때문에 outlier에 민감하다.

![fig3](cs231n/02-3.png){: width="100%"}

이미지 분류에 Nearest Neighbor를 사용하면 위와 같이 픽셀에 바로 distance metric을 적용하기 때문에 의미있는 유사도를 얻지 못한다.

예를들어 아래에서 오른쪽 3개의 이미지는 모두 왼쪽의 original 이미지와 픽셀 거리가 같은데, 시각적 의미를 반영하지 못한다.

![fig4](cs231n/02-4.png){: width="50%"}

### Setting Hyperparameters

![fig5](cs231n/02-5.png){: width="100%"}

1. Training data에서 잘 동작하는 하이퍼파라미터 선택
    
    훈련 데이터에 맞춰 최적화되므로 오버피팅이 발생하고 ****실제 새로운 데이터에 일반화되지 않는다.
    
2. Test data에서 잘 동작하는 하이퍼파라미터 선택
    
    테스트 데이터를 사용해 튜닝하는 것은 사실상 정답을 미리 본 것과 같기 때문에 부정행위에 해당한다.
    
3. Validation data에서 잘 동작하는 하이퍼파라미터 선택
    
    훈련 데이터를 일부 나누어 검증 데이터로 사용하는 것으로, 만약 검증 데이터 크기가 작으면 전체 데이터 분포를 대표하지 못할 수 있다는 문제점이 있다.

**k-Fold Cross-Validation**

![fig6](cs231n/02-6.png){: width="100%"}

- 훈련 데이터를 k개의 fold로 분할하여 각 fold를 한 번씩 validation으로 사용하고, 나머지 k-1개의 fold로 학습하는 방식이다.
- 최종적으로 모든 fold에서의 성능을 평균하여 하이퍼파라미터를 선택한다.
- 데이터가 적을 때 특히 유용하다.

## Linear Classifier

![fig7](cs231n/02-7.png){: width="100%"}

### Interpreting a Linear Classifier

**Algebraic Viewpoint**

![fig8](cs231n/02-8.png){: width="100%"}

$W$의 각 행은 특정 클래스에 대응하는 가중치 벡터로, 입력 $x$와 내적하여 가장 점수가 큰 클래스로 분류한다.

**Visual Viewpoint**

가중치 행렬의 각 행은 해당 클래스가 어떻게 생겼는지를 대략적으로 나타내는 일종의 템플릿이다.

![fig9](cs231n/02-9.png){: width="100%"}

위에서 아래 10개의 그림은 각 클래스의 가중치 행을 시각화한 것으로, car의 경우 자동차의 패턴이 보인다.

**Geometric Viewpoint**

이미지를 공간 위의 하나의 점으로 생각하고, Linear Classifier를 각 점을 구분하는 경계선으로 본다.

![fig10](cs231n/02-10.png){: width="100%"}

**고차원 공간에서의 이미지**

- 32x32x3 크기의 이미지의 경우, 이 이미지는 총 3,072개의 숫자들로 이루어진 데이터로 볼 수 있다.
- 3,072개의 숫자 하나하나를 각 차원의 좌표 값으로 생각하면, 이 이미지는 3,072차원 공간에 있는 하나의 점으로 표현할 수 있다.
- 같은 클래스의 이미지들은 고차원 공간에서 비슷한 영역에 모여 분포한다.

**분류기의 기하학적 의미**

- 서로 다른 클래스의 점들을 잘 나눌 수 있는 결정 경계(Decision Boundary)를 찾는 것.
- 이 결정 경계는 **고차원 공간 속의 초평면(hyperplane)**이며, 경계를 기준으로 한쪽은 고양이, 다른 쪽은 강아지로 분류됨.
- Weight $W$는 결정 경계의 기울기를 결정하며, 결정 경계에 수직인 방향을 가리킨다. 이 방향으로 갈수록 해당 클래스일 점수가 높아짐을 의미한다.
- Bias $b$는 결정 경계의 위치를 결정하며, 경계선을 데이터 분포에 맞게 평행 이동시켜 최적의 위치에 놓는 역할을 한다.

**학습 과정**

- 주어진 데이터를 바탕으로 정답을 가장 잘 분리할 수 있도록 최적의 $W$와 $b$ 값을 찾는 과정이다.
- 직관적으로는, 데이터 점들이 찍힌 그래프 위에 선을 이리저리 그어보며 두 그룹을 가장 잘 나누는 위치를 찾는 것과 같다.

### Softmax Classifier

$$
\text{Softmax}(s_i)=\frac{\exp(s_i)}{\sum_{j}\exp(s_j)}
$$

Softmax 함수는 위와 같이 정의되며, classifier의 raw score를 확률 분포로 변환할 때 사용한다.

![fig11](cs231n/02-11.png){: width="100%"}
![fig12](cs231n/02-12.png){: width="100%"}
![fig12](cs231n/02-13.png){: width="100%"}

Softmax 함수를 통해 얻은 확률 값은 MLE로 해석할 수도 있고, 실제 확률 분포와 비교하여 KL Divergence나 Cross Entropy를 손실 함수로 사용할 수도 있다.

### Hinge Loss (SVM Loss)

정답 점수와 오답 점수가 margin 이상 차이나지 않으면 패널티를 주는 손실 함수이다.

$$
L_i=\sum_{j\not=y_i}\max(0,s_j-s_{y_i}+1)
$$

Hinge Loss는 위와 같이 정의되며, $s_j$는 클래스 $j$에 대한 score, $s_{y_i}$는 정답 클래스 $y_i$에 대한 score를 의미한다.

$+1$은 margin으로 정답 클래스가 오답 클래스보다 최소한 1만큼은 크게 점수를 가져야 한다는 것을 의미한다.

![fig14](cs231n/02-14.png){: width="100%"}

위에서 고양이 이미지의 hinge loss를 계산하면 아래와 같다.

![fig15](cs231n/02-15.png){: width="100%"}
