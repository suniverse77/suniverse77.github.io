---
title: "Test"
date: 2024-01-01 00:00:00 +/-TTTT
categories: [인공지능 수학]
tags: [선형대수]
math: true
toc: true
author: sunho
img_path: /assets/images/math/
description: 👨‍👧‍👧 벡터의 기본 개념
---

## Vector Space

공집합이 아닌 집합 $\mathcal{V}$와 스칼라들의 집합(Field) $F$가 있을 때,

집합 $\mathcal{V}$에 벡터 덧셈 ($+$)과 스칼라 곱셈 ($\cdot$)의 두 가지 연산이 정의되어 있고, 아래 10가지 공리를 만족하면 집합 $\mathcal{V}$를 벡터 공간이라고 하며, $\mathcal{V}$의 원소를 벡터라고 한다.

즉, 단순히 벡터들의 집합이 아니라, 이 집합 내의 원소들이 특정 규칙을 만족하는 공간을 의미한다

이때, 벡터 덧셈은 $\mathcal{V} \times \mathcal{V} \to \mathcal{V}$로, 스칼라 곱셈은 $F \times \mathcal{V} \to \mathcal{V}$로 가는 함수로, 연산을 수행해도 다시 집합 $\mathcal{V}$ 내의 원소가 나온다.

### 벡터 덧셈에 대한 공리 (벡터 공간은 아벨 군을 이룬다)

1. 닫힘 (Closure): 임의의 벡터 $\mathbf{u}, \mathbf{v} \in \mathcal{V}$에 대해, $\mathbf{u} + \mathbf{v}$는 $\mathcal{V}$에 속한다.

2. 교환 법칙 () 성립: 임의의 벡터 $\mathbf{u}, \mathbf{v} \in \mathcal{V}$에 대해, $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$가 성립한다.

3. 결합 법칙 (Associativity) 성립: 임의의 벡터 $\mathbf{u}, \mathbf{v}, \mathbf{w} \in \mathcal{V}$에 대해, $(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$가 성립한다.

4. 항등원 (Identity) 존재: V 내에 모든 벡터 $\mathbf{u} \in \mathcal{V}$에 대해 $\mathbf{u} + \mathbf{0} = \mathbf{u}$를 만족하는 유일한 영벡터 $\mathbf{0}$이 존재한다.

5. 역원 (Inverse) 존재: 임의의 벡터 $\mathbf{u} \in \mathcal{V}$에 대해, $\mathbf{u} + (-\mathbf{u}) = \mathbf{0}$를 만족하는 유일한 역벡터 $-\mathbf{u}$가 V 내에 존재한다.

### 스칼라 곱셈에 대한 공리

6. 닫힘 (Closure): 임의의 스칼라 $c\in F$와 벡터 $\mathbf{u} \in \mathcal{V}$에 대해, $c \cdot \mathbf{u}$는 $\mathcal{V}$에 속한다.

7. 스칼라 덧셈에 대한 분배 법칙: 임의의 스칼라 $c,d\in F$와 벡터 $\mathbf{u} \in \mathcal{V}$에 대해, $(c+d) \cdot \mathbf{u} = c \cdot \mathbf{u} + d \cdot \mathbf{u}$가 성립한다.

8. 벡터 덧셈에 대한 분배 법칙: 임의의 스칼라 $c\in F$와 벡터 $\mathbf{u}, \mathbf{v} \in \mathcal{V}$에 대해, $c \cdot (\mathbf{u} + \mathbf{v}) = c \cdot \mathbf{u} + c \cdot \mathbf{v}$가 성립한다.

9. 스칼라 곱셈에 대한 결합 법칙 (Associativity of scalar multiplication): 임의의 스칼라 c,d∈F와 벡터 $\mathbf{u} \in \mathcal{V}$에 대해, $(cd) \cdot \mathbf{u} = c \cdot (d \cdot \mathbf{u})$가 성립한다.

10. 항등원 (Identity) 존재: 스칼라 집합 F의 곱셈 항등원인 1에 대해, 임의의 벡터 $\mathbf{u} \in \mathcal{V}$에 대해 $1 \cdot \mathbf{u} = \mathbf{u}$가 성립한다.

## Vector Subspace

벡터공간 $V$의 부분집합 $U$가 아래의 조건을 만족하면, $U$를 $V$의 subspace라고 한다.

1. $V$에 존재하는 $\mathbf{0}$ (zero vector)를 포함해야 한다.
2. $U$는 덧셈과 스칼라배에 대해 닫혀있어야 한다.

좌표 공간에서의 subspace는 원점을 포함해야 한다.

- $\mathbb{R}^2$ 공간에서 vector subspace는 원점을 포함하는 직선
- $\mathbb{R}^3$ 공간에서 vector subspace는 원점을 포함하는 직선 또는 평면



- 첫 번째 그림은 스칼라배에 대해 닫혀있지 않으므로, $\mathbb{R}^2$의 subspace가 아니다.
- 두 번째 그림은 원점을 포함하지 않으므로, $\mathbb{R}^2$의 subspace가 아니다.
- 세 번째 그림은 $\mathbb{R}^2$의 subspace가 아니다.
- 네 번째 그림은 subspace의 조건을 다 만족하므로, $\mathbb{R}^2$의 subspace이다.



- 해당 직선에 있는 벡터들끼리 덧셈을 하거나 벡터에 스칼라배를 할 경우, 직선 밖으로 나가기 때문에 subspace가 아니다.



- 해당 직선에 있는 벡터들끼리 덧셈을 하거나 벡터에 스칼라배를 해도 직선 내에 존재하기 때문에 subspace이다.



## Null Space

Homogeneous Equation의 solution을 모두 모아놓은 집합을 Null Space라고 부른다.

$$
N(A)=\lbrace\mathbf{x}\mid A\mathbf{x}=\mathbf{0}\rbrace
$$
    
- 어떠한 null space든지 항상 $\mathbf{0}$를 포함하므로, vector space이다.
- Null space의 차원을 **nullity**라고 하며, free variable의 개수와 동일하다.

## 선형 결합 (Linear Combination)

선형 결합이란 

벡터를 상수배하고 더하는 두 가지 기본 연산으로 이루어져 있는 1차식 형태이기 때문에 **선형**이고, 여러 개의 벡터를 함께 사용하여 새로운 벡터를 만들어내기 때문에 **결합**이라고 표현한다.

$$
\mathbf{v}=c_1\mathbf{v}_1+\cdots+c_n\mathbf{v}_n
$$

## 선형 생성 (Span)

벡터 집합 $\mathcal{V}$에 있는 벡터들의 가능한 모든 선형 결합으로 만들어지는 집합을 $\text{span}(\mathcal{V})$라고 한다.

2차원 공간에서 $\mathbf{v}$와 $\mathbf{w}$의 span은 두 벡터의 모든 선형 결합의 집합 $a\mathbf{v}+b\mathbf{w}$이며, 두 벡터가 일렬로 있지 않다면 span의 결과는 2차원 공간의 벡터 전체가 된다.

<details>
<summary><font color='red'>Example</font></summary>
<div markdown="1">
<br>

$$
\text{span}(\begin{bmatrix}1\\0\end{bmatrix},\begin{bmatrix}0\\1\end{bmatrix})=\mathbb{R}^2
$$

$$
\text{span}(\begin{bmatrix}2\\1\end{bmatrix},\begin{bmatrix}1\\3\end{bmatrix})=\begin{bmatrix}2a+b\\a+3b\end{bmatrix}=\mathbb{R}^2
$$

</div>
</details>
<br>

## 선형 독립 (Linear Independence)

벡터들의 집합 $\lbrace\mathbf{v}_1,\dots,\mathbf{v}_n\rbrace$이 주어졌을 때,

$$
\mathbf{0}=\lambda_1\mathbf{v}_1+\cdots+\lambda_n\mathbf{v}_n
$$

을 만족하는 스칼라 $\lambda_1,\cdots,\lambda_n$이 오직 $\lambda_1=\cdots=\lambda_2=0$일 때만 존재한다면 (trivial solution만 존재한다면), 이 벡터 집합을 선형 독립이라고 한다.

만약 0이 아닌 $\lambda$가 존재한다면 $\lambda_1\mathbf{v}_1=-\lambda_2\mathbf{v}_2-\lambda_n\mathbf{v}_3$처럼 한 벡터가 다른 벡터들의 조합으로 표현될 수 있으며, 이를 선형 종속이라고 한다.

즉, 선형 독립이란 집합 내의 어떤 벡터도 다른 벡터들의 조합으로 만들어지지 않는다는 것을 의미한다.

선형 독립이 중요한 이유는 해당 벡터들이 span할 수 있는 공간의 차원과 연관되어 있기 때문이다.

예를 들어, $\mathbf{v}_1=(1,1)$과 $\mathbf{v}_2=(2,2)$는 서로 비례 관계이므로 선형 종속이다. 따라서 두 벡터를 아무리 조합해도 결국 하나의 직선 (1차원 공간)밖에 만들 수 없다. 이 경우 하나의 벡터를 제거해도 span되는 공간은 변하지 않는다.

반면 $\mathbf{v}_1=(1,1)$과 $\mathbf{v}_2=(1,2)$는 선형 독립이므로, 이들의 조합으로 2차원 전체를 다 표현할 수 있다. 이 경우에는 벡터 중 하나라도 제거하면 span되는 공간의 차원이 줄어들게 된다.

### 선형 독립의 성질

1. 벡터 집합 내 하나라도 $\mathbf{0}$이 존재한다면, 해당 집합은 선형 종속이다. ($\mathbf{0}$은 스칼라배로 표현될 수 있기 때문이다.)
2. 행렬에서 non-pivot column은 항상 그 왼쪽에 존재하는 pivot column들의 선형 결합으로 표현된다.
   
   $$
   \begin{bmatrix}1&3&0\\0&0&2\end{bmatrix} → \begin{bmatrix}3\\0\end{bmatrix}=3\begin{bmatrix}1\\0\end{bmatrix}
   $$

### 선형 독립을 판단하는 방법

벡터 집합이 선형 독립인지 판단하는 방법에는 크게 2가지가 있다.

**1. 행렬의 pivot column 확인**

   열벡터로 이루어진 행렬을 REF로 변환했을 때 모든 열이 pivot column이면, 벡터 집합은 선형 독립이다.
    
   즉, pivot column들이 선형 독립이어야 한다.
    
**2. 동차 방정식 (Homogeneous Equation)의 해 확인**

   동차 방정식 $\sum\lambda_i\mathbf{v}_i=\mathbf{0}$의 해 $\boldsymbol\lambda$가 오직 trivial solution만 존재한다면, 벡터 집합은 선형 독립이다.
    
   즉, 변수 $\lambda_i$에 free variable이 존재하면 안된다.

<details>
<summary><font color='red'>Example</font></summary>
<div markdown="1">

아래의 벡터들이 선형 독립인지 판단

$$
\begin{bmatrix}1\\2\\-3\\4\end{bmatrix}
~,~
\begin{bmatrix}1\\1\\0\\2\end{bmatrix}
~,~
\begin{bmatrix}-1\\-2\\1\\1\end{bmatrix}
$$

---

**1. 벡터들을 행렬로 표현**

   $$
   \begin{bmatrix}1&1&-1\\2&1&-2\\-3&0&1\\4&2&1\end{bmatrix}
   $$

**2. 가우스 소거법을 이용해 REF로 변환**

   $$
   \begin{bmatrix}1&1&-1\\0&1&0\\0&0&1\\0&0&0\end{bmatrix}
   $$

**3-1. 행렬의 pivot column 확인**

모든 열이 pivot column이므로, 세 벡터는 선형 독립이다.

**3-2. 동차 방정식의 해 확인**

동차방정식의 해가 trivial solution이므로, 세 벡터는 선형 독립이다.
   
$$
\lambda_1\begin{bmatrix}1\\0\\0\\0\end{bmatrix}+\lambda_2\begin{bmatrix}1\\1\\0\\0\end{bmatrix}
+\lambda_3\begin{bmatrix}-1\\0\\1\\0\end{bmatrix}=\begin{bmatrix}0\\0\\0\\0\end{bmatrix}
$$

위 방정식을 만족시키는 해는 trivial solution밖에 없다.

$$
\boldsymbol{\lambda}=\begin{bmatrix}0\\0\\0\end{bmatrix}
$$

</div>
</details>

## 기저 (Basis)

벡터 공간의 basis란, 공간 전체를 생성할 수 있는 선형 독립인 벡터들의 집합을 말한다.

즉, 벡터 공간 $V$를 span하는 가장 작은 집합 $B$를 $V$의 basis라고 부르며, 이는 특정 공간에서 좌표축 역할을 하는 벡터들의 집합이다.

Basis는 아래와 같이 표현 가능하다.

- $B$는 $V$의 minimal generating set이다. 

   (벡터를 하나라도 제거하면 더 이상 $V$ 전체를 생성할 수 없다.)

- $B$는 $V$의 maximally linearly independent set이다.

   (벡터를 하나라도 추가하면 선형 독립성이 깨진다.)

### 직교 기저 (Orthogonal Basis)

Basis를 이루는 벡터들이 서로 직교(orthogonal)하는 경우, 직교 기저라고 부른다.

$$
\mathbf{u}_i\cdot\mathbf{u}_j=0~,~(i\not=j)
$$

직교 기저가 중요한 이유는 2가지가 있다.

1. 서로 직교한다는 것은 <span style="background-color:#fff5b1">선형 독립을 자동으로 보장</span>해준다.

2. 어떤 벡터를 basis들의 선형 결합으로 나타낼 때, 계수를 쉽게 구할 수 있다.

$$
\mathbf{v}=\sum c_i\mathbf{u}_i
~\to~
c_i=\frac{\mathbf{v}\cdot\mathbf{u}_i}{\mathbf{u}_i\cdot\mathbf{u}_i}
$$

<details>
<summary><font color='#0000FF'>증명</font></summary>
<div markdown="1">

$$
\mathbf{v}=\sum c_i\mathbf{u}_i
$$

1. 양변에 $\mathbf{u}_k$를 내적함

$$
\mathbf{v}\cdot\mathbf{u}_k=\left(\sum c_i\mathbf{u}_i\right)\cdot\mathbf{u}_k=\sum c_i(\mathbf{u}_i\cdot\mathbf{u}_k)
$$

2. $\mathbf{u}_i\cdot\mathbf{u}_j=0~,~(i\not=j)$이기 때문에 우변에는 $i=k$인 항만 남음

$$
\mathbf{v}\cdot\mathbf{u}_k=c_k(\mathbf{u}_k\cdot\mathbf{u}_k)
$$

3. $c_1$만 남기고 이항

$$
c_k=\frac{\mathbf{v}\cdot\mathbf{u}_k}{\mathbf{u}_k\cdot\mathbf{u}_k}
$$

</div>
</details>
<br>

### 정규 직교 기저 (Orthonormal Basis)

Basis를 이루는 벡터들이 서로 직교하고 각각의 크기가 1일 경우, 정규 직교 기저라고 부른다.

$$
\mathbf{u}_i\cdot\mathbf{u}_j=
\begin{cases}
1&(i=j)\\
0&(i\not=j)
\end{cases}
$$

정규 직교 기저는 분모가 사라져 더 단순한 계산으로 계수를 구할 수 있다.

$$
c_i=\mathbf{v}\cdot\mathbf{u}_i
$$

### Gram-Schmidt method

Basis 집합 $B=\lbrace\mathbf{b}_1,\dots,\mathbf{b}_n\rbrace$를 orthonormal basis로 변환하는 방법에는 Gram-Schmidt method가 있다.

<details>
<summary><font color='#FF0000'>Example</font></summary>
<div markdown="1">

$$
B=
\begin{bmatrix}

\end{bmatrix}
$$

---
1. $\mathbf{b}_2$를 $\mathbf{b}_1$ 방향과 $\mathbf{b}_1$에 수직한 벡터의 합으로 분리

3. $\mathbf{b}_1$에 수직한 벡터를 구함

5. 크기를 1로 조절해 orthonormal basis로 변환


</div>
</details>

## 차원 (Dimension)

벡터 공간 $V$의 basis의 개수를 $V$의 차원이라고 부른다.

벡터의 차원과 벡터 공간의 차원은 다르다.

- 벡터의 차원은 벡터를 구성하는 원소의 개수를 의미한다.

   $$
   \mathbf{v}=\begin{bmatrix}2\\4\\6\end{bmatrix}~\to~\text{dim}(\mathbf{v})=3
   $$

- 벡터 공간의 차원은 공간을 생성하는 선형 독립 벡터의 개수를 의미한다.

   $$
   V=\text{span}\left(\begin{bmatrix}1\\3\\5\end{bmatrix},\begin{bmatrix}2\\4\\6\end{bmatrix}\right)~\to~\text{dim}(V)=2
   $$

부분공간의 차원은 항상 원래 공간의 차원보다 작거나 같다.

$$
U\subseteq V \implies \text{dim}(U)\leq \text{dim}(V)
$$

## 랭크 (Rank)

열 벡터로 이루어진 행렬 $A$에서 선형 독립인 열의 개수를 rank라고 한다.

즉, 열 벡터들이 span하는 공간의 최대 차원을 나타낸다.

행렬의 rank가 행렬의 열 수와 같으면 full rank라고 부른다.

### Rank의 성질

1. $\text{rank}(A)=\text{rank}(A^T)$
2. $A\in\mathbb{R}^{m\times n}$에 대해 $\text{rank}(A)=\min(m,n)$의 경우, 해당 행렬을 full rank라고 한다.
3. Square matrix $A\in\mathbb{R}^{n\times n}$에 대해 $\text{rank}(A)=n$일 경우, 해당 행렬은 invertible함
4. $\text{rank}(A)=\text{rank}(A\mid \mathbf{b})$인 경우에만, $A\mathbf{x}=\mathbf{b}$의 해를 구할 수 있다.

<details>
<summary><font color='#FF0000'>Example</font></summary>
<div markdown="1">
  
$$
A=\begin{bmatrix}1&2\\3&4\\5&6\end{bmatrix}
$$

---

Gauss Elimination을 하면 $\text{rank}(A)=2$

즉, 3개의 column vector들이 span하는 공간 $V$의 차원은 2차원 평면이다.

</div>
</details>
