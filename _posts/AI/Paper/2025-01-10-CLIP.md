---
title: "[논문리뷰] CLIP: Learning Transferable Visual Models From Natural Language Supervision"
date: 2025-01-10 00:00:00 +/-TTTT
categories: [AI, 논문리뷰]
tags: [Multimodal, Image Generation]
math: true
toc: true
author: sunho
description: 📝 ICML 2021
---

[[Paper]](https://arxiv.org/abs/2103.00020)

## Introduction

### NLP에서의 변화

과거에는 기계가 언어를 배우기 위해 번역, 질문-답변 등 task마다 별도의 라벨링된 데이셋을 만들어야 했다.

그런데 최근에는 웹에 존재하는 원시 텍스트 (raw text)만으로도 학습할 수 있는 방법이 발전했다. 이 방식에서는 더이상 별도의 라벨링이 필요하지 않다.

여기에 '입력도 텍스트, 출력도 텍스트 (text-to-text)'라는 규칙을 적용하면서, 모든 task를 아래와 같이 단순한 텍스트 변환 문제로 통일할 수 있게 되었다.

- 영어 문장을 넣으면 번역된 한국어도 텍스트
- 질문을 넣으면 답변도 텍스트

이러한 아이디어를 극단적으로 확장한 모델이 GPT-3이다.
GPT-3는 번역을 위한 별도의 데이터로 학습하지 않고, 단지 수많은 웹 텍스트만 학습했을 뿐인데 요청만 하면 다양한 작업을 수행할 수 있었다.

즉, 추가 학습 없이 새로운 task를 수행할 수 있는 zero-shot transfer  능력이 생긴 것이다.

### Computer Vision에서의 적용

컴퓨터 비전 분야에서는 대부분의 모델이 ImageNet과 같이 사전에 정해진 라벨이 붙은 데이터셋으로 pre-training되는 것이 일반적이었다.

이러한 방식은 새로운 개념이나 task를 다루기 위해, 추가 라벨링 데이터를 모아 추가적인 학습이 필요하다는 한계가 있었다.

이때 자연어를 supervision으로 사용하면 컴퓨터 비전에서도 돌파구를 만들 수 있지 않을까 하는 의문이 제기되었다.

사실 과거에도 image-text 학습에 대한 시도가 존재했었지만, 대부분 성능이 낮고 softmax classifier 기반의 구조를 사용해 고정된 클래스 집합 내에서만 예측이 가능하였다.

Natural language supervision을 비전에 적용하기 위해 인터넷에서 수집한 4억 쌍의 image-text 데이터셋을 구축했고, contrastive learning objective를 통해 이미지와 텍스트의 올바른 짝을 맞추는 방식으로 모델을 학습시켰다. 

이를 통해 CLIP은 별도의 fine-tuning 없이도 다양한 컴퓨터 비전 task에서 zero-shot transfer가 가능하다는 것을 보여주었다.

## Methods

### Contrastive pre-training

![fig1](paper/clip-1.png){: style="display:block; margin:0 auto; width:80%;"}

$N$개의 이미지와 텍스트 쌍을 각각 인코더에 통과시켜 image representation $I_n$와 text representation $T_n$를 얻는다. $n$은 인덱스 번호를 의미한다.

학습 과정에서는 $N\times N$개의 pair 중에서 $N$개의 positive pair에 대해서는 cosine similarity가 높아지도록, 나머지 $N^2-N$개의 negative pair에 대해서는 cosine similarity가 낮아지도록 학습을 시킨다.

이렇게 계산된 similarity score를 기반으로 cross entropy loss를 사용해 이미지 인코더와 텍스트 인코더를 동시에 학습시킨다.

본 연구에서는 이미지 인코더로 ResNet-50, Vision Transformer를, 텍스트 인코더로 Transformer를 이용했다고 한다.

아래는 pseudo code이다.

![fig2](paper/clip-2.png){: style="display:block; margin:0 auto; width:60%;"}

위에서 `I_f`와 `T_f`는 인코더를 거쳐 얻은 feature representation이며, `I_e`와 `T_e`는 추가적인 linear projection을 거쳐 얻은 multimodal embedding을 의미한다.

- **Feature representation**: 인코더가 추출한 각각 모달리티의 feature 표현
- **Multimodal embedding**: 두 모달리티가 projection을 거친 뒤 공통된 공간에 놓인 벡터 (이미지와 텍스트가 같은 공간에 놓여짐)

`loss_i`는 이미지를 기준으로 했을 때 정답 텍스트를 맞추는 loss, `loss_t`는 텍스트를 기준으로 했을 때 정답 이미지를 맞추는 loss를 의미한다.

### Zero-shot prediction

학습된 CLIP 모델은 다양한 downstream task에 적용될 수 있다.

![fig3](paper/clip-3.png){: style="display:block; margin:0 auto; width:80%;"}

예를 들어, 이미지 분류 task에서는 먼저 이미지 인코더를 통해 입력 이미지의 representation을 추출한다. 이어서 텍스트 인코더를 사용해 `A photo of a {label}`과 같은 프롬프트를 각 클래스 라벨에 대해 변환하여 텍스트 representation을 얻는다.

이후 이미지 representation과 각 텍스트 representation 간의 cosine similarity를 계산하고, 가장 유사도가 높은 라벨을 최종 예측값으로 선택한다. 예를 들어, similarity가 'dog' 라벨 문장과 가장 높다면, 해당 이미지는 dog로 분류된다.

모델이 'dog', 'cat', 'plane'과 같은 클래스에 대해 fine-tuning을 하지 않고, 텍스트 설명만으로 새로운 라벨을 정의하고 예측할 수 있기 때문에 zero-shot prediction이라고 한다.

### Prompt engineering

CLIP에서는 단순히 `cat`이라는 단어를 바로 텍스트 인코더에 넣지 않고, `A photo of cat`과 같은 문장 형태의 프롬프트로 변환해 텍스트 인코더에 입력한다.

이렇게 하는 이유는 다의성이 있는 단어가 있고, 실제 인터넷에는 단어보다 문장 형태가 더 많이 등장하기 때문이다.

또한 프롬프트 문장은 커스터마이징할 수 있다. 예를 들어, pet 데이터셋에 맞추어 `A photo of a cat, a type of pet`과 같이 문장을 확장하면 정확도가 향상되는 결과를 얻을 수 있다.
