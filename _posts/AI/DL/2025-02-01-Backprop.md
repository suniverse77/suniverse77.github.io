---
title: "[NN] 역전파 (Backpropagation)"
date: 2025-02-01 06:00:00 +/-TTTT
categories: [AI, 딥러닝]
tags: [신경망]
math: true
toc: true
author: sunho
---

## Backpropagation

매번 새로운 손실 함수에 대해 $\nabla_WL$을 계산하는 것은 매우 비효율적이다.

그렇다면 신경망에서 어떻게 효율적으로 그라디언트를 계산할 수 있을까?

### Computational Graphs

신경망의 연산은 덧셈, 곱셈 등의 일련의 기본 연산으로 구성되기 때문에 이를 계산 그래프로 표현할 수 있다.

- **순전파 (Forward pass)**에서는 입력에서 출발해 그래프를 따라가면서 각 노드의 출력을 차례대로 계산하며, 최종적으로 손실 함수 값 $L$을 얻는다.
- **역전파 (Backward pass)**에서는 출력에서 입력 방향으로 그래프를 거슬러 올라가면서 chain rule을 이용해 각 파라미터에 대한 손실의 기울기 $\nabla_WL$를 계산한다.

![fig4](cs231n/04-4.png){: style="display:block; margin:0 auto; width:100%;"}

### Backprop with Scalars

스칼라 함수

![fig5](cs231n/04-5.png){: style="display:block; margin:0 auto; width:80%;"}

<details>
<summary><font color='#FF0000'>Example 1</font></summary>
<div markdown="1">



---

</div>
</details>

<details>
<summary><font color='#FF0000'>Example 2</font></summary>
<div markdown="1">



---

</div>
</details>
<br>



### Backprop with Vectors

벡터 함수

### Backprop with Matrices (Tensors)
