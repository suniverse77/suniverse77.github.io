---
title: "[평가지표] Metric"
date: 2025-03-01 00:00:00 +/-TTTT
categories: [AI, 딥러닝]
tags: [3D Vision]
math: true
toc: true
author: sunho
---

## Classification 평가 지표

분류 평가 지표는 모델의 예측이 실제 정답과 얼마나 일치하는지 측정하며, 혼동 행렬 (Confusion Matrix)을 기반으로 한다.

![fig1](dl/metric/1-1.png){: style="display:block; margin:0 auto; width:40%;"}
_[[출처]](https://www.blog.trainindata.com/confusion-matrix-precision-and-recall/)_

**Positive/Negative**는 모델이 예측한 값을, **True/False**는 그 예측이 맞았는지 틀렸는지를 의미한다.

- **TP (True Positive)** : 실제 **Positive**인 것을 **Positive**라고 올바르게 예측한 경우
- **TN (True Negative)** : 실제 **Negative**인 것을 **Negative**라고 올바르게 예측한 경우
- **FP (False Positive)** : 실제 **Negative**인 것을 **Positive**라고 잘못 예측한 경우
- **FN (False Negative)** : 실제 **Positive**인 것을 **Negative**라고 잘못 예측한 경우

<details>
<summary><font color='#FF0000'>Example 1</font></summary>
<div markdown="1">



</div>
</details>
<br>

![fig2](dl/metric/1-2.png){: style="display:block; margin:0 auto; width:80%;"}
_[[출처]](https://velog.io/@jjw9599/ConfusionMatrix-ClassificationEvaluation)_

### 정확도 (Accuracy)

Accuracy는 전체 데이터 중에서 모델이 올바르게 예측한 비율을 의미한다.

$$
\text{Accuracy}=\frac{\vphantom{\big(}\text{TP}+\text{TN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}}
$$

데이터가 불균형할 때 성능을 제대로 평가하기 어렵다는 단점이 있다.

예를 들어 99%가 정상, 1%가 불량인 경우, 모델이 전부 정상으로 예측해도 99%의 정확도가 나오게 된다.

### 정밀도 (Precision)

Precision은 모델이 **Positive**라고 <span style="background-color:#FFE6E6">예측</span>한 것들 중에서, 실제로 **Positive**인 것 (정답을 맞춤)의 비율을 의미한다.

$$
\text{Precision}=\frac{\vphantom{\big(}\text{TP}}{\text{TP}+\text{FP}}
$$

이 지표는 모델이 **Positive** 예측을 얼마나 잘 수행했는지를 나타내며, <span style="background-color:#fff5b1">실제 **Negative**인 데이터를 잘못 잡으면 안될 때</span> 중요하다. 즉, $\text{FP}$가 치명적인 경우에 중요하다.
    
스팸 메일 분류기를 예로 들 수 있다.

메일 분류기는 정상 메일 (**Negative**)을 스팸 (**Positive**)으로 분류하면 안된다. 만약, 정상 메일을 스팸으로 분류한다면 $\text{FP}$가 증가하게 된다.

### 재현율 (Recall, Sensitivity)

Recall은 <span style="background-color:#FFE6E6">실제</span> **Positive**인 것들 중에서, 모델이 **Positive**라고 올바르게 찾아낸 비율을 의미한다.

$$
\text{Recall}=\frac{\vphantom{\big(}\text{TP}}{\text{TP}+\text{FN}}
$$

이 지표는 모델이 실제 **Positive** 데이터를 얼마나 빠짐없이 잘 찾아내는가를 나타내며, <span style="background-color:#fff5b1">실제 **Positive**인 데이터를 놓치면 안 될 때</span> 중요하다. 즉, 잘못 예측한 $\text{FN}$이 치명적인 경우에 중요하다.

암 진단 모델을 예로 들 수 있다.

암 진단 모델은 실제 암 환자 (**Positive**)를 정상 (**Negative**)으로 진단하면 안된다. 만약, 암 환자를 정상으로 진단한다면 $\text{FN}$이 증가하게 된다.

### F1 Score

F1 Score는 Precision과 Recall의 조화 평균이다.

$$
\text{F1 Score}=2\times\frac{\vphantom{\big(}\text{Precision}\times\text{Recall}}{\text{Precision}+\text{Recall}}
$$

데이터가 불균형할 때 Accuracy 대신 사용하며, Precision과 Recall 모두 중요할 때 사용한다.

### AUROC (Area Under ROC Curve)

ROC (Receiver Operating Characteristic) 곡선은 모델의 임계값 변화에 따라 **FPR (False Positive Rate)**이 변할 때 **TPR (True Positive Rate, Recall)**이 어떻게 변하는지를 그린 곡선이다.

- **FPR**: 가짜를 진짜라고 잘못 판단할 비율로, 값이 $0$에 가까울수록 좋다.
- **TPR**: 진짜를 진짜라고 정확히 맞힐 비율로, 값이 $1$에 가까울수록 좋다.

이때, AUROC는 이 ROC 곡선 아래의 면적을 의미하며, 값이 $1$에 가까울수록 모델의 성능이 좋다고 본다.

![fig3](dl/metric/1-3.png){: style="display:block; margin:0 auto; width:50%;"}
_[[출처]](https://www.blog.trainindata.com/auc-roc-analysis/)_

#### TPR과 FPR의 관계

해당 부분은 '공돌이의 수학정리노트'님의 [*'ROC curve'*](https://angeloyeo.github.io/2020/08/05/ROC.html) 포스트를 참고하였습니다.

![fig4](dl/metric/1-4.png){: style="display:block; margin:0 auto; width:50%;"}
_[[출처]](https://www.blog.trainindata.com/auc-roc-analysis/)_

예를 들어, 의사 A는 모든 환자들을 다 암환자라고 판단한다고 하자.

그러면 실제로 암에 걸린 환자들은 모두 암 환자로 판정되고, 암에 걸리지 않은 환자들도 모두 암 환자로 판정된다.

즉, True Positive Rate와 False Positive Rate가 모두 높아지게 된다.

이는 아래와 같이 threshold가 낮다는 뜻이다.

![fig5](dl/metric/1-5.gif){: style="display:block; margin:0 auto; width:60%;"}
_[[출처]](https://www.blog.trainindata.com/auc-roc-analysis/)_

한편, 의사 B는 모든 환자들을 암환자가 아니라고 판단한다고 하자.

그러면 암에 걸리지 않은 환자들 뿐만 아니라, 실제로 암에 걸린 환자들도 모두 정상인으로 판정되게 된다.

즉, True Positive Rate와 False Positive Rate 모두 낮아지게 된다.

이는 아래와 같이 threshold가 높다는 뜻이다.

![fig6](dl/metric/1-6.gif){: style="display:block; margin:0 auto; width:60%;"}
_[[출처]](https://www.blog.trainindata.com/auc-roc-analysis/)_

#### 곡선 위의 점의 의미

곡선 위의 한 점은 특정 임계값 (Threshold)에서의 FPR과 TPR의 관계를 나타낸다.

즉, AUROC는 임계값을 0%부터 100%까지 쭉 변경해 가면서 얻어지는 (FPR, TPR) 좌표들을 모두 이어 그린 선인 것을 알 수 있다.

![fig7](dl/metric/1-7.gif){: style="display:block; margin:0 auto; width:80%;"}
_[[출처]](https://angeloyeo.github.io/2020/08/05/ROC.html)_

#### 곡선의 휘어짐 정도의 의미

곡선의 휘어짐은 모델이 **Positive**와 **Negative** 클래스를 얼마나 잘 구별하는지를 나타낸다.

모델이 두 클래스를 더 잘 구별할수록, ROC 커브는 좌상단에 더 가까워지게 된다.

![fig8](dl/metric/1-8.gif){: style="display:block; margin:0 auto; width:80%;"}
_[[출처]](https://angeloyeo.github.io/2020/08/05/ROC.html)_

## Object Detection & Segmentation 평가 지표

### IoU (Intersection over Union)

IoU는 모델이 예측한 영역과 실제 정답 영역이 얼마나 겹치는지를 나타내는 지표이며, Object Detection과 Semantic Segmentation에서 사용된다.

$$
\text{IoU}=\frac{\text{Area of Overlap}}{\text{Area of Union}}
$$

$0\sim1$ 사이의 값을 가지며, $1$이면 완벽히 일치한다는 의미이다.

Object Detection에서는 이 IoU 값이 특정 임계값을 넘어야 올바르게 탐지했다고 판단한다.

![fig9](dl/metric/2-1.png){: style="display:block; margin:0 auto; width:50%;"}
_[[출처]](https://glee1228.tistory.com/5)_

#### mIoU (mean IoU)

mIoU는 모든 클래스의 IoU 값들을 더해 평균낸 값을 의미한다.

$$
\text{mIoU}=\frac{1}{C}\sum_{i=1}^C\text{IoU}_i
$$

여기서 $C$는 클래스 개수를 의미한다.

### AP (Average Precision)

AP는 하나의 클래스에 대해 모델이 얼마나 정밀하게 탐지하는지를 종합적으로 나타내는 지표이며, Object Detection에서 사용된다.

AP는 PR 곡선 (Precision-Recall Curve) 곡선의 아래쪽 면적을 계산한 값이다.

![fig10](dl/metric/2-2.png){: style="display:block; margin:0 auto; width:50%;"}
_[[출처]](https://ctkim.tistory.com/entry/mAPMean-Average-Precision-%EC%A0%95%EB%A6%AC)_

#### mAP (mean Average Precision)

mAP는 모든 클래스의 AP 값들의 평균을 의미한다.

$$
\text{mAP}=\frac{1}{C}\sum_{i=1}^C\text{AP}_i
$$

여기서 $C$는 클래스 개수를 의미한다.

## 생성 모델 평가 지표

### IS (Inception Score)

IS는 생성된 이미지가 얼마나 고품질이고 얼마나 다양한지를 동시에 평가하는 지표이다.

이 지표는 생성된 이미지 $x$를 Inception 모델에 입력하여 평가한다.

$$
\text{IS}=\exp\left(\mathbb{E}_{x\sim p_g}\left[D_{KL}(p(y\mid x)~||~p(y))\right]\right)
$$

- **높은 품질**

    Inception 모델에 생성된 이미지 $x$를 입력했을 때, 특정 클래스 $y$로 정확하게 분류되어야 한다. (엔트로피가 낮은 분포)

    즉, 조건부 확률 분포 $p(y\mid x)$의 엔트로피는 뾰족한 분포여야 한다.
- **높은 다양성**

    전체 클래스에 대한 주변 확률 분포 $p(y)$는 균등한 분포여야 한다. (엔트로피가 높은 분포)

IS가 높다는 것은 두 확률 분포 $p(y\mid x)$와 $p(y)$의 차이가 크다는 것을 의미한다. 즉, <span style="background-color:#fff5b1">IS는 높을수록 좋다.</span>

하지만 실제 이미지와 비교하지 않고, 생성된 이미지 자체로만 평가하기 때문에 현실성이 떨어질 수 있다는 단점이 있다.

### FID (Fréchet Inception Distance)

FID는 생성된 이미지의 통계적 분포가 실제 이미지의 통계적 분포와 얼마나 유사한지를 측정하는 지표이다.

마찬가지로, Inception 모델을 이용해서 평가한다.

$$
\text{FID}=\lVert\mu_r-\mu_g\rVert^2+\text{tr}\left(\Sigma_r+\Sigma_g-2(\Sigma_r\Sigma_g)^{\frac{1}{2}}\right)
$$

실제 이미지와 생성된 이미지를 Inception 모델에 통과시켜 중간 레이어의 특징 벡터를 추출한다.

이후 두 분포의 평균 $\mu$와 공분산 $\Sigma$를 구한 뒤, Fréchet Distance 공식을 사용해 두 분포 간의 거리를 계산한다.

FID가 낮다는 것은 실제 이미지와 생성된 이미지의 분포가 통계적으로 거의 동일하다는 의미이다. 즉, <span style="background-color:#fff5b1"> FID는 낮을수록 좋다.</span>

### CLIP Score


