---
title: "[CNN] CNN에서의 데이터 정규화"
date: 2025-02-06 00:00:00 +/-TTTT
categories: [AI, 딥러닝]
tags: [CS231n]
math: true
toc: true
author: sunho
---

## Weight Initialization

가중치를 초기화하는 여러 가지 시도가 있었다.

### Small random numbers

가중치를 랜덤한 작은 값들로 초기화하는 방법이 있다.

```python
# 4096개 뉴런을 가진 레이어 7개 (입력 + 은닉6)
dims = [4096] * 7

hs = []

# 가우시안 분포를 따르는 초기 입력 데이터 생성 (16개 샘플, 4096개 특징)
x = np.random.rand(16, dims[0])

for Din, Dout in zip(dims[:-1], dims[1:]):
    # 가우시안 분포를 따르는 4096x4096 크기의 가중치 행렬 생성
    W = np.random.randn(Din, Dout)

    # 표준편차를 0.01로 만듦
    W = 0.01 * W
    
    # 순전파: 행렬 곱 + ReLU 활성화 함수
    x = np.maximum(0, x.dot(W))
    hs.append(x)
```

첫 번째 layer에서 출력 activation의 분산은 대략 $1^2\times 0.01^2\times4096\approx0.4$가 된다. 그럼 두 번째 layer의 입력의 분산이 1보다 작은 $0.4$가 되므로 layer를 지날수록 activation의 분산은 0으로 수렴하게 된다.

입력과 가중치의 평균이 0이므로, 출력의 평균도 0이 된다. 하지만 ReLU 함수는 음수 영역을 제거하기 때문에 출력의 평균은 양수의 어떤 값이 된다. 하지만 출력의 분산이 작다는 것은 결국 0 근처에 모여있다는 뜻이기 때문에 평균 또한 0으로 수렴하게 된다. 

![fig19](cs231n/06-19.png){: style="display:block; margin:0 auto; width:100%;"}

시각화를 한 결과, 가중치의 평균과 표준편차가 점점 작아져 0에 수렴하는 것을 볼 수 있다.

### Large random numbers

가중치를 랜덤한 큰 값들로 초기화하는 방법이 있다.

```python
dims = [4096] * 7
hs = []
x = np.random.rand(16, dims[0])

for Din, Dout in zip(dims[:-1], dims[1:]):
    W = np.random.randn(Din, Dout)

    # 표준편차를 0.05로 만듦
    W = 0.05 * W
    
    x = np.maximum(0, x.dot(W))
    hs.append(x)
```

첫 번째 layer에서 출력 activation의 분산은 대략 $1^2\times 0.05^2\times4096\approx10.24$가 된다. 그럼 두 번째 layer의 입력의 분산이 1보다 큰 $10.24$가 되므로 layer를 지날수록 activation의 분산은 발산하게 된다.

위에서 말했듯이 ReLU 함수는 음수 영역을 제거하기 때문에 출력의 평균은 양수의 어떤 값이 되며, 분산이 크기 때문에 평균이 점점 큰 양수쪽으로 움직이게 된다.

![fig20](cs231n/06-20.png){: style="display:block; margin:0 auto; width:100%;"}

시각화를 한 결과, 가중치의 평균과 표준편차가 점점 커져 발산하는 것을 볼 수 있다.

### Kaiming He Initialization

ReLU 함수를 사용할 때 신호가 layer를 지나면서 사라지거나 폭발하지 않도록, 가중치를 평균이 $0$, 표준편차가 $\sqrt{\frac{2}{n_{in}}}$인 가우시안 분포를 따르도록 초기화하는 것을 제안하였다.

$$
W\sim \mathcal{N}(0,\sqrt{\frac{2}{n_{in}}})
$$

- $n_{in}$은 입력 노드의 개수이다.

```python
dims = [4096] * 7
hs = []
x = np.random.rand(16, dims[0])

for Din, Dout in zip(dims[:-1], dims[1:]):
    # 
    W = np.random.randn(Din, Dout) * np.sqrt(2/Din)
    x = np.maximum(0, x.dot(W))
    hs.append(x)
```

![fig21](cs231n/06-21.png){: style="display:block; margin:0 auto; width:100%;"}

시각화를 한 결과, layer를 거쳐도 가중치의 평균과 표준편차가 유지되는 것을 볼 수 있다.

# How to train CNNs?

## Data Preprocessing for Image

이미지에 대한 전처리는 매우 간단하다.

각 RGB 채널의 평균과 표준편차를 계산해 normalizaiton을 수행하면 된다.

특정 데이터셋에서 미리 계산된 값을 이용할 수도 있다. 가장 일반적인 것은 ImageNet의 평균과 표준편차를 이용하는 것이다.

## Data Augmentation

데이터 세트의 크기를 증가시키는 것으로, 오버피팅을 방지할 수 있다.

![fig22](cs231n/06-22.png){: style="display:block; margin:0 auto; width:100%;"}

데이터를 증강하는 방법에는 여러 가지가 있다.

### Horizontal Flips

이미지를 수평으로 뒤집는 방법이다.

![fig23](cs231n/06-23.png){: style="display:block; margin:0 auto; width:60%;"}

### Random crops and scales

원본 이미지의 크기를 다양하게 조절한 뒤, 그중 일부를 무작위로 잘라내는 방법이다. 과정은 아래와 같다.

1. $[256,480]$ 사이에서 랜덤하게 $L$ 값을 선택한다.
2. 이미지의 가로, 세로 중 짧은 부분을 $L$로 설정하고, 긴 부분을 비율에 맞게 조정한다. 

    예를 들어 $L=300$일 때, $240\times360$ 크기의 이미지는 $300\times450$으로 scaling된다.
3. 랜덤한 위치를 선택해 특정 크기로 (예를 들어 $224\times 224$) 잘라낸다.

이 방법을 통해서 모델은 고양이의 얼굴만 크게 확대된 이미지, 몸통 일부만 보이는 이미지 등 다양한 상황을 학습할 수 있다.

![fig24](cs231n/06-24.png){: style="display:block; margin:0 auto; width:40%;"}

### Color Jitter

이미지의 대비와 밝기 등 색상과 관련된 속성값을 랜덤하게 조절하는 방법이다.

![fig25](cs231n/06-25.png){: style="display:block; margin:0 auto; width:60%;"}

### Cutout

이미지의 특정 영역을 랜덤으로 선택하여 검은색 또는 회색 사각형으로 가려버리는 방법이다.

![fig26](cs231n/06-26.png){: style="display:block; margin:0 auto; width:60%;"}
