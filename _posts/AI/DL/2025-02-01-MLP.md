---
title: "[NN] MLP (Multi-Layer Perceptron)"
date: 2025-02-01 00:00:00 +/-TTTT
categories: [AI, 딥러닝]
tags: [신경망]
math: true
toc: true
author: sunho
---

## Linear Classifier

![fig7](cs231n/02-7.png){: style="display:block; margin:0 auto; width:80%;"}

### Interpreting a Linear Classifier

**Algebraic Viewpoint**

![fig8](cs231n/02-8.png){: style="display:block; margin:0 auto; width:100%;"}

$W$의 각 행은 특정 클래스에 대응하는 가중치 벡터로, 입력 $x$와 내적하여 가장 점수가 큰 클래스로 분류한다.

**Visual Viewpoint**

가중치 행렬의 각 행은 해당 클래스가 어떻게 생겼는지를 대략적으로 나타내는 일종의 템플릿이다.

![fig9](cs231n/02-9.png){: style="display:block; margin:0 auto; width:100%;"}

위에서 아래 10개의 그림은 각 클래스의 가중치 행을 시각화한 것으로, car의 경우 자동차의 패턴이 보인다.

**Geometric Viewpoint**

이미지를 공간 위의 하나의 점으로 생각하고, Linear Classifier를 각 점을 구분하는 경계선으로 본다.

![fig10](cs231n/02-10.png){: style="display:block; margin:0 auto; width:80%;"}

**고차원 공간에서의 이미지**

- 32x32x3 크기의 이미지의 경우, 이 이미지는 총 3,072개의 숫자들로 이루어진 데이터로 볼 수 있다.
- 3,072개의 숫자 하나하나를 각 차원의 좌표 값으로 생각하면, 이 이미지는 3,072차원 공간에 있는 하나의 점으로 표현할 수 있다.
- 같은 클래스의 이미지들은 고차원 공간에서 비슷한 영역에 모여 분포한다.

**분류기의 기하학적 의미**

- 서로 다른 클래스의 점들을 잘 나눌 수 있는 결정 경계(Decision Boundary)를 찾는 것.
- 이 결정 경계는 **고차원 공간 속의 초평면(hyperplane)**이며, 경계를 기준으로 한쪽은 고양이, 다른 쪽은 강아지로 분류됨.
- Weight $W$는 결정 경계의 기울기를 결정하며, 결정 경계에 수직인 방향을 가리킨다. 이 방향으로 갈수록 해당 클래스일 점수가 높아짐을 의미한다.
- Bias $b$는 결정 경계의 위치를 결정하며, 경계선을 데이터 분포에 맞게 평행 이동시켜 최적의 위치에 놓는 역할을 한다.

**학습 과정**

- 주어진 데이터를 바탕으로 정답을 가장 잘 분리할 수 있도록 최적의 $W$와 $b$ 값을 찾는 과정이다.
- 직관적으로는, 데이터 점들이 찍힌 그래프 위에 선을 이리저리 그어보며 두 그룹을 가장 잘 나누는 위치를 찾는 것과 같다.

Activation, 함수를 매핑, 특징을 질문,

## Neural Networks

하나의 layer로 구성된 신경망은 아래와 같이 정의된다.

$$
f=Wx
$$

2개의 layer를 쌓으면, 아래와 같이 표현할 수 있다. (ReLU 함수 사용)

$$
f=W_2\max(0,W_1x)
$$

만약 layer 사이에 비선형 함수를 넣지 않는다면, 여러 layer를 쌓아도 결국 하나의 선형 변환과 동일해진다. 

비선형 함수는 입력 공간을 변형하여 원래 선형적으로 분리되지 않던 데이터를 구분 가능하게 만드는 역할을 한다.

예를 들어, 아래의 그림은 유클리디안 좌표계에서 선형 분리가 불가능했던 데이터가, 극좌표계로 변환되면서 쉽게 분리가 가능해진 상황을 보여준다.

![fig1](cs231n/04-1.png){: style="display:block; margin:0 auto; width:90%;"}

신경망의 구조는 아래처럼 간단히 도식화할 수 있으며, 그림에서 보이는 동그라미는 뉴런 (neuron)을 의미한다.

입력층은 layer로 세지 않기 때문에 아래 그림에서 신경망은 3개의 layer로 구성되어 있다고 할 수 있다. (hiddne layer 2개 + 출력 layer 1개)

![fig2](cs231n/04-2.png){: style="display:block; margin:0 auto; width:90%;"}

일반적으로 뉴런의 개수가 많아질수록 분리 능력이 향상된다.

![fig3](cs231n/04-3.png){: style="display:block; margin:0 auto; width:90%;"}

## Backpropagation

매번 새로운 손실 함수에 대해 $\nabla_WL$을 계산하는 것은 매우 비효율적이다.

그렇다면 신경망에서 어떻게 효율적으로 그라디언트를 계산할 수 있을까?

### Computational Graphs

신경망의 연산은 덧셈, 곱셈 등의 일련의 기본 연산으로 구성되기 때문에 이를 계산 그래프로 표현할 수 있다.

- **순전파 (Forward pass)**에서는 입력에서 출발해 그래프를 따라가면서 각 노드의 출력을 차례대로 계산하며, 최종적으로 손실 함수 값 $L$을 얻는다.
- **역전파 (Backward pass)**에서는 출력에서 입력 방향으로 그래프를 거슬러 올라가면서 chain rule을 이용해 각 파라미터에 대한 손실의 기울기 $\nabla_WL$를 계산한다.

![fig4](cs231n/04-4.png){: style="display:block; margin:0 auto; width:100%;"}

### Backprop with Scalars

스칼라 함수

![fig5](cs231n/04-5.png){: style="display:block; margin:0 auto; width:80%;"}

<details>
<summary><font color='#FF0000'>Example 1</font></summary>
<div markdown="1">



---

</div>
</details>

<details>
<summary><font color='#FF0000'>Example 2</font></summary>
<div markdown="1">



---

</div>
</details>
<br>



### Backprop with Vectors

벡터 함수

### Backprop with Matrices (Tensors)
