---
title: "[생성 모델] GAN (Generative Adversarial Network)"
date: 2025-10-02 00:00:00 +/-TTTT
categories: [AI, 생성 모델]
tags: [생성 모델]
math: true
toc: true
author: sunho
---

## GAN

GAN은 크게 생성자 (Generator)와 판별자 (Discriminator)로 구성되어 있다. Generator는 실제와 유사한 데이터를 생성하는 역할을 하며, Discriminator는 입력 데이터가 실제인지 가짜인지 구별하는 역할을 한다.

이름에서 알 수 있듯이, Generator와 Discriminator라는 두 개의 신경망이 서로 적대적으로 경쟁하며 학습하는 구조를 가진다.

이들의 개념은 주로 위조지폐범과 경찰로 비유된다.

![fig1](AI/Generative/GAN-1.png){: style="display:block; margin:0 auto; width:70%;"}
_[[출처]](https://baechu-story.tistory.com/12)_

- **Generator (위조 지폐범)**: 진짜와 구분할 수 없을 만큼 정교한 가짜 지폐를 만드는 것이 목표이다.
- **Discriminator (경찰)**: 진짜 지폐와 가짜 지폐를 정확하게 구별해내는 것이 목표이다.

GAN의 학습 과정은 아래와 같이 비유할 수 있다.

1. 위조 지폐범은 처음에 어설픈 가짜 지폐를 만들고, 경찰은 이를 쉽게 가짜 지폐라고 판별한다.
2. 위조범은 '어떻게 만들었더니 가짜라고 들켰는지' 피드백을 받아, 더 정교한 가짜 지폐를 만든다.
3. 경찰도 '어떤 점이 진짜와 달랐는지'를 학습하며, 판별 능력을 키운다.

위 과정을 반복하게 되면, 위조범은 경찰조차 속을 만큼 진짜 같은 가짜 지폐를 만들게 되고, 경찰은 아주 미세한 차이도 구별해낼 수 있게 된다.

### GAN의 학습 과정

![fig2](AI/Generative/GAN-2.png){: style="display:block; margin:0 auto; width:70%;"}
_[[출처]](https://wikidocs.net/146217)_

생성자 (Generator)를 $G$, 식별자 (Discriminator) $D$로 표기한다.

실제 이미지의 라벨은 $1$, 가짜 이미지의 라벨은 $0$으로 설정된다.

1. $G$에게 랜덤한 노이즈 벡터 $z$를 입력한다.
2. $G$는 입력된 노이즈를 이용해 가짜 이미지를 생성한다.
3. 실제 이미지와 $G$가 생성한 가짜 이미지를 섞어서 $D$에게 입력한다.
4. $D$는 실제 이미지를 $1$로, 가짜 이미지를 $0$으로 구별하도록 학습한다. (이때 $G$는 고정)
5. $G$는 $D$를 속이기 위해 더욱 정교하게 이미지를 생성하도록 학습한다. (이때 $D$는 고정)

![fig3](AI/Generative/GAN-3.png){: style="display:block; margin:0 auto; width:70%;"}
_출처: [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)_

위 그림에서 검은색 점선은 실제 데이터의 분포, 초록색 실선은 Generator가 생성한 가짜 데이터의 분포, 파란색 점선은 Discriminator의 예측 결과를 의미한다.

**(a)** 처음 Discriminator는 학습이 되어있지 않기 때문에 무작위로 예측한다.

**(b)** 학습이 진행될수록, Discriminator는 점점 진짜와 가짜를 잘 구별하게 된다.

**(c)** Generator도 학습이 진행될수록 실제 데이터 분포에 가까워진다.

**(d)** 학습이 완료되면 실제 분포와 가짜 분포는 동일해지고, Discriminator의 예측 결과는 $0.5$이다.

#### 목적 함수

GAN의 목적 함수는 아래와 같다.

$$
\min_\theta\max_\phi
\left[
\mathbb{E}_{x\sim p_{data}}\log D_\phi(x)+
\mathbb{E}_{z\sim p(z)}\log(1-D_\phi(G_\theta(z)))
\right]
$$

여기서 $\theta$는 Generator의 파라미터, $\phi$는 Discriminator의 파라미터를 의미한다.

Discriminator는 아래의 식을 최대화하도록 학습된다.

$$
\mathbb{E}_{x\sim p_{data}}\log D_\phi(x)+
\mathbb{E}_{z\sim p(z)}\log(1-D_\phi(G_\theta(z)))
$$

실제 데이터 $x$를 $1$로, 가짜 데이터 $G(z)$를 $0$으로 잘 판별하면 위의 식은 최대가 된다.

Generator는 아래의 식을 최소화하도록 학습된다.

$$
\mathbb{E}_{z\sim p(z)}\log(1-D_\phi(G_\theta(z)))
$$

Discriminator가 자신이 만든 데이터 $G(z)$를 진짜로 판별할 때 최소가 된다. 즉, $D(G(z))=1$일 때 위 식은 최소가 된다.

#### Zero-Sum Game

GAN의 학습 과정은 두 개의 신경망이 서로 경쟁하는 제로섬 게임 (Zero-Sum Game)으로 볼 수 있다. 이 경쟁이 반복되면, $G$는 점점 더 정교한 가짜를 만들고, $D$는 점점 더 잘 구분하게 된다. 

이 게임이 이상적인 균형점인 내쉬 균형 (Nash Equilibrium)에 도달하면, $G$가 만든 가짜는 진짜와 통계적으로 구별이 불가능할 정도로 완벽해진다.

이 상태가 바로 $D$가 $0.5$를 출력하는 순간이다. (위 그림에서 **(d)** 그림) 

이는 $D$가 멍청해져서 구별을 못 하는 것이 아니라, $G$가 너무 완벽한 가짜를 만들어서 $D$가 자신이 가진 모든 지식을 총동원해도 주어진 데이터가 '진짜일 확률 50%, 가짜일 확률 50%'라고 판단할 수밖에 없는 상태를 의미한다.

#### Non-Saturating Loss

위에서 언급한 손실 함수에는 문제가 있다.

$$
\log(1-D_\phi(G_\theta(z)))
$$

학습 초기에 $G$의 성능은 매우 낮기 때문에, $D$가 진짜와 가짜를 쉽게 구분한다. 즉, $D(G_\theta(z))$값이 $0$에 매우 가깝게 된다.

문제는 $\log(1-x)$ 함수의 형태에 있다.

실제 손실 함수에는 sigmoid 함수를 지난 값이 입력되게 된다.

$$
L(a)=\log(1-\sigma(a))~,~\frac{\partial L}{\partial a}=\frac{\sigma'(a)}{1-\sigma(a)}=\frac{\sigma(a)(1-\sigma(a))}{1-\sigma(a)}
$$

만약 $D$가 가짜를 쉽게 판별한다면 $\sigma(a)$값이 $0$에 가까워지므로, 그라디언트가 $0$에 수렴하게 되어 기울기 소실 문제가 발생한다.

$$
\sigma(a)\to0\implies\frac{\partial L}{\partial a}\approx 0
$$

이를 해결하기 위해 실제 구현에서는 아래의 손실 함수를 최대화하도록 수정한다.

$$
\log D(G(z))
$$

위의 식은 $\sigma(a)\to0$이 되어도, 더이상 기울기 소실 문제가 발생하지 않는다.

$$
\frac{\partial L}{\partial a}=\frac{\sigma'(a)}{\sigma(a)}=\frac{\sigma(a)(1-\sigma(a))}{\sigma(a)}=1-\sigma(a)
$$

## Mode Collapse

Mode Collapse는 $G$가 다양한 이미지를 생성하지 못하고, $D$를 속이기 쉬운 몇 가지의 비슷한 이미지만 계속해서 생성하는 현상이다.

아래 그림은 MNIST 데이터셋에서 Mode Collapse가 발생한 결과를 보여준다.

![fig5](AI/Generative/GAN-5.png){: style="display:block; margin:0 auto; width:70%;"}
_[[출처]](https://dev-hani.tistory.com/15)_