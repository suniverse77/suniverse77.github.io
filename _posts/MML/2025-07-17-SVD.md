---
title: "[선형대수] 특이값 분해 (SVD)"
date: 2025-07-17 00:00:00 +/-TTTT
categories: [인공지능 수학, 선형대수]
tags: [선형대수]
math: true
toc: true
author: sunho
---

## 특이값 분해 (SVD - Singularvalue Decomposition)

SVD는 EVD와 달리 어떠한 행렬이든 3개의 행렬로 분해가 가능하다.

![fig1](mlm/17-1.png){: style="display:block; margin:0 auto; width:50%;"}
_[[출처]](https://my-mindpalace.tistory.com/10)_

$$
A=U\Sigma V^\top
$$

- $U\in\mathbb{R}^{m\times m}$: 각 열이 $A$의 특이 벡터들로 구성된 직교 행렬
- $\Sigma\in\mathbb{R}^{m\times n}$: 주대각선에 $A$의 특이값들이 크기순으로 나열된 대각 행렬
- $V\in\mathbb{R}^{n\times n}$: 각 열이 $A$의 특이 벡터들로 구성된 직교 행렬

### SVD의 기하학적 의미

![fig2](mlm/17-2.png){: style="display:block; margin:0 auto; width:70%;"}
_[[출처]](https://my-mindpalace.tistory.com/10)_

- $V^\top$는 입력 공간의 기저를 회전시킨다.
- $\Sigma$는 회전된 기저를 특이값의 크기만큼 늘리거나 줄인다. $\Sigma$는 비정사각행렬 $\mathbb{R}^{m\times n}$이기 때문에 출력 공간의
 차원을 늘리거나 줄일 수도 있다.
- $U$는 출력 공간의 기저를 다시 회전시킨다.

<details>
<summary><font color='#0000FF'>Why?</font></summary>
<div markdown="1">

기저 벡터가 $\mathbf{b}_1,\mathbf{b}_2$인 입력 공간에 대해 생각해보자.

**$V^\top$의 역할**

$V^\top$을 곱한다는 것은 기존 기저 벡터를 새로운 축으로 정사영시키는 것으로 볼 수 있다.

$$
V^\top=\begin{bmatrix}\vphantom{\Big(}-\mathbf{v}_1^\top-\vphantom{\Big(}\\\vdots\\-\mathbf{v}_n^\top-\end{bmatrix}
~\to~
V^\top B=\begin{bmatrix}\vphantom{\Big(}\mathbf{v}_1^\top\mathbf{b}_1&\mathbf{v}_1^\top\mathbf{b}_2\\\vdots&\vdots\\\mathbf{v}_n^\top\mathbf{b}_1&\mathbf{v}_n^\top\mathbf{b}_2\end{bmatrix}
=\begin{bmatrix}|&|\\\mathbf{y}_1&\mathbf{y}_2\\|&|\end{bmatrix}
$$

이때 $V$가 직교 행렬이기 때문에 $V^\top$도 직교 행렬이며, 직교 행렬은 두 벡터 사이의 각도와 각각의 크기를 보존하기 때문에 벡터를 회전하는 것으로 볼 수 있다.

**$\Sigma$의 역할**

$\Sigma$를 곱한다는 것은 회전된 벡터 $\mathbf{y}$를 각 축의 방향으로 늘리거나 줄이는 스케일링하는 것으로 볼 수 있다.

$$
\Sigma\mathbf{y}=
\begin{bmatrix}\sigma_1&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&\sigma_n\\0&0&0\end{bmatrix}\begin{bmatrix}y_1\\\vdots\\y_n\end{bmatrix}
=\begin{bmatrix}\sigma_1y_1\\\vdots\\\sigma_ny_n\end{bmatrix}
=\begin{bmatrix}z_1\\\vdots\\z_n\end{bmatrix}
$$

새로운 축 $\mathbf{v}_1$ 방향의 성분 $y_1$은 $\sigma_1$만큼 늘리고, $\mathbf{v}_2$ 방향의 성분 $y_2$는 $\sigma_2$만큼 늘린다.

**$U$의 역할**

$U$를 곱한다는 것은 스케일링된 기저 벡터를 회전시키는 것으로 볼 수 있다.

$$
U=\begin{bmatrix}|&&|\\\mathbf{u}_1&\cdots&\mathbf{u}_n\\|&&|\end{bmatrix}
~\to~
U\mathbf{z}=z_1\mathbf{u}_1+z_2\mathbf{u}_2+\cdots
$$

$\mathbf{z}$를 표준 기저 $\mathbf{e}$에 대해 표현하면 아래와 같다.

$$
\mathbf{z}=z_1\mathbf{e}_1+z_2\mathbf{e}_2+\cdots
$$

$U$는 직교 행렬이기 때문에 $\mathbf{u}_i$들은 서로 직교하며, 따라서 $U$의 열벡터들은 $E$의 열벡터들을 회전시킨 것으로 볼 수 있다.

즉, $U$는 입력 $\mathbf{z}$를 회전시킨다.

---

</div>
</details>

## EVD와 SVD의 관계

SVD는 EVD를 일반화한 개념이며, $A^\top A$와 $AA^\top$ 행렬의 EVD를 통해 SVD를 이해할 수 있다.

$A^\top A$와 $AA^\top$는 대칭 행렬이기 때문에 항상 EVD가 가능하다.

**$A^\top A$와 $V$의 관계**

$A^\top A$의 고유벡터들은 SVD의 오른쪽 특이 벡터 행렬 $V$를 구성한다.

$A^\top A$의 고유값 $\lambda_i$는 행렬 $A$의 특이값 $\sigma_i$
의 제곱과 같다.

$$
\vphantom{\Big(}\lambda_i=\sigma_i^2
$$

<details>
<summary><font color='#0000FF'>증명</font></summary>
<div markdown="1">

$A^\top A=(U\Sigma V^\top)^\top(U\Sigma V^\top)=V\Sigma^\top (U^\top U)\Sigma V^\top=V(\Sigma^\top\Sigma) V^\top$

$$
A^\top A=V(\Sigma\Sigma^\top) V^\top=V\begin{bmatrix}\sigma_1^2&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&\sigma_n^2\end{bmatrix}V^\top
$$

$A^\top A$는 대칭 행렬이므로, $A^\top A=V\Lambda V^\top$ 형태로 EVD가 가능하다.

즉, 위의 식에서 $\Sigma\Sigma^\top$ 부분이 EVD의 $\Lambda$에 해당하기 때문에 $\sigma_i^2=\lambda_i$이 성립한다.

---

</div>
</details>
<br>

**$AA^\top$와 $U$의 관계**

$AA^\top$의 고유벡터들은 SVD의 왼쪽 특이 벡터 행렬 $U$를 구성한다.

$AA^\top$의 고유값 $\lambda_i$는 행렬 $A$의 특이값 $\sigma_i$
의 제곱과 같다.

$$
\lambda_i=\sigma_i^2
$$

<details>
<summary><font color='#0000FF'>증명</font></summary>
<div markdown="1">

$AA^\top=(U\Sigma V^\top)(U\Sigma V^\top)^\top=U\Sigma (V^\top V)\Sigma^\top U^\top=U(\Sigma\Sigma^\top)^\top$

$$
AA^\top=U(\Sigma\Sigma^\top) U^\top=U\begin{bmatrix}\sigma_1^2&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&\sigma_m^2\end{bmatrix}U^\top
$$

$AA^\top$는 대칭 행렬이므로, $AA^\top=V\Lambda V^\top$ 형태로 EVD가 가능하다.

즉, 위의 식에서 $\Sigma\Sigma^\top$ 부분이 EVD의 $\Lambda$에 해당하기 때문에 $\sigma_i^2=\lambda_i$이 성립한다.

---

</div>
</details>
<br>

SVD를 계산할 때 두 행렬 $A^\top A$와 $AA^\top$에 대해 두 번의 EVD를 계산할 필요 없이, 하나에 대해 EVD를 계산한 뒤 아래의 공식을 사용해서 나머지 행렬을 구할 수 있다.

$$
\mathbf{u}_i=\frac{1}{\sigma_i}A\mathbf{v}_i
~~,~~\mathbf{v}_i^\top=\frac{1}{\sigma_i}\mathbf{u}_i^\top A
$$

- $\mathbf{v}_i$: $V$의 $i$번째 열벡터
- $\mathbf{u}_i$: $U$의 $i$번째 열벡터

<details>
<summary><font color='#0000FF'>공식 유도</font></summary>
<div markdown="1">

$$
A=U\Sigma V^\top~\to~AV=U\Sigma
$$

$AV$ 행렬의 각 열은 $A\mathbf{v}_i$이다.

$$
AV=\begin{bmatrix}|&&|\\A\mathbf{v}_1&\cdots&A\mathbf{v}_n\\|&&|\end{bmatrix}\in\mathbb{R}^{m\times n}
$$

$U\Sigma$ 행렬의 각 열은 $\sigma_i\mathbf{u}_i$이다.

$$
U\Sigma=\begin{bmatrix}|&&|\\\sigma_1\mathbf{u}_1&\cdots&\sigma_n\mathbf{u}_n\\|&&|\end{bmatrix}\in\mathbb{R}^{m\times n}
$$

따라서, $A\mathbf{v}_i=\mathbf{u}_i\sigma_i$이다.

---

$$
A=U\Sigma V^\top~\to~U^\top A=\Sigma V^\top
$$

$U^\top A$ 행렬의 각 행은 $\mathbf{u}_i^\top A$이다.

$$
U^\top A=\begin{bmatrix}-\mathbf{u}_1^\top A-\\-\mathbf{u}_2^\top A-\\\vdots\end{bmatrix}\in\mathbb{R}^{m\times n}
$$

$\Sigma V^\top$ 행렬의 각 행은 $\sigma_i\mathbf{v}_i^\top$이다.

$$
\Sigma V^\top=\begin{bmatrix}-\sigma_1\mathbf{v}_1^\top-\\-\sigma_2\mathbf{v}_2^\top-\\\vdots\end{bmatrix}\in\mathbb{R}^{m\times n}
$$

따라서, $A\mathbf{v}_i=\mathbf{u}_i\sigma_i$이다.

---

</div>
</details>
<br>

간단히 말해 행렬 $A$가 있을 때 $A^\top A$와 $AA^\top$에 대해 EVD를 수행하면, 그 고유벡터와 고유값을 이용해 $A$의 SVD를 구성할 수 있다는 것이다.

<details>
<summary><font color='#FF0000'>Example 1</font></summary>
<div markdown="1">

$$
A=\begin{bmatrix}1&5&0\\5&1&0\end{bmatrix}
$$

---

**1. $\Sigma$ 구하기**

1. $AA^\top$에 대해 EVD 수행한다. ($A^\top A$도 가능)

    $$
    AA^\top=\begin{bmatrix}26&10\\10&26\end{bmatrix}
    $$

    $$
    \lambda_1=36~,~\lambda_2=16
    $$   

2. 고유값을 이용해 특이값을 계산한다.

    $$
    \sigma_1=\sqrt{\lambda_1}=6~,~\sigma_2=\sqrt{\lambda_2}=4
    $$

3. 크기가 큰 순서대로 정렬하여 주대각선에 배치한다. ($\Sigma$의 크기는 $A$와 동일)

    $$
    \Sigma=\begin{bmatrix}6&0&0\\0&4&0\end{bmatrix}
    $$

**2. $V$ 구하기**

1. $A^\top A$ 구성

    $$
    A^\top A=\begin{bmatrix}26&10&0\\10&26&0\\0&0&0\end{bmatrix}
    $$

2. EVD를 수행하여 고유값과 고유벡터를 얻는다.

    $$
    \lambda_1=36~,~\lambda_2=16~,~\lambda_3=0
    $$
    
    $$
    \mathbf{x}_1=\begin{bmatrix}1\\1\\0\end{bmatrix}~,~
    \mathbf{x}_2=\begin{bmatrix}1\\-1\\0\end{bmatrix}~,~
    \mathbf{x}_3=\begin{bmatrix}0\\0\\1\end{bmatrix}
    $$

3. 고유벡터를 정규화하여 행렬 $V$를 구성한다.

    $$
    \mathbf{v}_1=\begin{bmatrix}\frac{1}{\sqrt2}\\\frac{1}{\sqrt2}\\0\end{bmatrix}~,~
    \mathbf{v}_2=\begin{bmatrix}\frac{1}{\sqrt2}\\-\frac{1}{\sqrt2}\\0\end{bmatrix}~,~
    \mathbf{v}_3=\begin{bmatrix}0\\0\\1\end{bmatrix}
    $$

    $$
    V=\begin{bmatrix}\frac{1}{\sqrt2}&\frac{1}{\sqrt2}&0\\\frac{1}{\sqrt2}&-\frac{1}{\sqrt2}&0\\0&0&1\end{bmatrix}
    $$

**3. $U$ 구하기**

1. $\mathbf{u}_i=\frac{1}{\sigma_i}A\mathbf{v}_i$ 공식을 이용해 $\mathbf{u}_i$를 계산한다.

    $$
    \mathbf{u}_1=\frac{1}{6}\begin{bmatrix}1&5&0\\5&1&0\end{bmatrix}\begin{bmatrix}\frac{1}{\sqrt2}\\\frac{1}{\sqrt2}\\0\end{bmatrix}
    =\begin{bmatrix}\frac{1}{\sqrt2}\\\frac{1}{\sqrt2}\end{bmatrix}
    $$

    $$
    \mathbf{u}_2=\frac{1}{4}\begin{bmatrix}1&5&0\\5&1&0\end{bmatrix}\begin{bmatrix}\frac{1}{\sqrt2}\\-\frac{1}{\sqrt2}\\0\end{bmatrix}
    =\begin{bmatrix}-\frac{1}{\sqrt2}\\\frac{1}{\sqrt2}\end{bmatrix}
    $$

2. $U$를 구성한다.

    $$
    U=\begin{bmatrix}\frac{1}{\sqrt2}&-\frac{1}{\sqrt2}\\\frac{1}{\sqrt2}&\frac{1}{\sqrt2}\end{bmatrix}
    $$

**4. 최종적인 SVD 결과**

$$
\begin{bmatrix}1&5&0\\5&1&0\end{bmatrix}=
\begin{bmatrix}\frac{1}{\sqrt2}&-\frac{1}{\sqrt2}\\\frac{1}{\sqrt2}&\frac{1}{\sqrt2}\end{bmatrix}
\begin{bmatrix}6&0&0\\0&4&0\end{bmatrix}
\begin{bmatrix}\frac{1}{\sqrt2}&\frac{1}{\sqrt2}&0\\\frac{1}{\sqrt2}&-\frac{1}{\sqrt2}&0\\0&0&1\end{bmatrix}
$$

---

</div>
</details>
<br>

## EVD와 SVD의 수치적 안정성 (Numerical Stability)

우리가 실제 다루는 데이터는 대부분 직사각행렬이다.

임의의 행렬 $A\in\mathbb{R}^{m\times n}$에 대해 PCA를 수행할 때, 일반적으로 공분산 행렬 $A^\top A$에 대한 EVD를 사용한다.

수학적으로 $A$에 대한 SVD와 동치이지만, 수치적으로는 SVD가 훨씬 더 안정적이다.

**EVD의 불안정성**

EVD에서 $A^\top A$를 형성하는 순간, 조건수가 제곱으로 증폭된다.

$$
\vphantom{\Big(} \kappa_2(A^\top A) = \kappa_2(A)^2
$$

만약 원본 행렬 $A$의 조건수가 크다면, $A^\top A$는 훨씬 더 불안정해진다는 뜻이다.

결국 불안정한 행렬 $A^\top A$에 대해 EVD를 수행하면, round-off 오차가 증폭되어 고유값과 고유벡터가 부정확하게 계산될 위험이 크다.

> Round-off 오차는 실수를 유한한 비트 수로 근사하여 표현할 때 생기는 오차이다.
> 
> 이는 컴퓨터의 부동소수점 (floating-point) 연산은 유한한 자릿수만 저장할 수 있고, 계산 결과가 그 범위를 넘어가면 가장 가까운 수로 반올림을 하기 때문에 발생한다.
>
> 예를 들어 아래 연산에서 실제 결과는 $0.3$이지만, 부동소수점 표현에서는 이를 정확히 표현할 수 없어 근사값으로 저장된다.
>
> $$0.1+0.2=0.30000000000000004$$
>
> 따라서 반복적인 연산을 거치면 round-off 오차가 매우 커질 수 있다.

**SVD의 안정성**

반면, SVD는 원본 행렬 $A$에 대해 바로 SVD를 수행하기 때문에, 조건수 제곱 효과가 발생하지 않아 수치적으로 더 안정적이다.

$$
\vphantom{\Big(}A=U\Sigma V^\top
$$

위에서는 $AA^\top$와 $A^\top A$를 이용해 SVD를 수행했지만, 실제로는 이러한 행렬을 직접 구성하지 않고 SVD를 수행하는 알고리즘이 존재한다.

### 조건수 (Condition Number)

입력에 생긴 작은 오차가 출력에 얼마나 크게 증폭되는지를 수치적으로 나타내는 민감도 지표이다.

값이 클수록 불안정 (ill-conditioned)하고, 값이 작을수록 안정 (well-conditioned)하다고 한다.

$$
\kappa_2(A)=\lVert A\rVert_2\lVert A^{-1}\rVert_2
=\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}
$$

행렬의 조건수가 크다는 것은 해당 행렬의 변환이 한 쪽의 방향은 크게 늘리고 다른 쪽의 방향은 크게 눌렀다는 뜻이다. 이런 경우, 작은 축 방향 (분산이 약한 방향)의 신호들은 값이 작아져 노이즈와 구분이 어려워지고 수치적 안정성이 떨어지게 된다.

![fig2](mlm/17-3.png){: style="display:block; margin:0 auto; width:70%;"}
_[[출처]](https://math.stackexchange.com/questions/290267/need-help-understanding-matrix-norm-notation)_
