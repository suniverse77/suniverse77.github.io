---
title: "[확률] 최대 사후 추정 (MAP)"
date: 2025-08-11 00:00:00 +/-TTTT
categories: [인공지능 수학, 확률]
tags: [확률, 머신러닝]
math: true
toc: true
author: sunho
---

## 최대 사후 추정 (MAP - Maximum a Posteriori Estimation)

MLE에서 사전 확률 $p(\theta)$를 고려한 것으로, 베이즈 정리를 사용한다.

$$
\hat{\theta}_{MAP}
=\underset{\theta}{\text{argmax}}~p(\theta\mid X)
=\underset{\theta}{\text{argmax}}~\frac{p(X\mid \theta)p(\theta)}{p(X)}
$$

이때 $p(X)$는 $\theta$와 무관한 값이므로, 최적화 과정에서는 아래와 같이 무시할 수 있다.

$$
\hat{\theta}_{MAP}=\underset{\theta}{\text{argmax}}~p(X\mid \theta)p(\theta)
$$

$X$의 각 데이터 샘플 $\mathbf{x}^{(i)}$가 i.i.d를 따른다고 가정하고, 로그 함수를 취하면 아래와 같이 나타낼 수 있다.

$$
\hat{\theta}_{MAP}
=\underset{\theta}{\text{argmax}}~\bigg(\sum_{i=1}^n\log p(\mathbf{x}^{(i)}\mid\theta)+{{\log p(\theta)}}\bigg)
$$

### 사전 확률의 역할

사전 확률 $\log p(\theta)$는 패널티 항 역할을 하며, 모델의 과적합을 방지한다.

$\theta$의 크기가 커질수록, 모델은 훈련 데이터에 민감하게 반응한다. 즉, $\log p(\theta)$는 모델이 우도를 최대화하는 것에만 신경써서 분포가 너무 복잡해지는 것을 방지한다.

예를 들어 가우시안 분포에서는 $\lVert\theta\rVert$가 커질수록 $p(\theta)$는 0에 수렴하며, 이로 인해 $\log p(\theta)$가 음의 무한대로 발산하게 된다.
