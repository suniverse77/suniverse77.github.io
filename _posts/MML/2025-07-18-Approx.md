---
title: "[선형대수] 행렬 근사"
date: 2025-07-18 00:00:00 +/-TTTT
categories: [인공지능 수학, 선형대수]
tags: [선형대수]
math: true
toc: true
author: sunho
img_path: /assets/images/math/
description: ✏️ 벡터의 기본 개념
---

## Rank-k Approximation

SVD를 통해 행렬 $A$를 $U\Sigma V^\top$으로 분해할 수 있고, 이는 아래와 같이 표현할 수 있다.

$$
A=U\Sigma V^\top=\sum_{i=1}^r\sigma_i\mathbf{u}_i\mathbf{v}_i^\top=\sum_{i=1}^r\sigma_iA_i
$$

- $r$은 0이 아닌 특이값의 개수로, $r=\text{rank}(A)$이다.
- $A_i=\mathbf{u}_i\mathbf{v}_i^\top$는 rank-1 행렬로, $A$는 rank-1 행렬들의 선형 결합으로 이루어져 있다.
- $\sigma_i$는 rank-1 성분 중 $i$번째 방향의 기여를 나타낸다.

<details>
<summary><font color='#0000FF'>왜  rank-1행렬일까?</font></summary>
<div markdown="1">

$A_i=\mathbf{u}_i\mathbf{v}_i^\top$는 단순히 두 벡터의 외적으로 표현되는 행렬이다.

두 벡터의 외적은 두 벡터의 수직인 벡터이고, 하나의 방향성만 가지기 때문에 $\text{rank}(A)=1$이다.

아래와 같이 직접 계산해보면, $A$의 각 행은 단순히 첫 번째 행의 상수배이기 때문에 $\text{rank}(A)=1$인 것을 알 수 있다.
 
$$
A=\mathbf{u}\mathbf{v}^\top
~\rightarrow~
\begin{bmatrix}1\\3\\5\end{bmatrix}\begin{bmatrix}2&4&6\end{bmatrix}
=\begin{bmatrix}2&4&6\\6&12&18\\10&20&30\end{bmatrix}
$$

Rank는 더한다고 유지되지 않으며, 일반적으로 증가한다.

$$
A=\begin{bmatrix}1&0\\0&0\end{bmatrix}~,~
B=\begin{bmatrix}0&0\\0&1\end{bmatrix}
~\to~
A+B=\begin{bmatrix}1&0\\0&1\end{bmatrix}
$$

</div>
</details>
<br>

특이값은 크기 순으로 정렬되어 있으며, 이 중 상위 $k$개의 항만 취했을 때를 rank-k 근사라고 한다.

$$
\hat{A}(k)=\sum_{i=1}^k\sigma_i\mathbf{u}_i\mathbf{v}_i^\top=\sum_{i=1}^k\sigma_iA_i
$$

$\hat{A}(k)$는 $A$의 SVD에서 가장 큰 k개의 특이값과 대응되는 rank-1 성분만 더한 것으로, 전체 정보를 k개의 중요한 방향만 남겨서 압축한 근사 행렬을 의미한다.

![fig1](mlm/18-1.png){: style="display:block; margin:0 auto; width:70%;"}
_출처: Mathematics for Machine Learning p._

![fig2](mlm/18-1.png){: style="display:block; margin:0 auto; width:70%;"}

위 사진을 보면 큰 특이값부터 $k$개만 남겨도 데이터의 주요 구조를 유지할 수 있다는 것을 확인할 수 있다.

### Eckart-Young Theorem

모든 rank-k 행렬 $B$ 중에서 원본 $A$와 가장 가까운 최고의 근사 행렬은 SVD 기반의 rank-k 행렬 $\hat{A}(k)$라는 것을 수학적으로 증명해준다.

$$
\hat{A}(k)=\underset{\text{rank}(B)=k}{\text{argmin}}~\lVert A-B\rVert_2
$$

이때의 오차는 $k+1$번째 특이값과 같다.

$$
\lVert A-\hat{A}(k)\rVert_2=\sigma_{k+1}
$$

## Spectral Norm

Matrix norm에는 여러 종류가 있으며, 그 중 spectral norm은 아래와 같이 정의된다.

$$
\displaystyle\lVert A\rVert_2:=\underset{\mathbf{x}}{\max}\frac{\lVert A\mathbf x\rVert_2}{\lVert \mathbf x\rVert_2}=\underset{\mathbf{x}}{\max}\lVert A\mathbf x\rVert_2
$$

즉, spectral norm은 변환 전후의 크기 변화의 최대값으로 정의되며, 이는 $A$의 가장 큰 특이값과 동일하다.

<details>
<summary><font color='#0000FF'>증명</font></summary>
<div markdown="1">

계산의 편의성을 위해 $\lVert A\mathbf x\rVert_2$의 제곱에 대해 계산하고, $\rVert\mathbf{x}\lVert_2=1$이라고 가정한다.

1. Norm의 제곱

    $$
    \lVert A\mathbf x\rVert_2^2=(A\mathbf x)^\top A\mathbf x=\mathbf x^\top A^\top A\mathbf x
    $$

2. $A$에 SVD 적용

    $$
    A^\top A=(U\Sigma V^\top)^\top(U\Sigma V^\top)=V\Sigma^\top U^\top U\Sigma V^\top=V\Sigma^\top\Sigma V^\top
    $$

3. $\lVert A\mathbf x\rVert_2$에 대입

    $$
    \lVert A\mathbf x\rVert_2^2=\mathbf x^\top V(\Sigma^\top\Sigma)V^\top\mathbf x
    $$

4. $\mathbf{y}=V^\top\mathbf{x}$로 치환

    $$
    \lVert A\mathbf x\rVert_2^2=\mathbf{y}^\top(\Sigma^\top\Sigma)\mathbf{y}
    $$

    $V$는 직교 행렬이기 때문에 $\mathbf{x}$의 크기를 바꾸지 않는다.

    따라서 $\rVert\mathbf{y}\lVert_2^2=\rVert V^\top\mathbf{x}\lVert_2^2=\rVert\mathbf{x}\lVert_2^2=1$이다.
    
5. 최대값 찾기

    $$
    \lVert A\mathbf x\rVert_2^2=\mathbf{y}^\top(\Sigma^\top\Sigma)\mathbf{y}=
    \sigma_1^2\mathbf y_1^2+\sigma_2^2\mathbf y_2^2+\cdots+\sigma_r^2\mathbf y_r^2
    $$

    $\rVert\mathbf{y}\lVert_2^2=y_1^2+y_2^2+\cdots=1$의 조건이 있으며, 특이값은 크기순으로 정렬되어있기 때문에 위의 식이 제일 커지려면 $\sigma_1$에 가중치를 몰아주면 된다.

    즉, $\lVert A\mathbf x\rVert_2^2$는 $y_1=1$일 때 최대값 $\sigma_1^2$을 가진다.

6. 결론

    양쪽에 제곱근을 취하면 아래와 같은 결과를 얻을 수 있다.

    $$
    \underset{\lVert\mathbf{x}\rVert_2^2=1}{\max}\lVert A\mathbf x\rVert_2=\sigma_1
    $$

</div>
</details>
<br>
