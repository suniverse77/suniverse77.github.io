---
title: "[최적화] KKT"
date: 2025-08-18 00:00:00 +/-TTTT
categories: [인공지능 수학, 최적화]
tags: [최적화]
math: true
toc: true
author: sunho
---

## 부등식 제약 최적화 (Inequality Constrained Optimization)

부등식 제약 조건을 만족하면서, 목적 함수 $f(x)$의 값을 최소화하는 변수 $x$를 찾는 최적화 문제이다.

## KKT conditions

$$
\underset{\mathbf x}\min~f(\mathbf x)
\\
\text{subject to}~~
g_i(\mathbf x)\leq0~,~i=0,1,\mathbf,m
\\ ~~~~~~~~~~~~~~~~~
h_j(\mathbf x)=0~,~j=0,1,\dots,p
$$

최적화 문제가 위와 같이 주어져있을 때, KKT 조건은 아래와 같다.

**1. Stationarity**
        
$$
\nabla_\mathbf xf(\mathbf x^*)+
\sum_{i=1}^m\lambda_i^*\nabla_\mathbf xg_i(\mathbf x^*)+
\sum_{j=1}^p\mu_j^*\nabla_\mathbf xh_j(\mathbf x^*)
=0
$$

라그랑지안의 gradient가 0이 되는지점이 optimal이다.
        
**2. Primal constraints**
        
$$
$g_i(\mathbf x^*)\leq0~,~h_j(\mathbf x^*)=0
$$

Primal 문제에서의 제약 조건을 의미한다.
        
**3. Dual constraints**
        
$$
\lambda_i^*\geq0
$$

Dual 문제에서의 제약 조건을 의미한다.
        
**4. Complementary slackness**

$$
\lambda_i^*g_i(\mathbf x^*)=0
$$

<details>
<summary><font color='purple'>Complementary slackness</font></summary>
<div markdown="1">

**Case1)** $g_i(\mathbf{x}^\*)<0~\rightarrow~\lambda_i^\*=0$

<figure style="text-align: center;">
  <img src='{{ "/assets/images/인공지능수학/6-2. Figure2.png" | relative_url }}' style="display: block; margin: 0 auto; width: 30%;">
  <figcaption>출처: https://www.cnblogs.com/pingzeng/p/7019221.html</figcaption>
</figure>

- Optimal $\mathbf x^\*$가 제약 조건의 영역 내에 있는 경우를 의미한다. → $g_i(\mathbf x^\*)<0$
- 원래 목적 함수의 최적해가 이미 제약 조건 영역 내에 있었기 때문에, 해당 제약 조건을 없애더라도 기존의 최적해가 변하지 않는다.

---

**Case2)** $\lambda_i^\*>0~\rightarrow~g_i(\mathbf x^\*)=0$

<figure style="text-align: center;">
  <img src='{{ "/assets/images/인공지능수학/6-2. Figure3.png" | relative_url }}' style="display: block; margin: 0 auto; width: 30%;">
  <figcaption>출처: https://www.cnblogs.com/pingzeng/p/7019221.html</figcaption>
</figure>

- Optimal $\mathbf x^\*$가 제약 조건의 경계에 있는 경우를 의미한다. → $g_i(\mathbf x^\*)=0$
- 원래 목적 함수의 최적해가 제약 조건 영역의 바깥에 있었기 때문에, 제약 조건이 존재할 때의 최적해는 기존의 최적해와 다르다.
- 이 경우에 최적해는 일반적으로 제약 조건의 경계에 위치한다.

</div>
</details>

### lagrange multiplier의 의미

부등식에 대한 lagrange multiplier $\lambda$를 shadow price라고 부르며, <mark style='background-color: fff5b1'>목적 함수의 최적값이 제약 조건에 대해 얼마나 민감한지</mark>를 나타낸다.

$\lambda$가 클수록 해당 제약 조건이 목적 함수에 더 민감하게 영향을 준다는 뜻이며, $\lambda$를 조금만 완화해도 목적 함수가 크게 개선된다는 뜻으로 해석할 수도 있다.

<details>
<summary><font color='red'>Example</font></summary>
<div markdown="1">

공장에서의 물건 생산에서 아래의 조건이 있을 때, 원자재 1kg을 더 사용할 수 있다면 이윤이 얼마나 증가하는가?

- 제약 조건: 원자재는 최대 100kg 사용 가능
- 현재 최적 상태: 이윤 500달러, 원자재는 딱 100kg 사용 중

---

1kg 더 주었더니 이윤이 5달러 증가했다면, 이 5달러가 바로 shadow price이다.

</div>
</details>
