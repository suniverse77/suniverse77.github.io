---
title: "[선형대수] 주성분 분석 (PCA)"
date: 2025-07-30 00:00:00 +/-TTTT
categories: [인공지능 수학, 선형대수]
tags: [선형대수]
math: true
toc: true
author: sunho
---

## 주성분 분석 (PCA - Principal Component Analysis)

PCA는 고차원 데이터에 존재하는 변수들을 정보 손실을 최소화하면서 저차원으로 압축하는 차원 축소 기법이다. 

PCA는 데이터를 설명하는 다수의 특성이 있을 때, 그보다 적은 새로운 특성으로 데이터를 설명할 수 있다는 것이 기본 원리이다.

예를 들어, 아래와 같이 데이터가 '키'와 '몸무게' 2개의 축으로 표현되어 있다고 해보자.

![fig1](mlm/30-1.png){: style="display:block; margin:0 auto; width:60%;"}
_[[출처]](https://www.youtube.com/watch?v=CodLsdbfjvI)_

일반적으로 키가 큰 사람은 몸무게도 많이 나가는 경향이 있으며, 이 두 특성이 완전히 독립적이지 않고 연관되어 있음을 알 수 있다.

이때 데이터를 '키'와 '몸무게' 두 축으로 보기보다, 이 둘의 특성을 결합한 '덩치'라는 하나의 축으로 바라보면 데이터를 더 간결하게 표현할 수 있다.

'덩치가 크면 키가 크고 몸무게도 많이 나간다'와 같이, 이 축 하나만으로도 원래 두 변수의 대부분의 정보를 설명할 수 있다.

![fig2](mlm/30-2.png){: style="display:block; margin:0 auto; width:60%;"}
_[[출처]](https://www.youtube.com/watch?v=CodLsdbfjvI)_

즉, 원래 데이터를 가장 잘 설명하는 하나의 축이 있으며, 그 축을 주성분 (Principal Component)이라고 한다.

제2 주성분은 두 번째로 데이터를 잘 설명하는 축으로, 제1 주성분과 항상 직교한다.

왜냐하면 제1 주성분이 이미 데이터의 한 방향에 있는 정보를 모두 설명했기 때문에,
해당 방향의 정보는 더 이상 새로운 의미를 제공하지 않기 때문이다.

예를 들어, 아래 그림에서 보라색과 노란색의 덩치가 같다고 가정해보자.

보라색은 키가 크고 몸무게가 적게 나가지만, 노란색은 키가 작고 몸무게가 많이 나간다. 하지만 제1 주성분인 '덩치'만으로는 이 둘의 차이를 설명하지 못한다. 따라서 '체질'이라는 제2 주성분이 필요하다.

![fig3](mlm/30-3.png){: style="display:block; margin:0 auto; width:60%;"}
_[[출처]](https://www.youtube.com/watch?v=CodLsdbfjvI)_

데이터가 $N$차원이라면, 서로 직교하는 축은 최대 $N$개까지만 만들 수 있다. 즉, $N$차원 데이터에는 $N$개의 주성분만 존재하며, 그 이상은 서로 독립적이지 않기 때문이다.

예를 들어, 위 예시에서 '덩치'와 '체질' 그 이상의 축을 만들 필요가 없다.

<span style="background-color:#fff5b1">주성분은 데이터의 분산이 가장 큰 축을 의미한다.</span>

$$
\text{주성분}=\text{분산이 가장 큰 축}=\text{데이터를 가장 잘 설명하는 축}
$$

분산은 곧 데이터가 그 방향으로 얼마나 다양하게 퍼져 있는지, 즉 정보량을 나타낸다.

어떤 축을 따라 분산이 크다는 것은 그 축 방향으로 데이터들이 서로 크게 다르며 변동성이 크다는 뜻이다. 따라서 그 축은 데이터의 전체적인 구조와 패턴을 잘 드러낸다.

반대로, 분산이 작은 축에서는 대부분의 데이터가 비슷한 값에 모여 있어 변화가 거의 없다. 이런 축은 데이터 간 차이를 잘 구분하지 못하기 때문에 설명력이 낮은 축이 된다.

![fig4](mlm/30-4.gif){: style="display:block; margin:0 auto; width:70%;"}
_[[출처]](https://hrithickcodes.medium.com/understanding-principle-component-analysis-pca-from-scratch-db7ceda623eb)_

### PCA와 공분산 행렬

PCA는 원본 데이터를 어떤 하나의 축에 정사영시켰을 때 분산이 가장 커지는 축을 찾는 과정이며, 이는  [공분산 행렬](https://suniverse77.github.io/posts/MultiRV/#%EA%B3%B5%EB%B6%84%EC%82%B0-%ED%96%89%EB%A0%AC-covariance-matrix)의 고유벡터를 찾는 과정과 같다.

<span style="background-color:#fff5b1">데이터 공분산 행렬의 가장 큰 고유값에 대응하는 고유벡터가 주성분이다.</span>

<details>
<summary><font color='#0000FF'>증명</font></summary>
<div markdown="1">

$N$개의 샘플과 $D$차원의 특징으로 이루어진 데이터 행렬 $X\in\mathbb{R}^{N\times D}$를 가정하자. 이때 계산 편의를 위해 각 특성의 평균이 0으로 맞춰져 있다고 가정한다.

찾고 싶은 축의 방향 벡터를 $\mathbf{u}$라고 할 때, 데이터 행렬의 이 축으로의 정사영은 아래와 같이 표현할 수 있다.

$$
X\mathbf{u}=\begin{bmatrix}-\mathbf{x}_1^\top\mathbf{u}-\\-\mathbf{x}_2^\top\mathbf{u}-\\\vdots\\-\mathbf{x}_N^\top\mathbf{u}-\end{bmatrix}\in\mathbb{R}^{N\times1}
$$

$\mathbf{x}^\top\mathbf{u}$ 자체는 내적값이지만, 이를 원점에서 떨어진 정도로 볼 수 있다.

정사영된 벡터에 대해 분산을 구하면 아래와 같이 정리할 수 있다.

$$
\mathbb{V}[X\mathbf{u}]=\frac{1}{N}(X\mathbf{u})^\top(X\mathbf{u})=\frac{1}{N}\mathbf{u}^\top X^\top X\mathbf{u}
$$

이때, $\frac{1}{N}X^\top X$가 공분산 행렬 $S$의 정의이기 때문에 아래와 같이 정리할 수 있다.

$$
\mathbb{V}[X\mathbf{u}]=\mathbf{u}^\top S\mathbf{u}
$$

정사영된 데이터의 분산을 최대화하는 축을 찾는 것이 목표이므로, 풀어야 할 문제는 다음과 같이 정리된다.

$$
\max_{\lVert\mathbf{u}\rVert=1}\mathbb{V}[X\mathbf{u}]
=\max_{\lVert\mathbf{u}\rVert=1}\mathbf{u}^\top S\mathbf{u}
$$

[라그랑주 승수법]을 적용하면, 이 문제는 공분산 행렬의 가장 큰 고유값을 찾는 과정으로 변하게 된다.

$$
\mathcal{L}(\mathbf{u},\lambda)=\mathbf{u}^\top S\mathbf{u}-\lambda(\mathbf{u}^\top\mathbf{u}-1)
$$

$$
\frac{\partial\mathcal{L}}{\partial\mathbf{u}}=2S\mathbf{u}-2\lambda\mathbf{u}=0~\to~S\mathbf{u}=\lambda\mathbf{u}
$$

$$
\max_{\lVert\mathbf{u}\rVert=1}\mathbf{u}^\top S\mathbf{u}
=\max_{\lVert\mathbf{u}\rVert=1}\mathbf{u}^\top \lambda\mathbf{u}
=\max\lambda
$$

따라서, 공분산 행렬의 가장 큰 고유값에 대응하는 고유벡터가 주성분이다.

---

</div>
</details>
<br>
