---
title: "[선형대수] 주성분 분석 (PCA)"
date: 2025-07-30 00:00:00 +/-TTTT
categories: [인공지능 수학, 선형대수]
tags: [선형대수]
math: true
toc: true
author: sunho
---

## 주성분 분석 (PCA - Principal Component Analysis)

PCA는 고차원 데이터에 존재하는 변수들을 정보 손실을 최소화하면서 저차원으로 압축하는 차원 축소 기법이다. 

PCA는 데이터를 설명하는 다수의 특성이 있을 때, 그보다 적은 새로운 특성으로 데이터를 설명할 수 있다는 것이 기본 원리이다.

예를 들어, 아래와 같이 데이터가 '키'와 '몸무게' 2개의 축으로 표현되어 있다고 해보자.

![fig1](mlm/30-1.png){: style="display:block; margin:0 auto; width:60%;"}
_[[출처]](https://www.youtube.com/watch?v=CodLsdbfjvI)_

일반적으로 키가 큰 사람은 몸무게도 많이 나가는 경향이 있으며, 이 두 특성이 완전히 독립적이지 않고 연관되어 있음을 알 수 있다.

이때 데이터를 '키'와 '몸무게' 두 축으로 보기보다, 이 둘의 특성을 결합한 '덩치'라는 하나의 축으로 바라보면 데이터를 더 간결하게 표현할 수 있다.

'덩치가 크면 키가 크고 몸무게도 많이 나간다'와 같이, 이 축 하나만으로도 원래 두 변수의 대부분의 정보를 설명할 수 있다.

![fig2](mlm/30-2.png){: style="display:block; margin:0 auto; width:60%;"}
_[[출처]](https://www.youtube.com/watch?v=CodLsdbfjvI)_

즉, 원래 데이터를 가장 잘 설명하는 하나의 축이 있으며, 그 축을 주성분 (Principal Component)이라고 한다.

제2 주성분은 두 번째로 데이터를 잘 설명하는 축으로, 제1 주성분과 항상 직교한다.

왜냐하면 제1 주성분이 이미 데이터의 한 방향에 있는 정보를 모두 설명했기 때문에,
해당 방향의 정보는 더 이상 새로운 의미를 제공하지 않기 때문이다.

예를 들어, 아래 그림에서 보라색과 노란색의 덩치가 같다고 가정해보자.

보라색은 키가 크고 몸무게가 적게 나가지만, 노란색은 키가 작고 몸무게가 많이 나간다. 하지만 제1 주성분인 '덩치'만으로는 이 둘의 차이를 설명하지 못한다. 따라서 '체질'이라는 제2 주성분이 필요하다.

![fig3](mlm/30-3.png){: style="display:block; margin:0 auto; width:86%;"}
_[[출처]](https://www.youtube.com/watch?v=CodLsdbfjvI)_

데이터가 $N$차원이라면, 서로 직교하는 축은 최대 $N$개까지만 만들 수 있다. 즉, $N$차원 데이터에는 $N$개의 주성분만 존재하며, 그 이상은 서로 독립적이지 않기 때문이다.

예를 들어, 위 예시에서 '덩치'와 '체질' 그 이상의 축을 만들 필요가 없다.

### 분산이 가장 큰 축 = 데이터를 가장 잘 설명하는 축

위에서 언급한 주성분은 분산이 가장 큰 축을 의미한다. 즉, 데이터가 가장 많이 퍼져 있는 방향을 의미한다.

왜 분산이 가장 큰 축이 데이터를 가장 잘 설명하는 축일까?

예를 들어, 학교에서 '반'과 '성적' 두 가지 기준으로 학생들을 분석한다고 해보자.

**'반'으로 학생을 구분하는 경우**

각 반에는 다양한 실력이 섞여있기 때문에, 반이 다르다고 해서 성적의 분포가 크게 달라지지 않는다. 

즉, 반에 따른 데이터의 변화 (분산)가 작으며, 이는 이 축이 학생의 전체적인 특징을 잘 설명하지 못한다는 뜻이 된다.

**'성적'으로 학생을 구분하는 경우**

성적이라는 축을 따라 학생들이 넓게 퍼져 있고, 다양하게 분포해 있다.

이 축은 성적이 높을수록 공부를 잘하고, 낮을수록 부족하다는 경향이 잘 드러내며, 데이터를 잘 설명한다는 뜻이 된다.

### 수학적

주성분 벡터는 [공분산 행렬]의 고유벡터와 같다.
