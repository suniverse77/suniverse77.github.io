---
title: "[최적화] 등식 제약 최적화와 라그랑주 승수법 (Lagrange Multiplier Method)"
date: 2025-08-17 00:00:00 +/-TTTT
categories: [인공지능 수학, 최적화]
tags: [최적화]
math: true
toc: true
author: sunho
---

## 등식 제약 최적화 (Equality Constrained Optimization)

등식 제약 조건을 만족하면서, 목적 함수 $f(x)$의 값을 최소화하는 변수 $x$를 찾는 최적화 문제이다.

$$
\begin{aligned}
\mathbf{x}^*=\min_{\mathbf{x}}f(\mathbf{x})~~~~\\
\text{subject to}~g(\mathbf{x})=c
\end{aligned}
$$

$g(\mathbf{x})=c$라는 제약 조건 위에서만 $\mathbf{x}$를 찾아야 한다.

### 제약 조건에서의 최적해

아래와 같은 목적 함수 $f$와 제약 조건 $g$에 대해 생각해보자.

$$
f(x,y)=6-\frac{3}{16}(x^2+y^2)~~,~~g(x,y)=\frac{(x-2)^2}{0.25}+\frac{y^2}{4}-1
$$

$f$는 2차원 평면에서는 원 형태의 등고선 (파란색), 3차원 공간에서는 위로 볼록한 포물곡면 형태로 나타난다.

$g$는 2차원 평면에서는 타원 형태의 등고선 (초록색), 3차원 공간에서는 아래로 볼록한 타원포물면 형태 (빨간색)로 나타난다.

![fig1](mlm/o17-1.png){: style="display:block; margin:0 auto; width:70%;"}
_[[출처]](https://www.geogebra.org/m/hmcfh5cq)_

이때 빨간색 곡선 위에서 목적 함수가 최대가 될 때는 아래와 같다. 즉, 파란색 곡선이 초록색 곡선과 접할 때이다.

![fig2](mlm/o17-2.png){: style="display:block; margin:0 auto; width:70%;"}
_[[출처]](https://www.geogebra.org/m/hmcfh5cq)_

따라서 제약 조건을 만족하는 최적점에서는 목적 함수의 등고선과 제약 조건의 곡선이 서로 접할 때이며, 이 지점에서 두 함수의 그라디언트의 방향은 동일하다.

$$
\nabla f(\mathbf{x}^*)=\lambda\nabla g(\mathbf{x}^*)
$$

![fig3](mlm/o17-3.png){: style="display:block; margin:0 auto; width:50%;"}
_[[출처]](https://www.geogebra.org/m/hmcfh5cq)_

$\lambda$는 라그랑주 승수 (Lagrange Multiplier)로, 비례 상수이다. $\lambda>0$이면 두 그라디언트의 방향이 같다는 뜻이며, $\lambda<0$이면 두 그라디언트의 방향이 반대라는 뜻이다.

## 라그랑주 승수법 (Lagrange Multiplier Method)

비제약 최적화 문제에서는 단순히 $\nabla f(x)=0$을 만족하는 지점을 찾아 최적점을 찾을 수 있지만, 제약 조건이 있을 때는 그럴 수 없다.

이를 해결하기 위해 제약식을 목적 함수에 포함시킨 새로운 함수 $\mathcal{L}$을 정의하는데, 이를 라그랑주 함수 (Lagrangian) 라고 한다.

$$
\mathcal{L}(x,y,\lambda)=f(x,y)-\lambda g(x,y)
$$

라그랑주 승수법은 다음 조건을 만족하는 점을 찾는 방법이다.

$$
\nabla_{x,y,\lambda}\mathcal{L}(x,y,\lambda)=0
$$

즉, 모든 변수 $x,y,\lambda$에 대해 편미분이 0이 되는 지점을 찾는 방법이며, 이를 통해 제약 최적화 문제를 비제약 최적화 문제로 바꾸어 풀 수 있게 된다.

위 식을 전개하면 아래와 같다.

$$
\begin{equation}
\frac{\partial\mathcal{L}}{\partial x}=\nabla_x f(x,y)-\lambda\nabla_x g(x,y)=0
\end{equation}
$$

$$
\begin{equation}
\frac{\partial\mathcal{L}}{\partial y}=\nabla_y f(x,y)-\lambda\nabla_y g(x,y)=0
\end{equation}
$$

$$
\begin{equation}
\frac{\partial\mathcal{L}}{\partial\lambda}=-g(x,y)=0
\end{equation}
$$

$(1), (2)$번 식에 의해 아래 식이 성립한다.

$$
\nabla f(x,y)=\lambda\nabla g(x,y)
$$ 

$(3)$번 식에 의해 아래의 식이 성립한다.

$$
g(x,y)=0
$$

즉, $\nabla_{x,y,\lambda}\mathcal{L}(x,y,\lambda)=0$은 제약 조건을 만족하면서 목적 함수의 기울기 방향이 제약식의 기울기와 일치하는 점을 찾는 것과 같다.

<details>
<summary><font color='#FF0000'>Example 1</font></summary>
<div markdown="1">



</div>
</details>

### 제약 조건이 여러 개인 경우

제약 조건이 여러 개인 경우는 어떻게 풀어야할까? 먼저 제약 조건이 2개인 경우에 대해서

$$
\begin{aligned}
\mathbf{x}^*=\min_{\mathbf{x}}f(\mathbf{x})~~~~\\
\text{subject to}~g_1(\mathbf{x})=a\\
g_2(\mathbf{x})=b
\end{aligned}
$$

제약 조건에서의 최적점은 두 제약식의 그라디언트의 선형 결합으로 표현할 수 있다.

$$
\nabla f(\mathbf{x}^*)=\lambda_1\nabla g_1(\mathbf{x}^*)+\lambda_2\nabla g_2(\mathbf{x}^*)
$$

![fig1](mlm/o17-4.png){: style="display:block; margin:0 auto; width:60%;"}
_[[출처]](https://www.geogebra.org/m/hmcfh5cq)_

이를 일반화하면 아래와 같다.

$$
\nabla f(\mathbf{x}^*)=\sum_{k=1}^M\lambda_k\nabla g_k(\mathbf{x}^*)
$$

따라서, 제약 조건의 여러 개인 경우의 라그랑주 함수는 아래와 같이 정의된다.

$$
\mathcal{L}(\mathbf{x}^*,\boldsymbol\lambda)=f(\mathbf{x})-\sum_{k=1}^M\lambda_k g_k(\mathbf{x})
$$

$$
\nabla\mathcal{L}(\mathbf{x}^*,\boldsymbol\lambda)=\nabla f(\mathbf{x})-\sum_{k=1}^M\lambda_k \nabla g_k(\mathbf{x})
$$

## 라그랑주 승수의 의미

부등식에 대한 lagrange multiplier $\lambda$를 shadow price라고 부르며, <mark style='background-color: fff5b1'>목적 함수의 최적값이 제약 조건에 대해 얼마나 민감한지</mark>를 나타낸다.

$\lambda$가 클수록 해당 제약 조건이 목적 함수에 더 민감하게 영향을 준다는 뜻이며, $\lambda$를 조금만 완화해도 목적 함수가 크게 개선된다는 뜻으로 해석할 수도 있다.

<details>
<summary><font color='red'>Example</font></summary>
<div markdown="1">

공장에서의 물건 생산에서 아래의 조건이 있을 때, 원자재 1kg을 더 사용할 수 있다면 이윤이 얼마나 증가하는가?

- 제약 조건: 원자재는 최대 100kg 사용 가능
- 현재 최적 상태: 이윤 500달러, 원자재는 딱 100kg 사용 중

---

1kg 더 주었더니 이윤이 5달러 증가했다면, 이 5달러가 바로 shadow price이다.

</div>
</details>

## max-min equality

$$
\underset{\lambda}{\max}~\underset{x}{\min} ~f(x,\lambda)\leq\underset{x}{\min}~\underset{\lambda}{\max}~f(x,\lambda)
$$

임의의 함수 $f(x,\lambda)$에 대해 최대화하고 최소화하는 것이 최소화하고 최대화하는 것보다 항상 크다.

<details>
<summary><font color='blue'>공식 유도</font></summary>
<div markdown="1">

1. $g(x,\lambda):=\underset{x}{\min} ~f(x,\lambda)$
2. $g(x,\lambda)\leq f(x,\lambda)$
3. $\underset{\lambda}{\max} ~g(x,\lambda)\leq\underset{\lambda}{\max} ~f(x,\lambda)$
4. $\underset{\lambda}{\max} ~g(x,\lambda)\leq\underset{x}{\min} ~\underset{\lambda}{\max} ~f(x,\lambda)$

</div>
</details>

## Lagrangian

$$
\underset{\mathbf x}\min~f(\mathbf x)
\\
\text{subject to}~~
g_i(\mathbf x)\leq0~,~i=0,1,\dots,m
\\ ~~~~~~~~~~~~~~~~~
h_j(\mathbf x)=0~,~j=0,1,\dots,p
$$

Primal problem이 위와 같이 주어졌을 때, 아래와 같이 원래의 목적 함수와 제약 조건을 결합한 함수를 Lagrangian이라고 한다.

$$
\mathcal{L}(\mathbf x,\boldsymbol\lambda,\boldsymbol\nu)=f(\mathbf x)+
\sum_{i=1}^m\lambda_ig_i(\mathbf x)
+
\sum_{j=1}^p\mu_jh_j(\mathbf x)
$$

제약 조건이 있는 최적화 문제를 다루기 쉽게 만들고, Dual 문제를 정의하기 위해 사용한다.

이때 $\lambda$와 $\nu$를 lagrange multiplier라고 부르며, 부등식 조건에 대한 승수에는 항상 $\lambda_i\geq0$의 조건이 붙음

### Dual problem

원래의 최적화 문제 (primal problem)를 직접 푸는 대신, 다른 문제 (dual problem)를 풀어서 해를 유도하거나 경계를 줄 수 있다.

즉, primal problem은 원래의 최적화 문제, dual problem은 새롭게 정의한 최적화 문제이다.

### Lagrangian Dual problem

$$
\underset{\boldsymbol\mu}{\max}~\mathcal{D}(\boldsymbol\lambda,\boldsymbol\mu)
\\\text{s.t}~~\boldsymbol\lambda\geq0
$$

라그랑지안을 통해 정의한 새로운 최적화 문제이며, lagrangian dual 함수 $\mathcal{D}(\boldsymbol\lambda,\boldsymbol\mu)$는 아래와 같이 정의된다.

$$
\mathcal{D}(\boldsymbol\lambda,\boldsymbol\mu)
=\underset{\mathbf x}{\min}~\mathcal{L}(\mathbf x,\boldsymbol\lambda,\boldsymbol\mu)
$$

Lagrangian을 $\mathbf x$에 대해 최소화한 것으로 정의한다.

Primal 문제에서의 제약 조건의 개수는 dual 문제에서의 변수 개수와 같다.

<details>
<summary><font color='red'>Example</font></summary>
<div markdown="1">

![fig1](mlm/o17-1.png){: style="display:block; margin:0 auto; width:60%;"}
_[[출처]](https://www.youtube.com/watch?v=CodLsdbfjvI)_

---

**1. Lagrangian 함수를 정의한다.**

$$
\mathcal{L}(x,y,\lambda)=\frac{1}{2}(x^2+y^2)+\lambda(x+y-1)
$$

**2. Dual 함수를 정의한다.**

$$
\mathcal{D}(\lambda)=\underset{x,y}\min~\mathcal{L}(x,y,\lambda)=
\underset{x,y}\min~\big(\frac{1}{2}(x^2+y^2)+\lambda(x+y-1)\big)=-\lambda^2-\lambda
$$

**3. Dual problem의 해를 구한다.**

$$
\underset{\lambda}\max~\mathcal{D}(\lambda)=\underset{\lambda}\max~(-\lambda^2-\lambda)=\frac{1}{4}=d^*
$$

**4. Lower bound on the primal optimal**

$$
\underset{x,y}\min~\underset{\lambda}\max~\mathcal{L}(x,y,\lambda)\geq
\underset{\lambda}\max~\mathcal{D}(\lambda)=\frac{1}{4}
~\rightarrow~p^*\geq\frac{1}{4}
$$

</div>
</details>

## Duality gap

primal optimal과 dual optimal의 차이를 duality gap이라고 한다.

**Weak duality**

$$
d^*\leq p^*
$$

위의 부등식은 항상 성립한다.

**Strong duality**

$$
d^*=p^*
$$

위의 부등식은 Primal 문제가 convex이고, 제약 조건을 만족할 때 성립한다.
