---
title: "[선형대수] 행렬 근사"
date: 2025-07-19 00:00:00 +/-TTTT
categories: [인공지능 수학, 선형대수]
tags: [선형대수]
math: true
toc: true
author: sunho
---

## Rank-k 근사 (Rank-k Approximation)

SVD를 통해 행렬 $A$를 $U\Sigma V^\top$으로 분해할 수 있고, 이는 아래와 같이 표현할 수 있다.

$$
A=U\Sigma V^\top=\sum_{i=1}^r\sigma_i\mathbf{u}_i\mathbf{v}_i^\top=\sum_{i=1}^r\sigma_iA_i
$$

- $r$은 $0$이 아닌 특이값의 개수로, $r=\text{rank}(A)$이다.
- $A_i=\mathbf{u}_i\mathbf{v}_i^\top$는 rank-1 행렬로, $A$는 $\text{rank}-1$ 행렬들의 선형 결합으로 이루어져 있다.
- $\sigma_i$는 $\text{rank}-1$ 성분 중 $i$번째 방향의 기여를 나타낸다.

<details>
<summary><font color='#0000FF'>왜 rank-1행렬일까?</font></summary>
<div markdown="1">

$A_i=\mathbf{u}_i\mathbf{v}_i^\top$는 단순히 두 벡터의 외적으로 표현되는 행렬이다.

두 벡터의 외적은 두 벡터의 수직인 벡터이고, 하나의 방향성만 가지기 때문에 $\text{rank}(A)=1$이다.

아래와 같이 직접 계산해보면, $A$의 각 행은 단순히 첫 번째 행의 상수배이기 때문에 $\text{rank}(A)=1$인 것을 알 수 있다.
 
$$
A=\mathbf{u}\mathbf{v}^\top
~\rightarrow~
\begin{bmatrix}1\\3\\5\end{bmatrix}\begin{bmatrix}2&4&6\end{bmatrix}
=\begin{bmatrix}2&4&6\\6&12&18\\10&20&30\end{bmatrix}
$$

Rank는 더한다고 유지되지 않으며, 일반적으로 증가한다.

$$
A=\begin{bmatrix}1&0\\0&0\end{bmatrix}~,~
B=\begin{bmatrix}0&0\\0&1\end{bmatrix}
~\to~
A+B=\begin{bmatrix}1&0\\0&1\end{bmatrix}
$$

따라서 $A$를 여러 개의 $\text{rank}-1$ 행렬의 합으로 볼 수 있다.

---

</div>
</details>
<br>

특이값은 크기 순으로 정렬되어 있으며, 이 중 상위 $k$개의 항만 취했을 때를 $\text{rank}-k$ 근사라고 한다.

$$
\hat{A}(k)=\sum_{i=1}^k\sigma_i\mathbf{u}_i\mathbf{v}_i^\top=\sum_{i=1}^k\sigma_iA_i
$$

$\hat{A}(k)$는 $A$의 SVD에서 가장 큰 k개의 특이값과 대응되는 rank-1 성분만 더한 것으로, 전체 정보를 k개의 중요한 방향만 남겨서 압축한 근사 행렬을 의미한다.

![fig1](mlm/18-1.png){: style="display:block; margin:0 auto; width:70%;"}
_출처: Deisenroth, Faisal, & Ong, <i>Mathematics for Machine Learning</i>_

![fig2](mlm/18-1.png){: style="display:block; margin:0 auto; width:70%;"}
_출처: Deisenroth, Faisal, & Ong, <i>Mathematics for Machine Learning</i>_

위 사진을 보면 큰 특이값부터 $k$개만 남겨도 데이터의 주요 구조를 유지할 수 있다는 것을 확인할 수 있다.

<details>
<summary><font color='#FF0000'>Example 1</font></summary>
<div markdown="1">

$$
A=\begin{bmatrix}3&2&2\\2&3&-2\end{bmatrix}
$$

---

먼저 $A$에 대해 SVD를 수행하여 $U$와 $V$ 행렬을 얻는다.

$$
U=\frac{1}{\sqrt2}\begin{bmatrix}1&-1\\1&1\end{bmatrix}~~,~~
V=\frac{1}{\sqrt2}\begin{bmatrix}1&-\frac{1}{3}&-\frac{2\sqrt2}{3}\\1&\frac{1}{3}&\frac{2\sqrt2}{3}\\0&\frac{4}{3}&\frac{\sqrt2}{3}\end{bmatrix}
$$

$A$에 대한 $\text{rank}-1$ 근사는 아래의 식으로 얻을 수 있다.

$$
A_1=\sigma_1\mathbf{u}_1\mathbf{v}_1^\top=
5\cdot\frac{1}{2}\begin{bmatrix}1\\1\end{bmatrix}\begin{bmatrix}1&1&0\end{bmatrix}=\frac{5}{2}\begin{bmatrix}1&1&0\\1&1&0\end{bmatrix}
$$

</div>
</details>
<br>

### Eckart-Young Theorem

모든 rank-k 행렬 $B$ 중에서 원본 $A$와 가장 가까운 최고의 근사 행렬은 SVD 기반의 $\text{rank}-k$ 행렬 $\hat{A}(k)$라는 것을 수학적으로 증명해준다.

$$
\hat{A}(k)=\underset{\text{rank}(B)=k}{\text{argmin}}~\lVert A-B\rVert_2
$$

이때의 오차는 $k+1$번째 특이값과 같다.

$$
\lVert A-\hat{A}(k)\rVert_2=\sigma_{k+1}
$$
