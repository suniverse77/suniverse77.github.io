---
title: "[선형대수] 행렬식"
date: 2025-07-14 00:00:00 +/-TTTT
categories: [인공지능 수학]
tags: [선형대수]
math: true
toc: true
author: sunho
img_path: /assets/images/math/
description: 📕 벡터의 기본 개념
---

## SVD

![Figure 1](/assets/images/인공지능수학/3-4. Figure1.png){: style="display:block; margin:0 auto; width: 50%; height: 50%;"}

$$
A=U\Sigma V^\top
$$

EVD와 달리 어떠한 행렬이든 위와 같은 형태로 분해가 가능하다.

- $U\in\mathbb{R}^{m\times m}$: column vector가 singular vector인 orthogonal square matrix
- $\Sigma\in\mathbb{R}^{m\times n}$: 대각 성분이 특이값인 대각 행렬 (크기순으로 정렬되어 있음)
- $V\in\mathbb{R}^{n\times n}$: column vector가 singular vector인 orthogonal square matrix

### SVD의 기하학적 의미

![Figure 2](/assets/images/인공지능수학/3-4. Figure2.png){: style="display:block; margin:0 auto; width: 50%; height: 50%;"}

- $V^\top$: 축을 회전시켜 새로운 좌표계로 변환함 (Basis를 변환)
- $\Sigma$: 각 축의 길이를 특이값 크기만큼 scaling하고, 차원을 증강하거나 축소함
- $U$: 다시 축을 회전시켜 새로운 좌표계로 변환함 (Basis를 변환)

## EVD와 SVD의 관계

- **$A^\top A$, $AA^\top$의 고유값은 $A$의 특이값의 제곱이다.**
- Right-Singular vectors $V$는 $A^\top A$의 고유벡터들로 구성된 orthogonal square matrix이다.
- Left-Singular vectors $U$는 $AA^\top$의 고유벡터들로 구성된 orthogonal square matrix이다.

> Right-Singular vectors $V$
>
> $A^\top A=(U\Sigma V^\top)^\top(U\Sigma V^\top)=V\Sigma^\top (U^\top U)\Sigma V^\top=V(\Sigma^\top\Sigma) V^\top$
>
> $$
> A^\top A=V(\Sigma\Sigma^\top) V^\top=V\begin{bmatrix}\sigma_1^2&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&\sigma_n^2\end{bmatrix}V^\top
> $$
>
> $A^\top A$는 symmetric matrix이므로, 항상 EVD가 가능하다.
>
> $$
> A^\top A=PDP^\top=P\begin{bmatrix}\lambda_1&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&\lambda_n\end{bmatrix}P^\top
> $$
>
> SVD와 EVD를 비교해보면 $V=P$, $\sigma_i^2=\lambda_i$이다.
>
> 따라서, $V$는 $A^\top A$의 고유벡터들로 구성되어있으며, $A^\top A$의 고유값은 $A$의 특이값의 제곱이다.

> Left-Singular vectors $U$
>
> $AA^\top=(U\Sigma V^\top)(U\Sigma V^\top)^\top=U\Sigma (V^\top V)\Sigma^\top U^\top=U(\Sigma\Sigma^\top)^\top$
>
> $$
> AA^\top=U(\Sigma\Sigma^\top) U^\top=U\begin{bmatrix}\sigma_1^2&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&\sigma_m^2\end{bmatrix}U^\top
> $$
> 
> $AA^\top$는 symmetric matrix이므로, 항상 EVD가 가능하다.
>
> $$
> AA^\top=PDP^\top=P\begin{bmatrix}\lambda_1&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&\lambda_n\end{bmatrix}P^\top
> $$
>
> SVD와 EVD를 비교해보면 $U=P$, $\sigma_i^2=\lambda_i$이다.
>
> 따라서, $U$는 $AA^\top$의 고유벡터들로 구성되어있으며, $AA^\top$의 고유값은 $A$의 특이값의 제곱이다.