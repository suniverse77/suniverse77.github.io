---
title: "[앙상블] Boosting"
date: 2025-06-12 00:00:00 +/-TTTT
categories: [AI Theory, 머신러닝]
tags: [앙상블]
math: true
toc: true
author: sunho
---

## Boosting

Boosting은 여러 개의 약한 학습기 (Weak Learner)를 결합하여 강력한 하나의 강한 학습기 (Strong Learner)를 만드는 앙상블 기법이다.

Boosting에서는 모델들이 동시에 독립적으로 학습하는 것이 아니라, 이전 모델이 잘못 예측한 부분에 집중하여 다음 모델이 그 오차를 보완하도록 순차적으로 학습을 진행한다.

이때, 이전 모델의 에러를 어떻게 다룰지에 따라 여러 알고리즘으로 분류된다.

## AdaBoost (Adaptive Boosting)

AdaBoost는 틀린 데이터의 가중치를 조절하는 방식으로 약한 학습기를 개선해나가는 알고리즘이다.

구체적인 알고리즘은 아래와 같다.

1. 모든 $N$개의 학습 데이터에 동일한 가중치 $D_t$를 부여하며 시작한다.

    $$
    D_1(i)=\frac{1}{N}
    $$

    위의 식에서 아래첨자 $t$는 $t$번째 학습기임을 의미한다.
2. 현재 단계의 데이터 가중치 $D_t$를 반영하여, 약한 학습기 $h_t$를 학습시킨다. 이때 예측이 틀린 데이터에는 가중치를 높이고, 예측을 맞춘 데이터에는 가중치를 낮춘다.
3. 해당 모델 $h_t$가 얼마나 틀렸는지를 나타내는 total error $\epsilon_t$를 계산한다. 이는 틀린 데이터들의 가중치 합으로 계산된다.

    $$
    \epsilon_t=\sum_{i} D_t(i)~~,~~i=\text{틀린 데이터}
    $$
4. 3번에서 계산한 $\epsilon_t$를 바탕으로, 해당 모델의 신뢰도 (Amount of Say) $\alpha_t$를 계산한다.

    $$
    \alpha_t=\frac{1}{2}\ln\left(\frac{1-\epsilon_t}{\epsilon_t}\right)
    $$
5. 학습이 완료되면, 각 약한 학습기들의 예측을 신뢰도 바탕의 weighted voting을 통해 최종 예측을 수행한다.

    $$
    \hat{y}=\text{sign}\left(\sum_{t=1}^M\alpha_th_t(x)\right)
    $$

![fig1](ml/12-1.png){: style="display:block; margin:0 auto; width:60%;"}
_[[출처]](https://www.geeksforgeeks.org/machine-learning/bagging-vs-boosting-in-machine-learning/)_

Adaboost는 에러가 있다, 없다만 판별하여 가중치를 할당하기 때문에, 에러가 큰 데이터, 작은 데이터는 반영하지 못한다는 단점이 있다. 이를 보완하기 위해 GBM이 등장하였다.

Adaboost는 분류 성공/실패 여부를 중심으로 데이터 가중치를 조절한다. 이 때문에 각 데이터의 에러 크기는 반영하지 못한다는 단점이 있다. 이를 보완하기 위해 GBM이 등장하였다.

## GBM (Gradient Boosting Machine)

GBM은 데이터의 가중치를 조절하는 AdaBoost와 달리, 이전 모델의 에러 자체를 다음 모델의 학습 목표로 삼는다.

GBM에서는 여기서 그라디언트 (Gradient)가 핵심 역할을 한다. 그라디언트는 손실 함수를 기준으로, 정답에 다가가기 위해 어떤 방향으로 어느 정도로 변경해야 하는지를 알려주는 값이다.

구체적인 알고리즘은 아래와 같다.

1. 현재까지의 누적 모델 $F_t$를 기준으로, 각 데이터에 대해 그라디언트를 계산한다.
2. 다음 약한 학습기 $h_t$는 이 오차 값 자체를 예측하도록 학습된다.
3. 새로운 약한 학습기 $h_t$가 오차를 학습하고 나면, 이 모델의 예측 결과를 $\eta$만큼 축소하여 기존의 누적 모델 $F_t(x)$에 더한다.

    $$
    F_{t+1}(x)=F_t(x)+\eta\cdot h_t(x)
    $$

    위의 식에서 $\eta$는 학습률을 의미하며, $h_t(x)$의 영향력을 조절하여 모델이 한 번에 너무 많이 변하는 것을 막고 과적합을 방지하는 역할을 한다.

![fig2](ml/12-2.png){: style="display:block; margin:0 auto; width:40%;"}
_[[출처: 딥스푼]](https://www.youtube.com/watch?v=rVOYicsNgVQ&t=121s)_

GBM의 경우 모델을 순차적으로 학습시켜야 하므로 학습 속도가 느리고, 파라미터 튜닝이 민감하여 과적합 위험이 있어 일반화 능력이 떨어질 수 있다는 단점이 있다. 이를 보완하기 위해 XGBoost가 등장하였다.

## XGBoost (Extreme Gradient Boosting)

XGBoost는 GBM 알고리즘을 최적화하여 성능과 속도를 높인 모델이다.

- 손실 함수에 L1, L2 Regularization term을 포함하여, 모델이 학습 데이터에 오버피팅 되는 것을 방지한다.
- 알고리즘 자체는 순차적이지만, 하나의 Tree를 만들 때 최적의 분기점을 찾는 연산 과정을 병렬로 처리한다.
- 데이터에 결측치 (Missing Value)가 있어도 별도의 전처리 없이, XGBoost가 스스로 학습한다.

위의 개선 사항 외에도, 여러 가지가 존재한다.

![fig3](ml/12-3.png){: style="display:block; margin:0 auto; width:60%;"}
_[[출처]](https://m.blog.naver.com/luexr/223466048770)_
