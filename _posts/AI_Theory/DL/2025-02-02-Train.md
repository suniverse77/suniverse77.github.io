---
title: "[NN] 학습"
date: 2025-02-02 12:00:00 +/-TTTT
categories: [AI Theory, 딥러닝]
tags: [신경망]
math: true
toc: true
author: sunho
---

## 가중치 초기화 (Weight Initialization)

가중치를 초기화하는 여러 가지 시도가 있었다.

### Small random numbers

가중치를 랜덤한 작은 값들로 초기화하는 방법이 있다.

```python
# 4096개 뉴런을 가진 레이어 7개 (입력 + 은닉6)
dims = [4096] * 7

hs = []

# 가우시안 분포를 따르는 초기 입력 데이터 생성 (16개 샘플, 4096개 특징)
x = np.random.rand(16, dims[0])

for Din, Dout in zip(dims[:-1], dims[1:]):
    # 가우시안 분포를 따르는 4096x4096 크기의 가중치 행렬 생성
    W = np.random.randn(Din, Dout)

    # 표준편차를 0.01로 만듦
    W = 0.01 * W
    
    # 순전파: 행렬 곱 + ReLU 활성화 함수
    x = np.maximum(0, x.dot(W))
    hs.append(x)
```

첫 번째 layer에서 출력 activation의 분산은 대략 $1^2\times 0.01^2\times4096\approx0.4$가 된다. 그럼 두 번째 layer의 입력의 분산이 1보다 작은 $0.4$가 되므로 layer를 지날수록 activation의 분산은 0으로 수렴하게 된다.

입력과 가중치의 평균이 0이므로, 출력의 평균도 0이 된다. 하지만 ReLU 함수는 음수 영역을 제거하기 때문에 출력의 평균은 양수의 어떤 값이 된다. 하지만 출력의 분산이 작다는 것은 결국 0 근처에 모여있다는 뜻이기 때문에 평균 또한 0으로 수렴하게 된다. 

![fig1](dl/cnn/4-1.png){: style="display:block; margin:0 auto; width:100%;"}
_출처: Stanford CS231n, Lecture 6 (CNN Architectures)_

시각화를 한 결과, 가중치의 평균과 표준편차가 점점 작아져 0에 수렴하는 것을 볼 수 있다.

### Large random numbers

가중치를 랜덤한 큰 값들로 초기화하는 방법이 있다.

```python
dims = [4096] * 7
hs = []
x = np.random.rand(16, dims[0])

for Din, Dout in zip(dims[:-1], dims[1:]):
    W = np.random.randn(Din, Dout)

    # 표준편차를 0.05로 만듦
    W = 0.05 * W
    
    x = np.maximum(0, x.dot(W))
    hs.append(x)
```

첫 번째 layer에서 출력 activation의 분산은 대략 $1^2\times 0.05^2\times4096\approx10.24$가 된다. 그럼 두 번째 layer의 입력의 분산이 1보다 큰 $10.24$가 되므로 layer를 지날수록 activation의 분산은 발산하게 된다.

위에서 말했듯이 ReLU 함수는 음수 영역을 제거하기 때문에 출력의 평균은 양수의 어떤 값이 되며, 분산이 크기 때문에 평균이 점점 큰 양수쪽으로 움직이게 된다.

![fig2](dl/cnn/4-2.png){: style="display:block; margin:0 auto; width:100%;"}
_출처: Stanford CS231n, Lecture 6 (CNN Architectures)_

시각화를 한 결과, 가중치의 평균과 표준편차가 점점 커져 발산하는 것을 볼 수 있다.

### Kaiming He Initialization

ReLU 함수를 사용할 때 신호가 layer를 지나면서 사라지거나 폭발하지 않도록, 가중치를 평균이 $0$, 표준편차가 $\sqrt{\frac{2}{n_{in}}}$인 가우시안 분포를 따르도록 초기화하는 것을 제안하였다.

$$
W\sim \mathcal{N}(0,\sqrt{\frac{2}{n_{in}}})
$$

- $n_{in}$은 입력 노드의 개수이다.

```python
dims = [4096] * 7
hs = []
x = np.random.rand(16, dims[0])

for Din, Dout in zip(dims[:-1], dims[1:]):
    # 
    W = np.random.randn(Din, Dout) * np.sqrt(2/Din)
    x = np.maximum(0, x.dot(W))
    hs.append(x)
```

![fig3](dl/cnn/4-3.png){: style="display:block; margin:0 auto; width:100%;"}
_출처: Stanford CS231n, Lecture 6 (CNN Architectures)_

시각화를 한 결과, layer를 거쳐도 가중치의 평균과 표준편차가 유지되는 것을 볼 수 있다.

## Learning Rate Decay

학습률 (learning rate)을 고정하는 방법 외에도 다양한 전략이 존재한다.

경험적으로 배치 크기를 $k$배 증가시키면 학습률도 $k$배 증가시키는 것이 효과적이라고 한다. (이를 linear scaling rule이라고 함)

### Step

특정 epoch마다 learning rate를 감소시키는 방식이다.

![fig5](cs231n/03-5.png){: style="display:block; margin:0 auto; width:40%;"}

### Cosine

$$
\alpha_t=\frac{1}{2}\alpha_0(1+\cos(t\pi/T))
$$

![fig6](cs231n/03-6.png){: style="display:block; margin:0 auto; width:40%;"}

### Linear

선형적으로 감소시키는 방식이다.

$$
\alpha_t=\alpha_0(1-t/T)
$$

![fig7](cs231n/03-7.png){: style="display:block; margin:0 auto; width:40%;"}

### Invert Sqrt

$$
\alpha_t=\frac{\alpha_0}{\sqrt{t}}
$$

![fig8](cs231n/03-8.png){: style="display:block; margin:0 auto; width:40%;"}

### Linear Warmup

초기에 선형적으로 learning rate를 증가시킨 후 cosine 또는 invert sqrt 방법을 따라가는 방식이다.

![fig9](cs231n/03-9.png){: style="display:block; margin:0 auto; width:40%;"}
