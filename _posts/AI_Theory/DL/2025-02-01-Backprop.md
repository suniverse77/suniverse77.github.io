---
title: "[NN] 역전파 (Backpropagation)"
date: 2025-02-01 06:00:00 +/-TTTT
categories: [AI Theory, 딥러닝]
tags: [딥러닝, 역전파]
math: true
toc: true
author: sunho
---

## 역전파 (Backpropagation)

실제 신경망에는 파라미터가 매우 많다.

앞선 방법으로 그라디언트를 계산하려면 매번 새로운 손실 함수에 대해 $\nabla_\theta\mathcal{L}$을 계산해야하기 때문에 매우 비효율적이다.

그렇다면 컴퓨터는 복잡한 신경망에서 어떻게 효율적으로 그라디언트를 계산할 수 있을까?

계산 그래프와 연쇄 법칙 (Chain Rule)을 이용해 구한다.

### 계산 그래프 (Computational Graphs)

신경망의 연산은 덧셈, 곱셈 등의 일련의 기본 연산으로 구성되기 때문에 이를 아래와 같은 계산 그래프로 표현할 수 있다.

![fig1](AI_Theory/DL/Backprop-1.png){: style="display:block; margin:0 auto; width:60%;"}
_출처: Stanford CS231n, Lecture 4 (Neural Networks and Backpropagation)_

- **순전파 (Forward pass)**에서는 입력에서 출발해 그래프를 따라가면서 각 노드의 출력을 차례대로 계산하며, 최종적으로 손실 함수 값 $\mathcal{L}$을 얻는다.
- **역전파 (Backward pass)**에서는 출력에서 입력 방향으로 그래프를 거슬러 올라가면서 chain rule을 이용해 각 파라미터에 대한 손실의 기울기 $\nabla_\theta\mathcal{L}$를 계산한다.

### 연쇄 법칙 (Chain Rule)

$x$가 입력됐을 때 $y=f(x)$ 역할을 하는 노드가 있는 경우, 역전파의 과정은 아래 그림과 같다.

![fig2](AI_Theory/DL/Backprop-2.png){: style="display:block; margin:0 auto; width:60%;"}

$$
\text{Downstream Gradient}=\text{Upstream Gradient}\times\text{Local Gradient}
$$

역전파 과정에서 연쇄 법칙은 위의 식을 따르며, 각 구성 요소는 아래와 같다.

**Upstream Gradient**

$$\frac{\partial\mathcal{L}}{\partial y}$$

- 출력 방향에서 전해져 온 미분값
- Loss가 현재 노드의 출력값 변화에 얼마나 민감한지를 나타냄

**Local Gradient**

$$\frac{\partial y}{\partial x}$$

- 현재 노드 자체의 미분값
- 현재 노드의 출력값이 입력값 변화에 얼마나 민감한지를 나타냄

**Downstream Gradient**

$$
\frac{\partial\mathcal{L}}{\partial x}=
\frac{\partial\mathcal{L}}{\partial y}\cdot
\frac{\partial y}{\partial x}
$$

- 입력 방향으로 보낼 최종 미분값
- Loss가 현재 노드의 입력값 변화에 얼마나 민감한지를 나타냄
- Downstream Gradient는 앞선 layer의 Upstream Gradient가 됨

## 역전파 계산 과정

실제 딥러닝에서는 하나의 노드에 입력 $x$와 가중치 $w$가 입력으로 들어와 $y=f(x,w)$를 출력한다.

이때, 역전파 과정에서 Chain Rule에 의해 2가지의 Downstream Gradient가 생긴다.

$$\frac{\partial\mathcal{L}}{\partial x}~~,~\frac{\partial\mathcal{L}}{\partial w}$$

$\frac{\partial\mathcal{L}}{\partial x}$는 앞선 layer로 그라디언트를 전달할 때 사용되며, $\frac{\partial\mathcal{L}}{\partial w}$는 해당 노드의 파라미터를 업데이트하는 데 사용된다.

### 1. 입출력이 모두 스칼라인 경우 (Scalar to Scalar)

1개의 노드에서 입력을 받아 1개의 값을 출력하는 단일 뉴런의 경우를 생각해보자.

![fig3](AI_Theory/DL/Backprop-3.png){: style="display:block; margin:0 auto; width:30%;"}

이 경우 입력 데이터와 가중치는 각각 $x\in\mathbb{R}~,w\in\mathbb{R}$이며, forward 과정은 아래와 같다.

$$
y=wx\in\mathbb{R}
$$

**Chain Rule**

- 입력에 대한 미분

    $$
    \frac{\partial\mathcal{L}}{\partial x}=\frac{\partial\mathcal{L}}{\partial y}\cdot\frac{\partial y}{\partial x}
    =\frac{\partial\mathcal{L}}{\partial y}\cdot w
    $$
- 가중치에 대한 미분

    $$
    \frac{\partial\mathcal{L}}{\partial w}=\frac{\partial\mathcal{L}}{\partial y}\cdot\frac{\partial y}{\partial w}
    =\frac{\partial\mathcal{L}}{\partial y}\cdot x
    $$

#### ㄴ

![fig6](dl/nn/2-6.png){: style="display:block; margin:0 auto; width:60%;"}
_출처: Stanford CS231n, Lecture 4 (Neural Networks and Backpropagation)_



### 2. 벡터 입력과 스칼라 출력 (Vector to Scalar)

$n$개의 노드에서 입력을 받아 1개의 값을 출력하는 단일 뉴런의 경우를 생각해보자.

![fig7](dl/nn/2-7.png){: style="display:block; margin:0 auto; width:30%;"}

이 경우 입력 데이터와 가중치는 각각 $\mathbf{x}\in\mathbb{R}^n~,\mathbf{w}\in\mathbb{R}^n$이며, forward 과정은 아래와 같다.

$$
y=\mathbf{w}^\top\mathbf{x}\in\mathbb{R}
$$

**Chain Rule**

- 입력에 대한 미분

    $$
    \frac{\partial\mathcal{L}}{\partial\mathbf{x}}=\frac{\partial\mathcal{L}}{\partial y}\cdot\frac{\partial y}{\partial \mathbf{x}}
    =\frac{\partial\mathcal{L}}{\partial y}\cdot\mathbf{w}\in\mathbb{R}^{n}
    $$
- 가중치에 대한 미분

    $$
    \frac{\partial\mathcal{L}}{\partial\mathbf{w}}=\frac{\partial\mathcal{L}}{\partial y}\cdot\frac{\partial y}{\partial \mathbf{w}}
    =\frac{\partial\mathcal{L}}{\partial y}\cdot\mathbf{x}\in\mathbb{R}^{n}
    $$

### 3. 입출력이 모두 벡터인 경우 (Vector to Vector)

$n$개의 노드가 $m$개의 노드로 연결되는 경우를 생각해보자.

![fig8](dl/nn/2-8.png){: style="display:block; margin:0 auto; width:30%;"}

이 경우 입력 데이터와 가중치는 각각 $\mathbf{x}\in\mathbb{R}^n~,W\in\mathbb{R}^{m\times n}$이며, forward 과정은 아래와 같다.

$$
\mathbf{y}=W\mathbf{x}\in\mathbb{R}^m
$$

**Chain Rule**

- 입력에 대한 미분

    $$
    \frac{\partial\mathcal{L}}{\partial\mathbf{x}}=\frac{\partial\mathcal{L}}{\partial\mathbf{y}}\cdot\frac{\partial\mathbf{y}}{\partial \mathbf{x}}
    =W^\top\cdot\frac{\partial\mathcal{L}}{\partial\mathbf{y}}\in\mathbb{R}^{n}
    $$
- 가중치에 대한 미분

    $$
    \frac{\partial\mathcal{L}}{\partial W}=\frac{\partial\mathcal{L}}{\partial\mathbf{y}}\cdot\frac{\partial\mathbf{y}}{\partial W}
    =\frac{\partial\mathcal{L}}{\partial\mathbf{y}}\cdot\mathbf{x}^\top\in\mathbb{R}^{m\times n}
    $$

### 4. 입출력이 행렬인 경우: 배치 처리 (Matrix to Matrix)

$B$개의 배치가 있고, 각 배치에서 $n$개의 노드가 $m$개의 노드로 연결되는 경우를 생각해보자.

![fig9](dl/nn/2-9.png){: style="display:block; margin:0 auto; width:30%;"}

이 경우 입력 데이터와 가중치는 각각 $X\in\mathbb{R}^{n\times B}~,\mathbf{W}\in\mathbb{R}^{(m\times B)\times(n\times B)}$이며, forward 과정은 아래와 같다.

$$
Y=\mathbf{W}X\in\mathbb{R}^{m\times B}
$$

**Chain Rule**

- 입력에 대한 미분

    $$
    \frac{\partial\mathcal{L}}{\partial X}=\frac{\partial\mathcal{L}}{\partial Y}\cdot\frac{\partial Y}{\partial X}\in\mathbb{R}^{n\times B}
    $$
- 가중치에 대한 미분

    $$
    \frac{\partial\mathcal{L}}{\partial\mathbf{W}}=\frac{\partial\mathcal{L}}{\partial Y}\cdot\frac{\partial Y}{\partial \mathbf{W}}\in\mathbb{R}^{(m\times B)\times(n\times B)}
    $$

#### ReLU 함수에서의 역전파

ReLU는 Element-wise 연산이기 때문에 $x_1$은 $z_1$에만 영향을 주고 $z_2, z_3$ 등에는 영향을 주지 않는다.

따라서 Local Gradient는 대각선 성분만 값을 가지고 나머지는 모두 0인 대각 행렬이다.

![fig7](dl/nn/2-7.png){: style="display:block; margin:0 auto; width:70%;"}
_출처: Stanford CS231n, Lecture 4 (Neural Networks and Backpropagation)_

결과적으로 역전파 때, 원래 입력이 양수였던 요소의 기울기만 그대로 전파되고, 음수였던 곳은 0이 된다.

$$
\left(\frac{\partial L}{\partial \mathbf{x}}\right)_i=
\begin{cases}
\left(\frac{\partial L}{\partial \mathbf{z}}\right)_i&x_i>0\\
0&\text{otherwise}
\end{cases}
$$
