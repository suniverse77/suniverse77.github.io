---
title: "[NN] 경사 하강법 (Gradient Descent)"
date: 2025-02-01 06:00:00 +/-TTTT
categories: [AI Theory, 딥러닝]
tags: [딥러닝, 역전파]
math: true
toc: true
author: sunho
---

## 경사 하강법 (Gradient Descent)

앞서, 모델은 학습을 통해 가중치를 업데이트한다고 했다. 그럼 어떤 기준과 방법으로 파라미터들을 업데이트할까?

모델이 파라미터를 업데이트하는 기준은 손실 함수 $\mathcal{L}$의 값을 최소화하는 것이며, 이를 위해 그라디언트 정보를 이용한다.

기본적인 업데이트 수식은 아래와 같다.

$$
\theta_{t+1}=\theta_t-\eta\nabla\mathcal{L}_\theta(\theta_t)
$$

위의 수식에서 $\eta$는 학습률 (Learning Rate)로, 한 번에 얼마나 이동할지인 보폭을 의미한다.

그라디언트는 함수값이 증가하는 방향을 가리키는데, 우리는 손실을 줄여야 하므로 빼기 연산을 하여 업데이트한다.

아래 그림에서 볼 수 있듯이 손실 함수의 최소값을 찾기 위해 그라디언트를 이용해 경사를 따라 내려가듯 가중치를 업데이트하며, 이 알고리즘을 경사 하강법이라고 부른다.

![fig1](dl/nn/2-1.png){: style="display:block; margin:0 auto; width:50%;"}
_[[출처]](https://blog.ex-em.com/1750)_

### Gradient 계산 방법

컴퓨터가 그라디언트를 계산하는 방법에는 크게 두 가지가 있다.

#### Numerical gradient

미분의 수학적 정의를 이용해 그라디언트의 근사치를 계산하는 방법이다.

$$
\frac{df(x)}{dx}=\lim_{h\to0}\frac{f(x+h)-f(x)}{h}
$$

$h$가 완전한 0이 아니므로 근사치이며, 가중치 하나하나에 대해 두 가지 함수 $f(x+h)$, $f(x)$를 호출해야 하므로 매우 느리다.

하지만 수식이 직관적이기 때문에 코드로 구현하기는 쉽다.

#### Analytic gradient

미적분학을 이용해 그라디언트를 계산하는 공식을 유도하여 구현하는 방법이다.

$$
f(x)=2x^3 ~\to~f'(x)=6x^2
$$

수학 공식을 직접 사용하기 때문에 오차가 없고, 한 번의 계산으로 모든 가중치에 대한 그라디언트를 얻을 수 있기 때문에 매우 빠르다.

하지만 복잡한 모델의 그라디언트 공식을 유도하고 코드로 구현하는 것이 어렵다.
