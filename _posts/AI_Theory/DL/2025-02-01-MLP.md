---
title: "[NN] MLP (Multi-Layer Perceptron)"
date: 2025-02-01 00:00:00 +/-TTTT
categories: [AI Theory, 딥러닝]
tags: [딥러닝, MLP]
math: true
toc: true
author: sunho
---

## 퍼셉트론 (Perceptron)

퍼셉트론은 인공 신경망에서 가장 작은 세포 하나라고 생각하면 된다.

인간 뇌의 뉴런이 신호를 주고받는 방식을 단순하게 수학적으로 만든 모델로, 아래 그림에서 동그라미가 뉴런을 의미한다.

![fig1](dl/nn/1-1.png){: style="display:block; margin:0 auto; width:70%;"}
_[[출처]](https://lcyking.tistory.com/entry/%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0%EC%9D%98-%EA%B0%9C%EC%9A%94)_

위 그림에서 $x$는 입력 신호, $w$는 각 신호의 중요도를 의미하며, 모델은 학습을 통해 가중치 $w$의 값을 조절한다.

입력 신호들의 합이 활성화 함수 $F$를 통과하여 출력이 만들어진다.

$$
y=F(x_1w_1+x_2w_2+\cdots+x_nw_n)
$$

위의 수식은 벡터를 이용해 아래와 같이 표현할 수 있다.

$$
y=F(\mathbf{w}^\top\mathbf{x})~~,~~\mathbf{w},\mathbf{x}\in\mathbb{R}^n
$$

### 다층 퍼셉트론 (MLP)

이러한 퍼셉트론을 여러 층으로 쌓은 구조를 MLP라고 한다.

입력층은 layer로 세지 않기 때문에 아래 그림에서 신경망은 3개의 layer로 구성되어 있다고 할 수 있다. (hiddne layer 2개 + 출력 layer 1개)

![fig2](dl/nn/1-2.png){: style="display:block; margin:0 auto; width:70%;"}
_출처: Stanford CS231n, Lecture 4 (Neural Networks and Backpropagation)_

첫 번째 layer에 뉴런이 $n$개, 두 번째 layer에 뉴런이 $m$개 존재한다면, 이를 벡터와 행렬을 이용해 아래와 같이 표현할 수 있다.

$$
y=F(W\mathbf{x})~~,~~W\in\mathbb{R}^{m\times n},~\mathbf{x}^n
$$

### 활성화 함수 (Activation Function)

만약 layer 사이에 비선형 함수를 넣지 않는다면, 여러 layer를 쌓아도 결국 하나의 선형 변환과 동일해진다.

$$
y=W_3(W_2(W_1\mathbf{x}))=W\mathbf{x}
$$

만약 선형으로 분리되지 않는 데이터라면, layer를 여러 개 쌓아도 결국 분리하지 못하게 된다.

따라서 활성화 함수 (비선형 함수)를 이용해 입력 공간을 변형하여, 원래 선형적으로 분리되지 않던 데이터를 구분 가능하게 만든다.

예를 들어, 아래의 그림은 유클리디안 좌표계에서 선형 분리가 불가능했던 데이터가, 극좌표계로 변환되면서 쉽게 분리가 가능해진 상황을 보여준다.

![fig3](dl/nn/1-3.png){: style="display:block; margin:0 auto; width:90%;"}
_출처: Stanford CS231n, Lecture 4 (Neural Networks and Backpropagation)_

활성화 함수에는 여러 종류가 있다.

![fig4](dl/nn/1-4.png){: style="display:block; margin:0 auto; width:90%;"}
_출처: Stanford CS231n, Lecture 2 (Image Classification with Linear Classifiers)_

### 편향 (Bias)

이때, 가중치말고도 학습 가능한 파라미터인 Bias가 있다.

Bias는 모델이 원점 $(0,0)$을 지나지 않는 데이터도 표현할 수 있게 해준다.

$$
y=F(\mathbf{w}^\top\mathbf{x}+b)
$$

직관적으로는 뉴런이 얼마나 쉽게 활성화될지를 조절하는 역할을 한다.

예를 들어, Bias가 없다면 ReLU 함수를 사용한 뉴런은 $\mathbf{w}^\top\mathbf{x}>0$일 때만 활성화된다.

하지만 만약 $b=-3$이라면, ReLU함수를 사용한 뉴런은 입력값이 $3$보다 클 때만 활성화된다.

$$
y=\text{ReLU}(\mathbf{w}^\top\mathbf{x}-3)=\max(\mathbf{w}^\top\mathbf{x},3)
$$

## 선형 분류기 (Linear Classifier)

![fig5](dl/nn/1-5.png){: style="display:block; margin:0 auto; width:80%;"}
_출처: Stanford CS231n, Lecture 2 (Image Classification with Linear Classifiers)_

가중치 행렬의 각 행은 입력 이미지에 대해 각기 다른 질문을 던지는 것으로 생각할 수 있다.

예를 들어, 위의 그림에서 첫 번째 행은 `'이 이미지가 고양이야?'`, 두 번째 행은 `'이 이미지가 개야?'`, 세 번째 행은 `'이 이미지가 배야?'`라는 질문을 던지는 것으로 볼 수 있다.

만약 입력 이미지가 고양이라면, 고양이를 찾는 첫 번째 가중치 행과 이미지의 특징 벡터 $\mathbf{x}$를 내적했을 때 가장 높은 점수가 나오게 된다.

아래는 마지막 layer의 가중치 행렬의 각 행을 2D map으로 시각화한 것으로, 클래스마다 특정 패턴이 보이는 것을 확인할 수 있다.

![fig6](dl/nn/1-6.png){: style="display:block; margin:0 auto; width:80%;"}
_출처: Stanford CS231n, Lecture 2 (Image Classification with Linear Classifiers)_

### 기하학적 관점

32x32x3 크기의 이미지의 경우, 이 이미지는 총 3,072개의 숫자들로 이루어진 데이터로 볼 수 있다.

3,072개의 숫자 하나하나를 각 차원의 좌표 값으로 생각하면, 이 이미지는 3,072차원 공간에 있는 하나의 점으로 표현할 수 있다.

같은 클래스의 이미지들은 고차원 공간에서 비슷한 영역에 모여 분포한다.

즉, 이미지는 고차원 공간 위의 하나의 점이고, 같은 클래스의 점들은 서로 모여있기 때문에, Linear Classifier를 각 클래스의 점을 가장 잘 구분하는 경계선으로도 해석할 수 있다. 이 경계선을 결정 경계 (Decision Boundary)라고 부른다.

이때, $\mathbf{w}$는 결정 경계에 수직인 방향을 가리키는 벡터이며, 입력 데이터 $\mathbf{x}$의 방향이 $\mathbf{w}$의 방향과 비슷하다면, 해당 결정 경계가 분류하고 있는 클래스라는 의미이다.

Bias는 결정 경계의 위치를 결정하며, 경계선을 데이터 분포에 맞게 평행 이동시켜 최적의 위치에 놓는 역할을 한다.

![fig7](dl/nn/1-7.png){: style="display:block; margin:0 auto; width:80%;"}
_출처: Stanford CS231n, Lecture 2 (Image Classification with Linear Classifiers)_
