---
title: "[평가 지표] AUROC와 AP"
date: 2025-03-01 06:00:00 +/-TTTT
categories: [AI Theory, 평가 지표]
tags: [평가 지표]
math: true
toc: true
author: sunho
---

Accuracy, Precision, Recall, F1 Score는 특정 임계값을 고정한 상태에서 계산한 지표이다.

예를 들어, classification이나 segmentation에서는 확률값이 일정 기준을 넘어야 정답으로 판정하며, object detection 또한 바운딩박스의 IoU 임계값과 신뢰도 기준이 먼저 정해져야 한다.

이에 반해 AUROC와 AP는 모든 임계값에 대해 계산한 것으로, 모델 자체의 성능을 평가할 수 있다.

## AUROC (Area Under ROC Curve)

ROC (Receiver Operating Characteristic) 곡선은 임계값 변화에 따라 **FPR (False Positive Rate)**이 변할 때 **TPR (True Positive Rate)**이 어떻게 변하는지를 그린 곡선이다.

- **FPR**: 실제 **Negative** 중에서 모델이 잘못 예측한 것의 비율로, 값이 $0$에 가까울수록 좋다.

    $$
    \text{FPR}=\frac{\text{FP}}{\text{FP}+\text{TN}}
    $$
- **TPR**: 실제 **Positive** 중에서 모델이 맞춘 것의 비율로, 값이 $1$에 가까울수록 좋다. Recall과 동일하다.
    $$
    \text{TPR}=\frac{\text{TP}}{\text{TP}+\text{FN}}
    $$

이때, AUROC는 이 ROC 곡선 아래의 면적을 의미하며, 값이 $1$에 가까울수록 모델의 성능이 좋다고 본다.

![fig1](dl/metric/2-1.png){: style="display:block; margin:0 auto; width:50%;"}
_[[출처]](https://www.blog.trainindata.com/auc-roc-analysis/)_

### TPR과 FPR의 관계

해당 부분은 '공돌이의 수학정리노트'님의 [*'ROC curve'*](https://angeloyeo.github.io/2020/08/05/ROC.html) 포스트를 참고하였습니다.

![fig2](dl/metric/2-2.png){: style="display:block; margin:0 auto; width:50%;"}
_[[출처]](https://www.blog.trainindata.com/auc-roc-analysis/)_

예를 들어, 의사 A는 모든 환자들을 다 암환자라고 판단한다고 하자.

그러면 실제로 암에 걸린 환자들은 모두 암 환자로 판정되고, 암에 걸리지 않은 환자들도 모두 암 환자로 판정된다.

즉, True Positive Rate와 False Positive Rate가 모두 높아지게 된다.

이는 아래와 같이 threshold가 낮다는 뜻이다.

![fig3](dl/metric/2-3.gif){: style="display:block; margin:0 auto; width:60%;"}
_[[출처]](https://www.blog.trainindata.com/auc-roc-analysis/)_

한편, 의사 B는 모든 환자들을 암환자가 아니라고 판단한다고 하자.

그러면 암에 걸리지 않은 환자들 뿐만 아니라, 실제로 암에 걸린 환자들도 모두 정상인으로 판정되게 된다.

즉, True Positive Rate와 False Positive Rate 모두 낮아지게 된다.

이는 아래와 같이 threshold가 높다는 뜻이다.

![fig4](dl/metric/2-4.gif){: style="display:block; margin:0 auto; width:60%;"}
_[[출처]](https://www.blog.trainindata.com/auc-roc-analysis/)_

### 곡선 위의 점의 의미

곡선 위의 한 점은 특정 임계값 (Threshold)에서의 FPR과 TPR의 관계를 나타낸다.

즉, AUROC는 임계값을 0%부터 100%까지 쭉 변경해 가면서 얻어지는 (FPR, TPR) 좌표들을 모두 이어 그린 선인 것을 알 수 있다.

![fig5](dl/metric/2-5.gif){: style="display:block; margin:0 auto; width:80%;"}
_[[출처]](https://angeloyeo.github.io/2020/08/05/ROC.html)_

### 곡선의 휘어짐 정도의 의미

곡선의 휘어짐은 모델이 **Positive**와 **Negative** 클래스를 얼마나 잘 구별하는지를 나타낸다.

모델이 두 클래스를 더 잘 구별할수록, ROC 커브는 좌상단에 더 가까워지게 된다.

![fig6](dl/metric/2-6.gif){: style="display:block; margin:0 auto; width:80%;"}
_[[출처]](https://angeloyeo.github.io/2020/08/05/ROC.html)_

## AP (Average Precision)

AP는 모델이 얼마나 정확하게 예측하는지와 얼마나 빠짐없이 모든 객체를 찾아내는지를 하나의 숫자로 요약한 지표이다.

주로 Object Detection에서 사용된다.

AP는 PR 곡선의 아래쪽 면적을 계산한 값이다.

$$
\text{AP}=\int_0^1p(r)dr
$$

여기서 $r$은 Recall 값, $p(r)$은 Recall이 $r$일 때의 Precision 값을 의미한다.

$0\sim1$ 사이의 값을 가지며, $1$이면 Recall이 높아지는 모든 순간에도 Precision이 높게 유지된다는 의미이다.

### PR 곡선 (Precision-Recall Curve)

Precision과 Recall은 trade-off 관계이다.

임계값을 낮추면 Recall은 높아지지만 Precision은 낮아지고, 임계값을 높이면 Precision은 높아지지만 Recall은 낮아진다.

이때, 모든 지점에서의 (Recall, Precision) 쌍을 계산하여 그래프로 그린 것이 PR 곡선이다.

![fig7](dl/metric/2-7.png){: style="display:block; margin:0 auto; width:50%;"}
_[[출처]](https://ctkim.tistory.com/entry/mAPMean-Average-Precision-%EC%A0%95%EB%A6%AC)_

- 정답 (TP)을 만났을 때 Precision과 Recall은 모두 증가하므로, 그래프가 오른쪽 위로 올라간다.
- 오답 (FP)을 만났을 때 Recall은 그대로지만 Precision은 감소하므로, 그래프가 수직으로 뚝 떨어진다.

평가의 편의성을 위해 자신의 오른쪽 (더 높은 Recall)에 더 높은 Precision이 있다면, 그 값으로 보간한다.

### mAP (mean Average Precision)

mAP는 모든 클래스의 AP 값들의 평균을 의미한다.

$$
\text{mAP}=\frac{1}{C}\sum_{i=1}^C\text{AP}_i
$$

여기서 $C$는 클래스 개수를 의미한다.
